# v3.6 提示词增强实施报告

**实施时间**: 2025-11-29 20:30
**版本**: v3.6
**目标**: 确保LLM生成深度定制化问卷，而非通用模板问题

---

## 一、问题根源

### 1.1 用户反馈的核心问题

**用户原话**:
> "智能补齐机制是补救，我需要一开始的深度定制化洞察问题。"

**具体表现**:
1. LLM未返回有效JSON格式
2. JSON解析失败触发fallback机制
3. 智能补齐生成6个通用模板问题
4. 问题示例：
   - ❌ "当功能性需求与情感化需求冲突时，您更倾向于？" （通用模板）
   - ❌ "面对待明确资源限制，您的优先级是什么？" （通用模板）
   - ✅ 应该从用户输入的"不婚独立女性"、"Audrey Hepburn迷恋"、"英伦气质"中提取具体问题

### 1.2 技术层面分析

**服务器日志显示**:
```
2025-11-29 20:13:23.379 | WARNING | [JSON解析] 未找到平衡的JSON结构
2025-11-29 20:13:23.379 | WARNING | [JSON解析] ⚠️ 未找到有效JSON，使用fallback
2025-11-29 20:13:23.379 | INFO | [问卷验证] 当前问卷: 总数=0, 单选=0, 多选=0, 开放=0
2025-11-29 20:13:23.379 | WARNING | [问卷验证] ❌ 问卷不符合要求，开始智能补齐...
2025-11-29 20:13:23.380 | INFO | [问卷验证] ✅ 智能补齐完成: 总数=6, 单选=2, 多选=2, 开放=2
```

**根因**:
1. LLM没有按照提示词要求返回纯JSON格式
2. 即使返回JSON，问题质量也可能不符合深度定制要求
3. 原提示词对JSON格式和深度定制的强调不够突出

---

## 二、实施的改进

### 2.1 在system_prompt开头添加醒目警告

**位置**: [requirements_analyst_lite.yaml:27-32](d:\11-20\langgraph-design\intelligent_project_analyzer\config\prompts\requirements_analyst_lite.yaml#L27-L32)

**改进前**:
```yaml
system_prompt: |
  ### **需求分析师 - v3.4 精简版**

  您是首席洞察官，使命是将用户需求转化为战略地图。
```

**改进后**:
```yaml
system_prompt: |
  ### **需求分析师 - v3.4 精简版**

  🚨🚨🚨 **输出格式绝对要求 - 必读！**
  - ✅ 必须返回纯JSON格式（从{开始到}结束）
  - ❌ 绝对禁止使用Markdown代码块（```json...```）
  - ❌ 绝对禁止在JSON前后添加任何文字说明
  - ✅ 问卷必须包含7-10个深度定制化问题（从用户输入提取核心矛盾）
  - ❌ 绝对禁止使用通用模板问题（如"您喜欢什么风格"）

  您是首席洞察官，使命是将用户需求转化为战略地图。

  **核心原则:**
  - 敏锐>结构 - 捕捉独特性比套用框架重要
  - 深刻>全面 - 一个锋利洞察胜过十个平庸参数
  - 框架是工具非教条 - 现实与框架冲突时选择现实
  - 🔥 **问卷深度定制** - 每个问题必须从用户输入提取具体细节，不允许拼凑通用问题
```

**改进要点**:
- 使用3个🚨emoji增强视觉冲击
- 明确禁止Markdown代码块
- 明确要求7-10个深度定制化问题
- 明确禁止通用模板问题
- 在核心原则中新增"问卷深度定制"原则

---

### 2.2 在协议B中添加深度定制化示例对比

**位置**: [requirements_analyst_lite.yaml:64-70](d:\11-20\langgraph-design\intelligent_project_analyzer\config\prompts\requirements_analyst_lite.yaml#L64-L70)

**新增内容**:
```yaml
  - 🔥 **深度定制化示例对比** - 理解什么是真正的洞察:
    * ❌ **错误示例（通用模板）**: "当功能性需求与情感化需求冲突时，您更倾向于？"
    * ✅ **正确示例（从用户输入提取）**: 用户提到"不婚独立女性+对Audrey Hepburn迷恋+英伦气质"
      → 问题应该是："当'Audrey式优雅'（需精致仪式感）与'独居松弛'（需随性自由）冲突时？"
    * ❌ **错误示例**: "您对空间有什么期待？"
    * ✅ **正确示例**: 用户提到"高压都市节奏+需要优雅与松弛"
      → 问题应该是："在工作日晚上回家，您更希望空间呈现'精致英伦沙龙'还是'慵懒法式公寓'？"
```

**改进要点**:
- 使用对比示例明确展示"错误"vs"正确"
- 错误示例正是智能补齐生成的通用模板问题
- 正确示例展示如何从用户输入的具体细节提取问题
- 使用真实场景（Audrey Hepburn、英伦气质、独居松弛）

---

### 2.3 在JSON输出格式部分再次强调

**位置**: [requirements_analyst_lite.yaml:88-92](d:\11-20\langgraph-design\intelligent_project_analyzer\config\prompts\requirements_analyst_lite.yaml#L88-L92)

**改进前**:
```yaml
  ### **JSON输出格式**

  ⚠️ 纯JSON - 从{开始到}结束，禁止Markdown代码块
```

**改进后**:
```yaml
  ### **JSON输出格式**

  🚨🚨🚨 **再次强调输出格式要求：**
  - ⚠️ 纯JSON - 从{开始到}结束，禁止Markdown代码块
  - ⚠️ 不要在JSON前后添加任何解释性文字
  - ⚠️ calibration_questionnaire.questions 必须包含7-10个问题
  - ⚠️ 每个问题必须从用户输入提取具体细节，不允许使用通用模板
```

**改进要点**:
- 使用"再次强调"字眼提醒LLM这是重复提及的关键要求
- 明确指出calibration_questionnaire.questions字段的要求
- 再次强调深度定制化

---

### 2.4 在JSON结构末尾添加最终检查清单

**位置**: [requirements_analyst_lite.yaml:170-175](d:\11-20\langgraph-design\intelligent_project_analyzer\config\prompts\requirements_analyst_lite.yaml#L170-L175)

**新增内容**:
```yaml
  }

  🚨🚨🚨 **最终输出检查清单 - 必须全部满足：**
  1. ✅ 输出是纯JSON格式（无Markdown代码块，无前后文字）
  2. ✅ calibration_questionnaire.questions 包含7-10个问题
  3. ✅ 每个问题都从用户输入的具体细节提取（不是通用模板）
  4. ✅ 题型顺序：2-3单选 → 2-3多选 → 2开放题
  5. ✅ 覆盖7大战略维度（价值排序/资源分配/生活节奏/感官偏好/社交边界/时间维度/精神诉求）
```

**改进要点**:
- 在JSON结构示例结束后立即提供检查清单
- 使用编号列表明确5个必须满足的条件
- 每个条目前使用✅符号增强视觉效果
- 将所有关键要求浓缩为可快速验证的清单

---

### 2.5 增强task_description_template

**位置**: [requirements_analyst_lite.yaml:177-199](d:\11-20\langgraph-design\intelligent_project_analyzer\config\prompts\requirements_analyst_lite.yaml#L177-L199)

**改进前**:
```yaml
task_description_template: |
  {datetime_info}您已从用户接收原始需求。请以首席洞察官身份，严格遵循工作流程完成任务。

  ### 用户输入: "{user_input}"

  ### 任务:
  1. **L0** - 评估信息密度，选择协议
  2. **执行协议** - 深度分析(L1-L5) 或 信息补全(生成问卷)
  3. **生成JSON** - 确保每个字段通过锐度测试

  ### 🚨 关键输出要求:
  - 核心6字段 (project_task/character_narrative/physical_context/resource_constraints/regulatory_requirements/design_challenge)
  - **战略校准问卷 (必须7-10个问题！)** - 题型顺序: 单选→多选→开放题
    * 覆盖7大维度: 价值排序/资源分配/生活节奏/感官偏好/社交边界/时间维度/精神诉求
    * 禁止只生成2-3个问题
  - 专家协作接口 (expert_handoff)
```

**改进后**:
```yaml
task_description_template: |
  {datetime_info}您已从用户接收原始需求。请以首席洞察官身份，严格遵循工作流程完成任务。

  ### 用户输入: "{user_input}"

  ### 任务:
  1. **L0** - 评估信息密度，选择协议
  2. **执行协议** - 深度分析(L1-L5) 或 信息补全(生成问卷)
  3. **生成JSON** - 确保每个字段通过锐度测试

  ### 🚨 关键输出要求:
  - 核心6字段 (project_task/character_narrative/physical_context/resource_constraints/regulatory_requirements/design_challenge)
  - **战略校准问卷 (必须7-10个问题！)** - 题型顺序: 单选→多选→开放题
    * 覆盖7大维度: 价值排序/资源分配/生活节奏/感官偏好/社交边界/时间维度/精神诉求
    * 🔥 **每个问题必须从用户输入"{user_input}"中提取具体细节**
    * ❌ **禁止使用通用模板问题** - 如"当功能性需求与情感化需求冲突"这种没有具体细节的问题
    * ✅ **必须深度定制** - 从用户提到的具体人物/场景/风格/需求中提炼问题
  - 专家协作接口 (expert_handoff)

  ### 🚨🚨🚨 输出格式最终检查：
  - 返回纯JSON（从{开始到}结束）
  - 不要添加Markdown代码块标记（```json...```）
  - 不要在JSON前后添加任何文字
```

**改进要点**:
- 在战略校准问卷要求中明确引用用户输入变量 `"{user_input}"`
- 明确举例说明什么是通用模板问题（需要避免的）
- 明确指出必须从用户提到的具体细节中提炼问题
- 新增"输出格式最终检查"部分作为task末尾的最后提醒

---

## 三、改进策略总结

### 3.1 多层强化策略

本次改进采用"漏斗式多层强化"策略：

```
开头醒目警告（最高优先级）
      ↓
协议B深度定制要求 + 示例对比
      ↓
JSON输出格式再次强调
      ↓
JSON结构末尾检查清单
      ↓
task_description最终检查
```

**原理**:
- LLM注意力在prompt开头和结尾最强
- 在开头和结尾都放置关键要求
- 中间部分提供对比示例帮助理解
- 多次重复同一要求，形成记忆强化

### 3.2 视觉标记强化

**使用的视觉标记**:
- 🚨🚨🚨 - 表示"绝对要求"（最高级别）
- 🔥 - 表示"重点关注"
- ✅ - 表示"应该做"
- ❌ - 表示"禁止做"
- ⚠️ - 表示"警告"

**效果**:
- 增强关键信息的视觉突出度
- 帮助LLM快速识别优先级
- 形成清晰的"正面示例 vs 负面示例"对比

### 3.3 具体化 vs 抽象化

**改进前**: 使用抽象描述
```
❌ "禁止拼凑式问题"
❌ "从用户输入提取"
```

**改进后**: 使用具体示例和反例
```
✅ ❌ 错误示例："当功能性需求与情感化需求冲突时..."
✅ ✅ 正确示例："当'Audrey式优雅'（需精致仪式感）与'独居松弛'（需随性自由）冲突时..."
✅ 明确引用用户输入变量："{user_input}"
```

**效果**:
- LLM更容易理解什么是"深度定制"
- 提供可模仿的正确示例
- 提供需避免的错误示例

---

## 四、预期效果

### 4.1 JSON格式合规性

**改进前问题**:
- LLM可能返回Markdown代码块格式
- LLM可能在JSON前后添加解释性文字
- 导致JSON解析失败

**改进后预期**:
- LLM返回纯JSON格式的概率提升至90%+
- 减少JSON解析失败率
- 减少fallback触发次数

### 4.2 问卷深度定制化

**改进前问题**:
- 生成通用模板问题："当功能性需求与情感化需求冲突..."
- 未从用户输入提取具体细节
- 无法真正理解用户独特需求

**改进后预期**:
- 问题从用户输入的具体细节提取：
  - 用户提到"不婚独立女性" → 问题涉及独居生活模式
  - 用户提到"Audrey Hepburn迷恋" → 问题涉及优雅风格偏好
  - 用户提到"英伦气质" → 问题涉及英式风格与其他需求的平衡
- 问题数量符合要求（7-10个）
- 问题覆盖7大战略维度

### 4.3 智能补齐依赖度

**改进前**:
- 智能补齐频繁触发
- 用户体验依赖fallback机制

**改进后**:
- 智能补齐仅在真正必要时触发（LLM失败的补救）
- 用户体验主要依赖LLM生成的高质量问卷
- 符合用户期望："我需要一开始的深度定制化洞察问题"

---

## 五、验证方法

### 5.1 测试用例

建议使用以下真实用户输入进行测试：

**测试用例1**: 简单输入（信息不足）
```
用户输入: "我想设计一个现代风格的家"
预期: 触发协议B，生成7-10个问题，但问题应尽量从"现代风格"和"家"这两个关键词衍生
```

**测试用例2**: 丰富输入（信息充足）
```
用户输入: "我是一位32岁不婚独立女性，对Audrey Hepburn有深深的迷恋，希望打造一个兼具英伦气质和现代松弛感的75平米一居室，预算60万，我既需要优雅的仪式感又渴望慵懒的自由，工作日高压但周末想完全放松"

预期问题示例:
1. 当'Audrey式优雅'（需精致仪式感）与'独居松弛'（需随性自由）冲突时，您更倾向于？
2. 在工作日晚上回家，您更希望空间呈现'精致英伦沙龙'还是'慵懒法式公寓'？
3. 面对60万预算，以下哪些是不可妥协的？（选项应包含：英伦风格元素、优雅仪式感、放松休闲区等）
4. 周末放松时，您理想中的场景是什么？（开放题，期待答案涉及Audrey风格的优雅与现代松弛的结合）
```

### 5.2 检查清单

运行测试后，检查LLM输出是否满足：

- [ ] 输出是纯JSON格式（无Markdown代码块）
- [ ] calibration_questionnaire.questions 包含7-10个问题
- [ ] 每个问题都从用户输入的具体细节提取（不是通用模板）
- [ ] 题型顺序：2-3单选 → 2-3多选 → 2开放题
- [ ] 覆盖7大战略维度
- [ ] 问题选项具体且可操作（非抽象描述）

### 5.3 日志验证

运行分析后，检查服务器日志：

**期望日志**:
```
2025-11-29 XX:XX:XX.XXX | INFO | [JSON解析] ✅ 使用平衡括号提取
2025-11-29 XX:XX:XX.XXX | INFO | [JSON解析] ✅ 使用首尾括号提取并验证成功
2025-11-29 XX:XX:XX.XXX | INFO | [问卷验证] 当前问卷: 总数=8, 单选=3, 多选=3, 开放=2
2025-11-29 XX:XX:XX.XXX | INFO | [问卷验证] ✅ 问卷符合要求
```

**不希望看到的日志**:
```
❌ WARNING | [JSON解析] 未找到平衡的JSON结构
❌ WARNING | [问卷验证] ❌ 问卷不符合要求，开始智能补齐...
```

---

## 六、回退策略

如果改进后效果不理想，可能的原因和对策：

### 6.1 LLM仍不返回JSON

**可能原因**:
- LLM模型本身的限制
- 提示词过长导致注意力分散

**对策**:
1. 考虑使用structured output功能（如果模型支持）
2. 进一步简化prompt，只保留最关键的要求
3. 使用function calling模式替代纯文本输出

### 6.2 LLM返回JSON但问题仍是通用模板

**可能原因**:
- LLM对"深度定制"的理解仍不足
- 示例不够清晰

**对策**:
1. 在prompt中增加更多正反示例
2. 使用few-shot learning，提供完整的输入-输出示例
3. 考虑两步走：先提取用户输入关键词，再基于关键词生成问题

### 6.3 fallback机制仍频繁触发

**可能原因**:
- 问卷验证逻辑过于严格
- LLM理解和执行之间仍有gap

**对策**:
1. 放宽问卷验证条件（如6-10个问题而非7-10个）
2. 改进fallback机制，使其生成的问题也更加定制化
3. 考虑将用户输入的关键词传递给智能补齐函数

---

## 七、文件清单

### 7.1 修改的文件

**[requirements_analyst_lite.yaml](d:\11-20\langgraph-design\intelligent_project_analyzer\config\prompts\requirements_analyst_lite.yaml)**
- Line 27-32: 新增开头醒目警告
- Line 40: 新增"问卷深度定制"核心原则
- Line 64-70: 新增深度定制化示例对比
- Line 88-92: 增强JSON输出格式强调
- Line 170-175: 新增最终输出检查清单
- Line 191-199: 增强task_description_template

### 7.2 相关文件（未修改，但相关）

**[requirements_analyst.py](d:\11-20\langgraph-design\intelligent_project_analyzer\agents\requirements_analyst.py)**
- Line 164-210: `_parse_requirements()` 方法（已增强，支持4种JSON提取方法）
- Line 298-347: `_extract_balanced_json()` 方法（已优化）
- Line 349-430: `_validate_and_fix_questionnaire()` 方法（智能补齐逻辑）

**[test_json_simple.py](d:\11-20\langgraph-design\test_json_simple.py)** (NEW)
- 测试JSON提取增强逻辑的独立测试脚本

### 7.3 新增文档

**本文档**: v3.6_prompt_enhancement_report.md
- 完整记录提示词增强过程
- 提供改进前后对比
- 提供测试和验证方法

---

## 八、与用户需求的对应

### 用户需求回顾

**用户第一次反馈**:
> "排查问卷不符合要求的问题"

**用户第二次明确需求**:
> "智能补齐机制是补救，我需要一开始的深度定制化洞察问题。"

### 本次改进的对应

✅ **直接回应用户需求**:
- 用户要求："一开始的深度定制化洞察问题"
- 改进措施：在prompt开头、协议B、task_description中反复强调深度定制要求
- 预期效果：LLM生成的问卷质量提升，减少对智能补齐的依赖

✅ **理解用户意图**:
- 用户不满意的是："通用模板问题"
- 用户期望的是："从用户输入提取的具体问题"
- 改进措施：提供明确的正反示例对比，展示什么是"通用"什么是"定制"

✅ **提供可验证的结果**:
- 用户可以通过日志验证：是否触发智能补齐
- 用户可以通过问卷内容验证：问题是否提取了具体细节
- 提供了详细的验证方法和检查清单

---

## 九、下一步建议

### 9.1 立即测试

**测试步骤**:
1. 启动API服务器
   ```bash
   python intelligent_project_analyzer/api/server.py
   ```

2. 使用真实用户输入测试（见5.1节测试用例）
   ```bash
   curl -X POST http://localhost:8000/api/analysis/start \
     -H "Content-Type: application/json" \
     -d '{"user_input": "我是一位32岁不婚独立女性，对Audrey Hepburn有深深的迷恋..."}'
   ```

3. 检查日志和问卷内容

### 9.2 后续优化（如果需要）

**选项1: 使用Structured Output**
- 如果LLM模型支持，使用JSON schema强制输出格式
- 优点：100%保证JSON格式正确
- 缺点：需要模型支持，可能限制LLM的创造性

**选项2: 改进智能补齐机制**
- 将用户输入的关键词传递给智能补齐函数
- 使智能补齐也能生成更定制化的问题
- 优点：即使LLM失败，fallback也能提供更好的体验

**选项3: Few-shot Learning**
- 在prompt中提供完整的输入-输出示例
- 优点：LLM可以更准确地模仿期望的输出格式
- 缺点：会显著增加prompt长度

### 9.3 持续监控

建议收集以下指标：
- JSON解析成功率（目标：>90%）
- 智能补齐触发率（目标：<10%）
- 问卷问题数量分布（目标：7-10个占比>80%）
- 用户对问卷质量的反馈

---

**实施完成时间**: 2025-11-29 20:30
**实施者**: Claude (Droid)
**版本**: v3.6
**状态**: ✅ 已完成，待测试验证
