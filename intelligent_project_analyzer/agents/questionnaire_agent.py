"""
é—®å·ç”Ÿæˆæ™ºèƒ½ä½“ (LangGraph StateGraph)

ğŸ”¥ v7.16: å°† LLMQuestionGenerator å‡çº§ä¸ºçœŸæ­£çš„ LangGraph Agent

æ ¸å¿ƒåŠŸèƒ½:
1. ä¸Šä¸‹æ–‡æå– (Extract Context) - ä»éœ€æ±‚åˆ†æä¸­æå–å…³é”®ä¿¡æ¯
2. é—®é¢˜ç”Ÿæˆ (Generate Questions) - LLMé©±åŠ¨ç”Ÿæˆå®šåˆ¶åŒ–é—®é¢˜
3. ç›¸å…³æ€§éªŒè¯ (Validate Relevance) - éªŒè¯é—®é¢˜ä¸ç”¨æˆ·è¾“å…¥çš„ç›¸å…³æ€§

èŠ‚ç‚¹æµè½¬:
    extract_context â†’ generate_questions â†’ validate_relevance â†’ END
    
    å¦‚æœéªŒè¯å¤±è´¥:
    extract_context â†’ generate_questions â†’ validate_relevance â†’ regenerate_questions â†’ END
"""

from typing import TypedDict, Dict, Any, List, Optional, Tuple
from loguru import logger
from datetime import datetime
import json
import re
import time

from langgraph.graph import StateGraph, START, END

# å¯¼å…¥å…±äº«å·¥å…·å‡½æ•°
from ..utils.shared_agent_utils import PerformanceMonitor, extract_questionnaire_context


# ============================================================================
# çŠ¶æ€å®šä¹‰
# ============================================================================

class QuestionnaireState(TypedDict):
    """é—®å·ç”Ÿæˆæ™ºèƒ½ä½“çŠ¶æ€"""
    
    # è¾“å…¥
    user_input: str                   # ç”¨æˆ·åŸå§‹è¾“å…¥
    structured_data: Dict[str, Any]   # éœ€æ±‚åˆ†æå¸ˆçš„ç»“æ„åŒ–è¾“å‡º
    
    # ä¸­é—´çŠ¶æ€
    analysis_summary: str             # æ„å»ºçš„åˆ†ææ‘˜è¦
    user_keywords: List[str]          # ä»ç”¨æˆ·è¾“å…¥æå–çš„å…³é”®è¯
    raw_llm_response: str             # LLMåŸå§‹å“åº”
    
    # è¾“å‡º
    questions: List[Dict[str, Any]]   # ç”Ÿæˆçš„é—®é¢˜åˆ—è¡¨
    relevance_score: float            # ç›¸å…³æ€§åˆ†æ•° (0-1)
    low_relevance_questions: List[str]  # ä½ç›¸å…³æ€§é—®é¢˜
    generation_source: str            # ç”Ÿæˆæ¥æº ("llm_generated" | "fallback" | "regenerated")
    generation_rationale: str         # ç”Ÿæˆç†ç”±
    
    # é…ç½®
    _llm_model: Any                   # LLMæ¨¡å‹
    _max_regenerations: int           # æœ€å¤§é‡æ–°ç”Ÿæˆæ¬¡æ•°
    _regeneration_count: int          # å½“å‰é‡æ–°ç”Ÿæˆæ¬¡æ•°
    
    # å¤„ç†æ—¥å¿—
    processing_log: List[str]


# ============================================================================
# èŠ‚ç‚¹å‡½æ•°
# ============================================================================

def extract_context_node(state: QuestionnaireState) -> Dict[str, Any]:
    """
    ä¸Šä¸‹æ–‡æå–èŠ‚ç‚¹ - ä»éœ€æ±‚åˆ†æä¸­æå–å…³é”®ä¿¡æ¯
    """
    logger.info("ğŸ“Š æ‰§è¡Œä¸Šä¸‹æ–‡æå–èŠ‚ç‚¹")
    
    structured_data = state.get("structured_data", {})
    user_input = state.get("user_input", "")
    
    # æ„å»ºåˆ†ææ‘˜è¦
    summary_parts = []
    
    # é¡¹ç›®æ¦‚è§ˆ
    project_overview = structured_data.get("project_overview", "")
    if project_overview:
        summary_parts.append(f"## é¡¹ç›®æ¦‚è§ˆ\n{project_overview}")
    
    # é¡¹ç›®ä»»åŠ¡
    project_task = structured_data.get("project_task", "") or structured_data.get("project_tasks", "")
    if isinstance(project_task, list):
        project_task = "ï¼›".join(project_task[:5])
    if project_task and project_task != project_overview:
        summary_parts.append(f"## é¡¹ç›®ä»»åŠ¡\n{project_task}")
    
    # æ ¸å¿ƒç›®æ ‡
    core_objectives = structured_data.get("core_objectives", [])
    if core_objectives:
        if isinstance(core_objectives, list):
            objectives_text = "\n".join([f"- {obj}" for obj in core_objectives[:5]])
        else:
            objectives_text = str(core_objectives)
        summary_parts.append(f"## æ ¸å¿ƒç›®æ ‡\n{objectives_text}")
    
    # é¡¹ç›®ç±»å‹
    project_type = structured_data.get("project_type", "")
    if project_type:
        type_label = {
            "personal_residential": "ä¸ªäººä½å®…",
            "hybrid_residential_commercial": "æ··åˆå‹ï¼ˆä½å®…+å•†ä¸šï¼‰",
            "commercial_enterprise": "å•†ä¸š/ä¼ä¸šé¡¹ç›®",
            "cultural_educational": "æ–‡åŒ–/æ•™è‚²é¡¹ç›®",
            "healthcare_wellness": "åŒ»ç–—/åº·å…»é¡¹ç›®",
            "office_coworking": "åŠå…¬/è”åˆåŠå…¬",
            "hospitality_tourism": "é…’åº—/æ–‡æ—…é¡¹ç›®",
            "sports_entertainment_arts": "ä½“è‚²/å¨±ä¹/è‰ºæœ¯"
        }.get(project_type, project_type)
        summary_parts.append(f"## é¡¹ç›®ç±»å‹\n{type_label}")
    
    # è®¾è®¡æŒ‘æˆ˜
    design_challenge = structured_data.get("design_challenge", "")
    if design_challenge:
        summary_parts.append(f"## æ ¸å¿ƒè®¾è®¡æŒ‘æˆ˜\n{design_challenge}")
    
    # æ ¸å¿ƒå¼ åŠ›
    core_tension = structured_data.get("core_tension", "")
    if core_tension:
        summary_parts.append(f"## æ ¸å¿ƒçŸ›ç›¾/å¼ åŠ›\n{core_tension}")
    
    # äººç‰©å™äº‹
    narrative_characters = structured_data.get("narrative_characters", "") or structured_data.get("character_narrative", "")
    if isinstance(narrative_characters, list):
        narrative_characters = "\n".join([f"- {char}" for char in narrative_characters[:3]])
    if narrative_characters:
        summary_parts.append(f"## äººç‰©å™äº‹/ç”¨æˆ·ç”»åƒ\n{narrative_characters}")
    
    # ç‰©ç†ç¯å¢ƒ
    physical_contexts = structured_data.get("physical_contexts", "")
    if isinstance(physical_contexts, list):
        physical_contexts = "ï¼›".join(physical_contexts[:3])
    if physical_contexts:
        summary_parts.append(f"## ç‰©ç†ç¯å¢ƒ\n{physical_contexts}")
    
    # èµ„æºçº¦æŸ
    resource_constraints = structured_data.get("resource_constraints", "")
    if resource_constraints:
        summary_parts.append(f"## èµ„æºçº¦æŸ\n{resource_constraints}")
    
    analysis_summary = "\n\n".join(summary_parts) if summary_parts else "ï¼ˆæœªæå–åˆ°è¶³å¤Ÿçš„éœ€æ±‚ä¿¡æ¯ï¼‰"
    
    # æå–ç”¨æˆ·å…³é”®è¯ï¼ˆç”¨äºç›¸å…³æ€§éªŒè¯ï¼‰
    user_keywords = _extract_keywords(user_input)
    
    log_entry = f"ğŸ“Š ä¸Šä¸‹æ–‡æå–å®Œæˆ: æ‘˜è¦é•¿åº¦ {len(analysis_summary)}, æå– {len(user_keywords)} ä¸ªå…³é”®è¯"
    logger.info(log_entry)
    
    return {
        "analysis_summary": analysis_summary,
        "user_keywords": user_keywords,
        "processing_log": [log_entry]
    }


def generate_questions_node(state: QuestionnaireState) -> Dict[str, Any]:
    """
    é—®é¢˜ç”ŸæˆèŠ‚ç‚¹ - ä½¿ç”¨LLMç”Ÿæˆå®šåˆ¶åŒ–é—®é¢˜
    """
    logger.info("ğŸ¤– æ‰§è¡Œé—®é¢˜ç”ŸæˆèŠ‚ç‚¹")
    
    user_input = state.get("user_input", "")
    analysis_summary = state.get("analysis_summary", "")
    llm_model = state.get("_llm_model")
    user_keywords = state.get("user_keywords", [])
    regeneration_count = state.get("_regeneration_count", 0)
    
    # å¦‚æœæ˜¯é‡æ–°ç”Ÿæˆï¼Œä½¿ç”¨æ›´å¼ºçš„çº¦æŸ
    is_regeneration = regeneration_count > 0
    
    if llm_model is None:
        logger.warning("âš ï¸ LLMæ¨¡å‹æœªæä¾›ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ")
        return _fallback_generate(user_input, state.get("structured_data", {}))
    
    try:
        # æ„å»ºæç¤ºè¯
        system_prompt = _build_system_prompt(is_regeneration, user_keywords)
        human_prompt = _build_human_prompt(user_input, analysis_summary, is_regeneration, user_keywords)
        
        # è°ƒç”¨LLM
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": human_prompt}
        ]
        
        response = llm_model.invoke(messages)
        raw_content = response.content if hasattr(response, "content") else str(response)
        
        # è§£æLLMå“åº”
        questionnaire_data = _parse_llm_response(raw_content)
        questions = questionnaire_data.get("questions", [])
        
        if not questions:
            logger.warning("âš ï¸ LLMè¿”å›ç©ºé—®å·ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ")
            return _fallback_generate(user_input, state.get("structured_data", {}))
        
        # éªŒè¯å’Œä¿®å¤é—®é¢˜æ ¼å¼
        validated_questions = _validate_questions(questions)
        
        log_entry = f"ğŸ¤– é—®é¢˜ç”Ÿæˆå®Œæˆ: {len(validated_questions)} ä¸ªé—®é¢˜ (é‡æ–°ç”Ÿæˆ: {is_regeneration})"
        logger.info(log_entry)
        
        return {
            "questions": validated_questions,
            "raw_llm_response": raw_content,
            "generation_rationale": questionnaire_data.get("generation_rationale", ""),
            "generation_source": "regenerated" if is_regeneration else "llm_generated",
            "processing_log": [log_entry]
        }
        
    except Exception as e:
        logger.error(f"âŒ LLMç”Ÿæˆå¤±è´¥: {e}")
        return _fallback_generate(user_input, state.get("structured_data", {}))


def validate_relevance_node(state: QuestionnaireState) -> Dict[str, Any]:
    """
    ç›¸å…³æ€§éªŒè¯èŠ‚ç‚¹ - éªŒè¯é—®é¢˜ä¸ç”¨æˆ·è¾“å…¥çš„ç›¸å…³æ€§
    """
    logger.info("ğŸ¯ æ‰§è¡Œç›¸å…³æ€§éªŒè¯èŠ‚ç‚¹")
    
    questions = state.get("questions", [])
    user_input = state.get("user_input", "").lower()
    user_keywords = state.get("user_keywords", [])
    
    if not questions:
        return {
            "relevance_score": 0.0,
            "low_relevance_questions": [],
            "processing_log": ["âš ï¸ æ— é—®é¢˜å¯éªŒè¯"]
        }
    
    relevant_count = 0
    low_relevance_questions = []
    
    for q in questions:
        question_text = q.get("question", "").lower()
        
        # æ£€æŸ¥é—®é¢˜æ˜¯å¦åŒ…å«ç”¨æˆ·å…³é”®è¯
        is_relevant = False
        for keyword in user_keywords:
            if keyword.lower() in question_text or keyword.lower() in user_input:
                is_relevant = True
                break
        
        # ä¹Ÿæ£€æŸ¥é—®é¢˜ä¸ç”¨æˆ·è¾“å…¥çš„ç›´æ¥å…³è”
        if not is_relevant:
            # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹ä¸Šçš„ç›¸å…³æ€§ï¼ˆé€šè¿‡å…¬å…±è¯æ±‡ï¼‰
            question_words = set(question_text.split())
            input_words = set(user_input.split())
            common_words = question_words & input_words
            # æ’é™¤åœç”¨è¯
            stop_words = {"çš„", "æ˜¯", "åœ¨", "å’Œ", "æœ‰", "ä¸º", "ä¸", "äº†", "èƒ½", "å—", "å‘¢", "å§", "ä»€ä¹ˆ", "å¦‚ä½•", "æ€ä¹ˆ", "æ‚¨"}
            meaningful_common = common_words - stop_words
            if len(meaningful_common) >= 1:
                is_relevant = True
        
        if is_relevant:
            relevant_count += 1
        else:
            low_relevance_questions.append(q.get("question", "")[:50])
    
    relevance_score = relevant_count / len(questions) if questions else 0.0
    
    log_entry = f"ğŸ¯ ç›¸å…³æ€§éªŒè¯å®Œæˆ: åˆ†æ•° {relevance_score:.2f}, {len(low_relevance_questions)} ä¸ªä½ç›¸å…³é—®é¢˜"
    logger.info(log_entry)
    
    return {
        "relevance_score": relevance_score,
        "low_relevance_questions": low_relevance_questions,
        "processing_log": [log_entry]
    }


def should_regenerate(state: QuestionnaireState) -> str:
    """
    æ¡ä»¶è·¯ç”±ï¼šå†³å®šæ˜¯å¦éœ€è¦é‡æ–°ç”Ÿæˆ
    """
    relevance_score = state.get("relevance_score", 0.0)
    regeneration_count = state.get("_regeneration_count", 0)
    max_regenerations = state.get("_max_regenerations", 1)
    
    # å¦‚æœç›¸å…³æ€§å¤ªä½ä¸”è¿˜æœ‰é‡è¯•æœºä¼š
    if relevance_score < 0.3 and regeneration_count < max_regenerations:
        logger.info(f"ğŸ”„ ç›¸å…³æ€§è¿‡ä½ ({relevance_score:.2f})ï¼Œè§¦å‘é‡æ–°ç”Ÿæˆ")
        return "regenerate"
    
    return "complete"


def increment_regeneration_node(state: QuestionnaireState) -> Dict[str, Any]:
    """
    å¢åŠ é‡æ–°ç”Ÿæˆè®¡æ•°
    """
    current_count = state.get("_regeneration_count", 0)
    return {
        "_regeneration_count": current_count + 1,
        "processing_log": [f"ğŸ”„ è§¦å‘ç¬¬ {current_count + 1} æ¬¡é‡æ–°ç”Ÿæˆ"]
    }


# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================

def _extract_keywords(text: str) -> List[str]:
    """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
    # ç®€å•çš„å…³é”®è¯æå–ï¼šæå–å¼•å·å†…çš„å†…å®¹å’Œæ•°å­—ç›¸å…³çš„è¯
    keywords = []
    
    # æå–å¼•å·å†…å®¹
    quoted = re.findall(r'[""ã€Œã€]([^""ã€ã€]+)[""ã€ã€]', text)
    keywords.extend(quoted)
    
    # æå–æ•°å­—+å•ä½
    numbers = re.findall(r'\d+(?:\.\d+)?(?:ã¡|å¹³ç±³|ä¸‡|ç±³|å±‚|é—´|äºº|å¹´|%)', text)
    keywords.extend(numbers)
    
    # æå–å…³é”®åè¯ï¼ˆé€šè¿‡å¸¸è§æ¨¡å¼ï¼‰
    patterns = [
        r'(\w{2,4}(?:ä½å®…|å…¬å¯“|åˆ«å¢…|å•†é“º|åŠå…¬|é…’åº—|å­¦æ ¡|åŒ»é™¢|é¤å…|å’–å•¡å…))',
        r'((?:å®¢å…|å§å®¤|å¨æˆ¿|å«ç”Ÿé—´|é˜³å°|ä¹¦æˆ¿|é¤å…|ç„å…³)\w{0,4})',
        r'(ä¸‰ä»£åŒå ‚|äºŒèƒ|ç‹¬å±…|è€äºº|å­©å­|å® ç‰©)',
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text)
        keywords.extend(matches)
    
    return list(set(keywords))


def _build_system_prompt(is_regeneration: bool, user_keywords: List[str]) -> str:
    """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
    base_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é¡¹ç›®éœ€æ±‚é—®å·ç”Ÿæˆä¸“å®¶ã€‚

ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·çš„é¡¹ç›®æè¿°å’Œéœ€æ±‚åˆ†æç»“æœï¼Œç”Ÿæˆä¸€ä»½é«˜åº¦é’ˆå¯¹æ€§çš„æ¾„æ¸…é—®å·ã€‚

## é—®å·ç”ŸæˆåŸåˆ™

1. **é«˜åº¦é’ˆå¯¹æ€§**ï¼šæ¯ä¸ªé—®é¢˜å¿…é¡»ç›´æ¥å…³è”ç”¨æˆ·æè¿°ä¸­çš„å…·ä½“å†…å®¹
2. **å¼•ç”¨ç”¨æˆ·åŸè¯**ï¼šå°½é‡åœ¨é—®é¢˜ä¸­å¼•ç”¨ç”¨æˆ·ä½¿ç”¨çš„å…³é”®è¯ã€æ•°å­—ã€åœºæ™¯æè¿°
3. **æ¢ç´¢çŸ›ç›¾å†²çª**ï¼šå…³æ³¨éœ€æ±‚ä¸­å¯èƒ½å­˜åœ¨çš„çŸ›ç›¾æˆ–æƒè¡¡å–èˆ
4. **é¿å…æ³›åŒ–æ¨¡æ¿**ï¼šç¦æ­¢ä½¿ç”¨"æ‚¨å–œæ¬¢ä»€ä¹ˆé£æ ¼ï¼Ÿ"è¿™ç±»é€šç”¨é—®é¢˜

## è¾“å‡ºæ ¼å¼

è¿”å›JSONæ ¼å¼ï¼š
```json
{
    "questions": [
        {
            "id": "q1",
            "question": "é—®é¢˜å†…å®¹ï¼ˆå¿…é¡»åŒ…å«ç”¨æˆ·æè¿°ä¸­çš„å…³é”®è¯ï¼‰",
            "purpose": "é—®è¿™ä¸ªé—®é¢˜çš„ç›®çš„",
            "options": ["é€‰é¡¹A", "é€‰é¡¹B", "é€‰é¡¹C"],
            "allow_custom": true
        }
    ],
    "generation_rationale": "ç”Ÿæˆç†ç”±è¯´æ˜"
}
```"""

    if is_regeneration:
        keywords_str = "ã€".join(user_keywords[:10]) if user_keywords else "ç”¨æˆ·è¾“å…¥ä¸­çš„å…³é”®è¯"
        base_prompt += f"""

## âš ï¸ é‡æ–°ç”Ÿæˆæ³¨æ„äº‹é¡¹

ä¸Šä¸€æ¬¡ç”Ÿæˆçš„é—®é¢˜ç›¸å…³æ€§ä¸è¶³ã€‚è¿™æ¬¡å¿…é¡»ï¼š

1. **å¿…é¡»å¼•ç”¨å…³é”®è¯**ï¼š{keywords_str}
2. **æ¯ä¸ªé—®é¢˜å¿…é¡»åŒ…å«**è‡³å°‘ä¸€ä¸ªç”¨æˆ·åŸè¯ä¸­çš„å…³é”®è¯æˆ–æ•°å­—
3. **ç¦æ­¢ä½¿ç”¨é€šç”¨æ¨¡æ¿é—®é¢˜**

âš ï¸ å¦‚æœé—®é¢˜ä¸åŒ…å«ç”¨æˆ·è¾“å…¥çš„å…³é”®è¯ï¼Œå°†è¢«åˆ¤å®šä¸ºæ— æ•ˆé—®é¢˜ï¼"""

    return base_prompt


def _build_human_prompt(user_input: str, analysis_summary: str, is_regeneration: bool, user_keywords: List[str]) -> str:
    """æ„å»ºç”¨æˆ·æç¤ºè¯"""
    prompt = f"""## ç”¨æˆ·åŸå§‹è¾“å…¥

{user_input}

## éœ€æ±‚åˆ†ææ‘˜è¦

{analysis_summary}

## ä»»åŠ¡

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆ5-8ä¸ªé«˜åº¦é’ˆå¯¹æ€§çš„æ¾„æ¸…é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. æ¯ä¸ªé—®é¢˜å¿…é¡»ä¸ç”¨æˆ·æè¿°ç›´æ¥ç›¸å…³
2. é—®é¢˜ä¸­åº”å¼•ç”¨ç”¨æˆ·ä½¿ç”¨çš„å…·ä½“è¯æ±‡ã€æ•°å­—æˆ–åœºæ™¯
3. æ¢ç´¢ç”¨æˆ·å¯èƒ½é¢ä¸´çš„é€‰æ‹©å’Œæƒè¡¡
4. é¿å…è¿‡äºå®½æ³›çš„é€šç”¨é—®é¢˜"""

    if is_regeneration and user_keywords:
        prompt += f"""

âš ï¸ å¿…é¡»åœ¨é—®é¢˜ä¸­å¼•ç”¨çš„å…³é”®è¯ï¼š
{', '.join(user_keywords[:10])}

è¯·ç¡®ä¿æ¯ä¸ªé—®é¢˜è‡³å°‘åŒ…å«ä¸Šè¿°å…³é”®è¯ä¹‹ä¸€ï¼"""

    return prompt


def _parse_llm_response(raw_content: str) -> Dict[str, Any]:
    """è§£æLLMå“åº”"""
    try:
        # å°è¯•æå–JSONå—
        json_match = re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', raw_content)
        if json_match:
            raw_content = json_match.group(1)
        
        # æ¸…ç†å¯èƒ½çš„å‰åç¼€
        raw_content = raw_content.strip()
        if raw_content.startswith("```"):
            raw_content = raw_content[3:]
        if raw_content.endswith("```"):
            raw_content = raw_content[:-3]
        
        return json.loads(raw_content)
    except json.JSONDecodeError as e:
        logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
        return {"questions": []}


def _validate_questions(questions: List[Dict]) -> List[Dict]:
    """éªŒè¯å’Œä¿®å¤é—®é¢˜æ ¼å¼"""
    validated = []
    
    for i, q in enumerate(questions):
        if isinstance(q, dict) and q.get("question"):
            validated_q = {
                "id": q.get("id", f"q{i+1}"),
                "question": q.get("question", ""),
                "purpose": q.get("purpose", ""),
                "options": q.get("options", []),
                "allow_custom": q.get("allow_custom", True)
            }
            validated.append(validated_q)
    
    return validated


def _fallback_generate(user_input: str, structured_data: Dict[str, Any]) -> Dict[str, Any]:
    """å›é€€ç”Ÿæˆæ–¹æ¡ˆ"""
    logger.info("ğŸ“‹ ä½¿ç”¨å›é€€é—®å·ç”Ÿæˆæ–¹æ¡ˆ")
    
    # åŸºäºé¡¹ç›®ç±»å‹ç”ŸæˆåŸºç¡€é—®é¢˜
    project_type = structured_data.get("project_type", "personal_residential")
    
    questions = [
        {
            "id": "q1",
            "question": "æ‚¨å¯¹è¿™ä¸ªé¡¹ç›®æœ€çœ‹é‡çš„æ ¸å¿ƒç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ",
            "purpose": "æ˜ç¡®ä¼˜å…ˆçº§",
            "options": ["åŠŸèƒ½æ€§", "ç¾è§‚æ€§", "æˆæœ¬æ§åˆ¶", "æ—¶é—´æ•ˆç‡"],
            "allow_custom": True
        },
        {
            "id": "q2",
            "question": "åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œæ‚¨æ›´å€¾å‘äºä¼˜å…ˆä¿éšœå“ªä¸ªæ–¹é¢ï¼Ÿ",
            "purpose": "æƒè¡¡å–èˆ",
            "options": ["ç©ºé—´å°ºå¯¸", "ææ–™å“è´¨", "è®¾è®¡åˆ›æ„", "æ–½å·¥å‘¨æœŸ"],
            "allow_custom": True
        },
        {
            "id": "q3",
            "question": "æ‚¨å¯¹é¡¹ç›®çš„é¢„æœŸä½¿ç”¨å¹´é™æ˜¯å¤šä¹…ï¼Ÿ",
            "purpose": "äº†è§£é•¿æœŸè§„åˆ’",
            "options": ["çŸ­æœŸï¼ˆ1-3å¹´ï¼‰", "ä¸­æœŸï¼ˆ3-10å¹´ï¼‰", "é•¿æœŸï¼ˆ10å¹´ä»¥ä¸Šï¼‰", "æ°¸ä¹…ä½¿ç”¨"],
            "allow_custom": True
        }
    ]
    
    return {
        "questions": questions,
        "relevance_score": 0.5,
        "generation_source": "fallback",
        "generation_rationale": "LLMç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ",
        "processing_log": ["ğŸ“‹ ä½¿ç”¨å›é€€é—®å·ç”Ÿæˆæ–¹æ¡ˆ"]
    }


# ============================================================================
# çŠ¶æ€å›¾æ„å»º
# ============================================================================

def build_questionnaire_graph() -> StateGraph:
    """
    æ„å»ºé—®å·ç”ŸæˆçŠ¶æ€å›¾
    
    æµç¨‹:
        START â†’ extract_context â†’ generate_questions â†’ validate_relevance â†’ [æ¡ä»¶åˆ¤æ–­]
            - å¦‚æœç›¸å…³æ€§è¶³å¤Ÿ: â†’ END
            - å¦‚æœç›¸å…³æ€§ä¸è¶³: â†’ increment_regeneration â†’ generate_questions â†’ ...
    """
    workflow = StateGraph(QuestionnaireState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("extract_context", extract_context_node)
    workflow.add_node("generate_questions", generate_questions_node)
    workflow.add_node("validate_relevance", validate_relevance_node)
    workflow.add_node("increment_regeneration", increment_regeneration_node)
    
    # æ·»åŠ è¾¹
    workflow.add_edge(START, "extract_context")
    workflow.add_edge("extract_context", "generate_questions")
    workflow.add_edge("generate_questions", "validate_relevance")
    
    # æ¡ä»¶è¾¹ï¼šå†³å®šæ˜¯å¦é‡æ–°ç”Ÿæˆ
    workflow.add_conditional_edges(
        "validate_relevance",
        should_regenerate,
        {
            "regenerate": "increment_regeneration",
            "complete": END
        }
    )
    
    workflow.add_edge("increment_regeneration", "generate_questions")
    
    return workflow


# ============================================================================
# Agent å°è£…ç±»
# ============================================================================

class QuestionnaireAgent:
    """
    é—®å·ç”Ÿæˆæ™ºèƒ½ä½“ - LangGraph å°è£…
    
    ä½¿ç”¨æ–¹å¼:
        agent = QuestionnaireAgent(llm_model)
        result = agent.generate(user_input, structured_data)
    """
    
    def __init__(self, llm_model=None, config: Optional[Dict[str, Any]] = None):
        """åˆå§‹åŒ–é—®å·ç”Ÿæˆæ™ºèƒ½ä½“"""
        self.llm_model = llm_model
        self.config = config or {}
        self.max_regenerations = self.config.get("max_regenerations", 1)
        
        # æ„å»ºå¹¶ç¼–è¯‘çŠ¶æ€å›¾
        self._graph = build_questionnaire_graph().compile()
        
        logger.info("ğŸš€ QuestionnaireAgent (LangGraph) å·²åˆå§‹åŒ–")
    
    def generate(
        self, 
        user_input: str, 
        structured_data: Dict[str, Any]
    ) -> Tuple[List[Dict[str, Any]], str]:
        """
        ç”Ÿæˆé—®å·
        
        Args:
            user_input: ç”¨æˆ·åŸå§‹è¾“å…¥
            structured_data: éœ€æ±‚åˆ†æå¸ˆçš„ç»“æ„åŒ–è¾“å‡º
            
        Returns:
            Tuple[List[Dict], str]:
                - é—®é¢˜åˆ—è¡¨
                - ç”Ÿæˆæ¥æºæ ‡è¯†
        """
        logger.info("ğŸ¯ QuestionnaireAgent å¼€å§‹æ‰§è¡Œ")
        start_time = time.time()
        
        # å‡†å¤‡åˆå§‹çŠ¶æ€
        initial_state = {
            "user_input": user_input,
            "structured_data": structured_data,
            
            # é…ç½®
            "_llm_model": self.llm_model,
            "_max_regenerations": self.max_regenerations,
            "_regeneration_count": 0,
            
            # åˆå§‹åŒ–ä¸­é—´çŠ¶æ€
            "analysis_summary": "",
            "user_keywords": [],
            "raw_llm_response": "",
            
            # åˆå§‹åŒ–è¾“å‡º
            "questions": [],
            "relevance_score": 0.0,
            "low_relevance_questions": [],
            "generation_source": "",
            "generation_rationale": "",
            "processing_log": []
        }
        
        # æ‰§è¡ŒçŠ¶æ€å›¾
        try:
            final_state = self._graph.invoke(initial_state)
            
            questions = final_state.get("questions", [])
            source = final_state.get("generation_source", "unknown")
            
            logger.info(f"âœ… QuestionnaireAgent å®Œæˆ: {len(questions)} ä¸ªé—®é¢˜, æ¥æº: {source}")
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            PerformanceMonitor.record("QuestionnaireAgent", time.time() - start_time, "v7.16")
            
            return questions, source
            
        except Exception as e:
            # è®°å½•å¤±è´¥æ—¶çš„æ€§èƒ½æŒ‡æ ‡
            PerformanceMonitor.record("QuestionnaireAgent", time.time() - start_time, "v7.16-error")
            
            logger.error(f"âŒ QuestionnaireAgent æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            # ä½¿ç”¨å›é€€æ–¹æ¡ˆ
            fallback_result = _fallback_generate(user_input, structured_data)
            return fallback_result.get("questions", []), "fallback"
    
    def get_full_result(
        self, 
        user_input: str, 
        structured_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        è·å–å®Œæ•´ç»“æœï¼ˆåŒ…å«æ‰€æœ‰ä¸­é—´çŠ¶æ€ï¼‰
        
        Returns:
            å®Œæ•´çš„çŠ¶æ€å­—å…¸
        """
        initial_state = {
            "user_input": user_input,
            "structured_data": structured_data,
            "_llm_model": self.llm_model,
            "_max_regenerations": self.max_regenerations,
            "_regeneration_count": 0,
            "analysis_summary": "",
            "user_keywords": [],
            "raw_llm_response": "",
            "questions": [],
            "relevance_score": 0.0,
            "low_relevance_questions": [],
            "generation_source": "",
            "generation_rationale": "",
            "processing_log": []
        }
        
        try:
            return self._graph.invoke(initial_state)
        except Exception as e:
            logger.error(f"âŒ QuestionnaireAgent æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "questions": [],
                "relevance_score": 0.0,
                "generation_source": "error",
                "error": str(e)
            }


# ============================================================================
# å‘åå…¼å®¹å±‚
# ============================================================================

class LLMQuestionGeneratorCompat:
    """
    å‘åå…¼å®¹åŸ LLMQuestionGenerator æ¥å£
    """
    
    @classmethod
    def generate(
        cls,
        user_input: str,
        structured_data: Dict[str, Any],
        llm_model: Optional[Any] = None,
        timeout: int = 30
    ) -> Tuple[List[Dict[str, Any]], str]:
        """å…¼å®¹åŸ LLMQuestionGenerator.generate æ¥å£"""
        agent = QuestionnaireAgent(llm_model)
        return agent.generate(user_input, structured_data)
