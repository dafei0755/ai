"""
ä¸“ä¸šæ™ºèƒ½ä½“å·¥å‚ - Specialized Agent Factory

æ ¹æ®è§’è‰²é…ç½®åŠ¨æ€åˆ›å»ºä¸“ä¸šæ™ºèƒ½ä½“ã€‚
Dynamically creates specialized agents based on role configurations.
"""

from typing import Dict, Any, Callable
try:
    from langgraph.prebuilt import create_react_agent
except ImportError:
    # å°è¯•å¤„ç† langgraph å‘½åç©ºé—´åŒ…é—®é¢˜
    import sys
    from loguru import logger
    logger.warning("Failed to import langgraph.prebuilt directly. Attempting workarounds...")
    try:
        # å°è¯•å…ˆå¯¼å…¥ langgraph
        import langgraph
        from langgraph.prebuilt import create_react_agent
    except ImportError as e:
        logger.error(f"Failed to import create_react_agent: {e}")
        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        try:
            import langgraph
            logger.error(f"langgraph path: {getattr(langgraph, '__path__', 'unknown')}")
        except:
            pass
        raise

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage  # ğŸ†• N3ä¼˜åŒ–ï¼šæ·»åŠ é‡è¯•æ¶ˆæ¯ç±»å‹
from intelligent_project_analyzer.core.role_manager import RoleManager


class SpecializedAgentFactory:
    """
    ä¸“ä¸šæ™ºèƒ½ä½“å·¥å‚

    åŠŸèƒ½:
    1. æ ¹æ®è§’è‰²é…ç½®åŠ¨æ€åˆ›å»ºæ™ºèƒ½ä½“èŠ‚ç‚¹
    2. åŠ è½½å®¢æˆ·è‡ªå®šä¹‰çš„ system_prompt
    3. åŠ¨æ€æ„å»ºåŒ…å«é¡¹ç›®éœ€æ±‚å’Œå…¶ä»–ä¸“å®¶ç»“æœçš„å®Œæ•´ä¸Šä¸‹æ–‡
    4. æ”¯æŒ Dynamic Mode çš„å¹¶è¡Œæ‰§è¡Œ

    ä¸»è¦æ–¹æ³•:
    - create_simple_agent_node: åˆ›å»ºè½»é‡çº§æ™ºèƒ½ä½“èŠ‚ç‚¹ï¼ˆå½“å‰ä½¿ç”¨ï¼‰
    - create_agent: åˆ›å»º ReAct æ™ºèƒ½ä½“ï¼ˆä¿ç•™ç”¨äºæœªæ¥æ‰©å±•ï¼‰
    """
    
    @staticmethod
    def create_agent(
        role_id: str,
        role_config: Dict[str, Any],
        llm_model,
        tools: list = None
    ):
        """
        åˆ›å»ºä¸“ä¸šæ™ºèƒ½ä½“
        
        Args:
            role_id: è§’è‰²ID
            role_config: è§’è‰²é…ç½®
            llm_model: LLMæ¨¡å‹å®ä¾‹
            tools: å¯ç”¨å·¥å…·åˆ—è¡¨
        
        Returns:
            åˆ›å»ºçš„ReActæ™ºèƒ½ä½“
        """
        # è·å–system_prompt
        system_prompt = role_config.get("system_prompt", "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ™ºèƒ½ä½“åŠ©æ‰‹ã€‚")
        
        # åˆ›å»ºReActæ™ºèƒ½ä½“
        agent = create_react_agent(
            llm_model,
            tools or [],
            state_modifier=system_prompt
        )
        
        return agent
    
    @staticmethod
    def create_simple_agent_node(
        role_id: str,
        role_config: Dict[str, Any],
        llm_model
    ) -> Callable:
        """
        åˆ›å»ºç®€å•çš„æ™ºèƒ½ä½“èŠ‚ç‚¹(ä¸ä½¿ç”¨å·¥å…·)
        
        Args:
            role_id: è§’è‰²ID
            role_config: è§’è‰²é…ç½®
            llm_model: LLMæ¨¡å‹å®ä¾‹
        
        Returns:
            æ™ºèƒ½ä½“èŠ‚ç‚¹å‡½æ•°
        """
        # è·å–system_prompt
        system_prompt = role_config.get("system_prompt", "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ™ºèƒ½ä½“åŠ©æ‰‹ã€‚")
        
        # å®šä¹‰èŠ‚ç‚¹å‡½æ•°
        def simple_agent_node(state: Dict) -> Dict:
            """
            ç®€å•æ™ºèƒ½ä½“èŠ‚ç‚¹å‡½æ•°

            Args:
                state: å›¾çŠ¶æ€

            Returns:
                çŠ¶æ€æ›´æ–°
            """
            import json
            from loguru import logger

            # âœ… è·å–å®Œæ•´çš„é¡¹ç›®ä¿¡æ¯
            requirements = state.get("structured_requirements", {})
            task_description = state.get("task_description", "")
            agent_results = state.get("agent_results", {})
            messages = state.get("messages", [])
            
            # âœ… æ£€æŸ¥æ˜¯å¦ä¸ºé‡æ–°æ‰§è¡Œå¹¶è·å–å®¡æ ¸åé¦ˆ
            is_rerun = state.get("is_rerun", False)
            review_feedback_for_agent = state.get("review_feedback_for_agent", {})

            # ğŸ”§ æ›¿æ¢æç¤ºè¯ä¸­çš„å ä½ç¬¦ {user_specific_request}
            # ä¼˜å…ˆä½¿ç”¨è§’è‰²ç‰¹å®šä»»åŠ¡ï¼Œå¦åˆ™ä½¿ç”¨é€šç”¨ä»»åŠ¡æè¿°
            role_specific_task = state.get(f"{role_id}_task", "")  # å¦‚æœæœ‰è§’è‰²ç‰¹å®šä»»åŠ¡
            user_specific_request = role_specific_task or task_description or "å®Œæˆæ‚¨æ‰€æ“…é•¿é¢†åŸŸçš„ä¸“ä¸šåˆ†æ"
            
            # æ›¿æ¢å ä½ç¬¦
            processed_prompt = system_prompt.replace("{user_specific_request}", user_specific_request)

            # âœ… æ„å»ºåŒ…å«é¡¹ç›®éœ€æ±‚çš„æç¤ºè¯
            prompt_parts = [processed_prompt]

            # æ·»åŠ é¡¹ç›®éœ€æ±‚ä¿¡æ¯
            if requirements:
                prompt_parts.append("\n---\n\n## é¡¹ç›®éœ€æ±‚ä¿¡æ¯\n\n")
                prompt_parts.append(json.dumps(requirements, ensure_ascii=False, indent=2))

            # æ·»åŠ ä»»åŠ¡æè¿°
            if task_description:
                prompt_parts.append("\n---\n\n## ä½ çš„ä»»åŠ¡\n\n")
                prompt_parts.append(task_description)

            # æ·»åŠ å…¶ä»–ä¸“å®¶çš„åˆ†æç»“æœ
            if agent_results:
                prompt_parts.append("\n---\n\n## å…¶ä»–ä¸“å®¶çš„åˆ†æç»“æœï¼ˆä¾›å‚è€ƒï¼‰\n\n")
                for aid, result in agent_results.items():
                    analysis = result.get("analysis", "")
                    if analysis:
                        # åªæ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦ï¼Œé¿å…æç¤ºè¯è¿‡é•¿
                        prompt_parts.append(f"\n### {aid}\n{analysis[:200]}...\n")
            
            # âœ… æ–°å¢ï¼šå¦‚æœæ˜¯é‡æ–°æ‰§è¡Œï¼Œé™„åŠ å®¡æ ¸åé¦ˆ
            if is_rerun and review_feedback_for_agent:
                specific_tasks = review_feedback_for_agent.get("specific_tasks", [])
                iteration_context = review_feedback_for_agent.get("iteration_context", {})
                avoid_changes_to = review_feedback_for_agent.get("avoid_changes_to", [])
                
                if specific_tasks or iteration_context:
                    logger.info(f"ğŸ“ {role_id} æ”¶åˆ°å®¡æ ¸åé¦ˆï¼š{len(specific_tasks)} ä¸ªæ”¹è¿›ä»»åŠ¡")
                    
                    prompt_parts.append("\n" + "="*60 + "\n")
                    prompt_parts.append("âš ï¸ è¿™æ˜¯é‡æ–°æ‰§è¡Œè½®æ¬¡ - è¯·æ ¹æ®å®¡æ ¸åé¦ˆæ”¹è¿›\n")
                    prompt_parts.append("="*60 + "\n\n")
                    
                    # æ·»åŠ è¿­ä»£ä¸Šä¸‹æ–‡
                    if iteration_context:
                        round_num = iteration_context.get("round", 1)
                        prompt_parts.append(f"## è¿­ä»£è½®æ¬¡ï¼šç¬¬ {round_num} è½®\n\n")
                        
                        previous_summary = iteration_context.get("previous_output_summary", "")
                        if previous_summary:
                            prompt_parts.append(f"### ä¸Šä¸€è½®è¾“å‡ºæ‘˜è¦\n{previous_summary}\n\n")
                        
                        what_worked = iteration_context.get("what_worked_well", [])
                        if what_worked:
                            prompt_parts.append("### âœ… ä¸Šä¸€è½®åšå¾—å¥½çš„æ–¹é¢ï¼ˆä¿æŒï¼‰\n")
                            for item in what_worked:
                                prompt_parts.append(f"- {item}\n")
                            prompt_parts.append("\n")
                        
                        what_needs_improvement = iteration_context.get("what_needs_improvement", [])
                        if what_needs_improvement:
                            prompt_parts.append("### âš ï¸ éœ€è¦æ”¹è¿›çš„æ–¹é¢\n")
                            for item in what_needs_improvement:
                                prompt_parts.append(f"- {item}\n")
                            prompt_parts.append("\n")
                    
                    # æ·»åŠ å…·ä½“æ”¹è¿›ä»»åŠ¡
                    if specific_tasks:
                        prompt_parts.append("## ğŸ¯ å…·ä½“æ”¹è¿›ä»»åŠ¡æ¸…å•\n\n")
                        prompt_parts.append("è¯·é€ä¸€è§£å†³ä»¥ä¸‹é—®é¢˜ï¼Œç¡®ä¿æœ¬æ¬¡è¾“å‡ºè´¨é‡æ˜¾è‘—æå‡ï¼š\n\n")
                        
                        for task in specific_tasks:
                            task_id = task.get("task_id", "")
                            priority = task.get("priority", "medium")
                            instruction = task.get("instruction", "")
                            example = task.get("example", "")
                            validation = task.get("validation", "")
                            
                            priority_icon = "ğŸ”´" if priority == "high" else "ğŸŸ¡" if priority == "medium" else "ğŸŸ¢"
                            
                            prompt_parts.append(f"### {priority_icon} ä»»åŠ¡ {task_id} ({priority.upper()})\n\n")
                            prompt_parts.append(f"**è¦æ±‚**: {instruction}\n\n")
                            if example:
                                prompt_parts.append(f"**ç¤ºä¾‹**: {example}\n\n")
                            if validation:
                                prompt_parts.append(f"**éªŒè¯**: {validation}\n\n")
                            prompt_parts.append("---\n\n")
                    
                    # æ·»åŠ ä¿æŒä¸å˜çš„å†…å®¹
                    if avoid_changes_to:
                        prompt_parts.append("## ğŸ”’ ä»¥ä¸‹æ–¹é¢å·²ç»è‰¯å¥½ï¼Œæ— éœ€æ”¹åŠ¨\n\n")
                        for item in avoid_changes_to:
                            prompt_parts.append(f"- {item}\n")
                        prompt_parts.append("\n")
                    
                    prompt_parts.append("="*60 + "\n")
                    prompt_parts.append("ğŸ¯ æœ¬æ¬¡åˆ†æè¦æ±‚ï¼šé’ˆå¯¹ä¸Šè¿°æ”¹è¿›ä»»åŠ¡é€ä¸€è§£å†³ï¼Œæä¾›æ›´é«˜è´¨é‡çš„åˆ†æç»“æœ\n")
                    prompt_parts.append("="*60 + "\n\n")

            prompt_parts.append("\n---\n")
            
            # ğŸ”§ N3ä¼˜åŒ–ï¼šå¼ºåŒ–v3.5åè®®è¯´æ˜ï¼Œé†’ç›®æç¤ºå¿…å¡«å­—æ®µ
            prompt_parts.append("\n" + "="*80 + "\n")
            prompt_parts.append("ğŸš¨ **CRITICAL v3.5 EXPERT AUTONOMY PROTOCOL** ğŸš¨\n")
            prompt_parts.append("="*80 + "\n\n")
            
            prompt_parts.append("ğŸ“‹ **MANDATORY JSON FIELDS** (failure = automatic retry):\n\n")
            prompt_parts.append("1. âœ… expert_handoff_response (REQUIRED):\n")
            prompt_parts.append("   {\n")
            prompt_parts.append('     "critical_questions_responses": {"q1_...": "your answer", ...},\n')
            prompt_parts.append('     "chosen_design_stance": "your choice"\n')
            prompt_parts.append("   }\n\n")
            
            prompt_parts.append("2. âœ… design_rationale OR decision_rationale (REQUIRED):\n")
            prompt_parts.append("   Explain WHY you made your design decisions\n\n")
            
            prompt_parts.append("3. âœ… challenge_flags (REQUIRED):\n")
            prompt_parts.append("   - If you disagree with requirements analyst: provide challenges\n")
            prompt_parts.append("   - If you agree: use empty array []\n\n")
            
            prompt_parts.append("ğŸ”§ **OUTPUT FORMAT REQUIREMENTS**:\n")
            prompt_parts.append("- DO NOT use markdown code fences (no ```json or ```).\n")
            prompt_parts.append("- START with { and END with }\n")
            prompt_parts.append("- Include ALL 3 mandatory fields above\n\n")
            
            # ğŸŒ v3.5.1: æ·»åŠ ä¸­æ–‡è¾“å‡ºè¦æ±‚
            prompt_parts.append("ğŸŒ **OUTPUT LANGUAGE REQUIREMENTS (CRITICAL)**:\n")
            prompt_parts.append("- æ‰€æœ‰å­—æ®µå€¼ï¼ˆJSON valueï¼‰å¿…é¡»ä½¿ç”¨ç®€ä½“ä¸­æ–‡\n")
            prompt_parts.append("- åˆ†æå†…å®¹ã€å»ºè®®ã€æè¿°ã€åç§°ç­‰å¿…é¡»å…¨éƒ¨ç”¨ä¸­æ–‡æ’°å†™\n")
            prompt_parts.append("- ä¸“ä¸šæœ¯è¯­å¯ä¿ç•™è‹±æ–‡ï¼Œä½†éœ€é™„å¸¦ä¸­æ–‡è§£é‡Šï¼ˆå¦‚ï¼šWabi-Sabi/ä¾˜å¯‚ï¼‰\n")
            prompt_parts.append("- âŒ ç¦æ­¢è¾“å‡ºè‹±æ–‡åˆ†æå†…å®¹\n")
            prompt_parts.append("- âŒ ç¦æ­¢è¾“å‡ºæœªç¿»è¯‘çš„è‹±æ–‡ç­–ç•¥æˆ–å»ºè®®\n\n")
            
            prompt_parts.append("âš ï¸ Non-compliant outputs trigger automatic retry (max 1 retry).\n")
            prompt_parts.append("="*80 + "\n\n")

            complete_prompt = "".join(prompt_parts)

            # æ„å»ºæ¶ˆæ¯
            full_messages = [SystemMessage(content=complete_prompt)] + messages

            # è°ƒç”¨LLM
            response = llm_model.invoke(full_messages)
            
            # âœ… v3.5: éªŒè¯ä¸“å®¶ä¸»åŠ¨æ€§åè®®æ‰§è¡Œæƒ…å†µ
            result_content = response.content
            
            # ğŸ”§ N3ä¼˜åŒ–ï¼šå¢åŠ åè®®éµå®ˆç‡æ£€æŸ¥ï¼Œæ”¯æŒè‡ªåŠ¨é‡è¯•
            import json
            parsed_result = {}
            json_parse_failed = False
            protocol_violations = []
            retry_count = state.get(f"protocol_retry_{role_id}", 0)  # ğŸ†• åè®®é‡è¯•è®¡æ•°
            max_protocol_retries = 1  # æœ€å¤šé‡è¯•1æ¬¡
            
            try:
                # æŸ¥æ‰¾ JSON å—
                if "```json" in result_content:
                    json_start = result_content.find("```json") + 7
                    json_end = result_content.find("""`""", json_start)
                    json_str = result_content[json_start:json_end].strip()
                elif "{" in result_content and "}" in result_content:
                    json_str = result_content[result_content.find("{"):result_content.rfind("}")+1]
                else:
                    logger.error(f"âŒ [v3.5 PROTOCOL VIOLATION] {role_id} è¾“å‡ºä¸åŒ…å«JSONå¯¹è±¡")
                    json_parse_failed = True
                    json_str = "{}"
                
                if not json_parse_failed:
                    parsed_result = json.loads(json_str)
                    logger.info(f"âœ… [v3.5 Protocol] {role_id} JSONè§£ææˆåŠŸ")
                
                # ğŸ”§ P2ä¿®å¤ï¼šç»Ÿè®¡åè®®éµå®ˆæƒ…å†µ
                protocol_violations = []
                
                # æ£€æŸ¥1: expert_handoff_responseï¼ˆå›åº” critical_questionsï¼‰
                if "expert_handoff_response" in parsed_result:
                    handoff_response = parsed_result["expert_handoff_response"]
                    if isinstance(handoff_response, dict) and handoff_response:
                        # æ£€æŸ¥æ˜¯å¦çœŸæ­£å›ç­”äº†é—®é¢˜
                        has_responses = any(key in handoff_response for key in [
                            "critical_questions_responses", "answered_questions", "chosen_design_stance"
                        ])
                        if has_responses:
                            logger.info(f"âœ… [v3.5 Protocol] {role_id} åŒ…å«æœ‰æ•ˆçš„ expert_handoff_response")
                        else:
                            protocol_violations.append("expert_handoff_responseå­—æ®µå­˜åœ¨ä½†å†…å®¹ä¸ºç©º")
                            logger.warning(f"âš ï¸ [v3.5 VIOLATION] {role_id} çš„ expert_handoff_response æ— æœ‰æ•ˆå†…å®¹")
                    else:
                        protocol_violations.append("expert_handoff_responseå­—æ®µæ— æ•ˆ")
                        logger.warning(f"âš ï¸ [v3.5 VIOLATION] {role_id} çš„ expert_handoff_response ä¸ºç©ºæˆ–æ— æ•ˆ")
                else:
                    protocol_violations.append("ç¼ºå°‘expert_handoff_responseå­—æ®µ")
                    logger.error(f"âŒ [v3.5 VIOLATION] {role_id} æœªåŒ…å« expert_handoff_response å­—æ®µ")
                
                # æ£€æŸ¥2: challenge_flagsï¼ˆä¸“å®¶æŒ‘æˆ˜ï¼‰
                if "challenge_flags" in parsed_result:
                    challenge_flags = parsed_result["challenge_flags"]
                    if isinstance(challenge_flags, list) and challenge_flags:
                        logger.warning(f"ğŸ”¥ [v3.5 Protocol] {role_id} æå‡ºäº† {len(challenge_flags)} ä¸ªæŒ‘æˆ˜æ ‡è®°")
                        for i, challenge in enumerate(challenge_flags, 1):
                            # ğŸ”§ P0ä¿®å¤: æ£€æŸ¥challengeæ˜¯å¦ä¸ºå­—å…¸ç±»å‹
                            if isinstance(challenge, dict):
                                challenged_item = challenge.get("challenged_item", "æœªçŸ¥é¡¹ç›®")
                                logger.warning(f"   ğŸ”¥ æŒ‘æˆ˜ {i}: {challenged_item}")
                            elif isinstance(challenge, str):
                                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²å†…å®¹
                                logger.warning(f"   ğŸ”¥ æŒ‘æˆ˜ {i}: {challenge}")
                            else:
                                # å…¶ä»–ç±»å‹ï¼Œè½¬ä¸ºå­—ç¬¦ä¸²
                                logger.warning(f"   ğŸ”¥ æŒ‘æˆ˜ {i}: {str(challenge)}")
                    else:
                        logger.debug(f"â„¹ï¸ [v3.5 Protocol] {role_id} æ¥å—éœ€æ±‚åˆ†æå¸ˆçš„æ´å¯Ÿï¼ˆæ— æŒ‘æˆ˜ï¼‰")
                else:
                    logger.debug(f"â„¹ï¸ [v3.5 Protocol] {role_id} æœªåŒ…å« challenge_flags å­—æ®µï¼ˆé»˜è®¤æ¥å—ï¼‰")
                
                # æ£€æŸ¥3: design_rationale/decision_rationaleï¼ˆè®¾è®¡ç«‹åœºè§£é‡Šï¼‰
                has_rationale = any(key in parsed_result for key in [
                    "design_rationale", "decision_rationale", "decision_logic"
                ])
                if not has_rationale:
                    protocol_violations.append("ç¼ºå°‘design_rationaleå­—æ®µ")
                    logger.error(f"âŒ [v3.5 VIOLATION] {role_id} æœªæ˜ç¡®è§£é‡Šè®¾è®¡ç«‹åœºï¼ˆç¼ºå°‘ design_rationale/decision_rationaleï¼‰")
                
                # ğŸ”§ P2ä¿®å¤ï¼šæ±‡æ€»è¿è§„æƒ…å†µ
                if protocol_violations:
                    logger.error(f"\nğŸ“Š [v3.5 COMPLIANCE] {role_id} åè®®éµå®ˆç‡: {len(protocol_violations)} ä¸ªè¿è§„é¡¹")
                    for i, violation in enumerate(protocol_violations, 1):
                        logger.error(f"   {i}. {violation}")
                    
                    # ğŸ†• N3ä¼˜åŒ–: å¦‚æœè¿è§„ä¸”æœªè¾¾åˆ°é‡è¯•ä¸Šé™ï¼Œè§¦å‘é‡è¯•
                    if retry_count < max_protocol_retries:
                        logger.warning(f"ğŸ”„ [v3.5 RETRY] {role_id} è§¦å‘åè®®éµå®ˆé‡è¯• ({retry_count + 1}/{max_protocol_retries})")
                        
                        # æ„å»ºé‡è¯•æç¤º
                        retry_prompt = f"""
ğŸš¨ CRITICAL: Your previous response violated the v3.5 Expert Autonomy Protocol.

Missing fields:
{chr(10).join(f'  - {v}' for v in protocol_violations)}

YOU MUST include these fields in your JSON response:
1. expert_handoff_response: {{
     "critical_questions_responses": {{"q1_...": "your answer", ...}},
     "chosen_design_stance": "your choice"
   }}
2. design_rationale or decision_rationale: "explain your design decisions"
3. challenge_flags: [] (empty array if you accept requirements analyst's insights)

Please regenerate your complete response with ALL required fields.
Start directly with the JSON object (no markdown, no explanations).
"""
                        
                        # æ·»åŠ é‡è¯•æç¤ºåˆ°messages
                        retry_messages = full_messages + [
                            AIMessage(content=result_content),
                            HumanMessage(content=retry_prompt)
                        ]
                        
                        # æ›´æ–°é‡è¯•è®¡æ•°
                        state[f"protocol_retry_{role_id}"] = retry_count + 1
                        
                        # é‡æ–°è°ƒç”¨LLM
                        logger.info(f"ğŸ”„ é‡æ–°è°ƒç”¨LLMè¿›è¡Œåè®®ä¿®æ­£...")
                        response = llm_model.invoke(retry_messages)
                        result_content = response.content
                        
                        # é‡æ–°è§£æ
                        try:
                            if "```json" in result_content:
                                json_start = result_content.find("```json") + 7
                                json_end = result_content.find("```", json_start)
                                json_str = result_content[json_start:json_end].strip()
                            elif "{" in result_content and "}" in result_content:
                                json_str = result_content[result_content.find("{"):result_content.rfind("}")+1]
                            else:
                                json_str = "{}"
                            
                            parsed_result = json.loads(json_str)
                            logger.info(f"âœ… [v3.5 RETRY] {role_id} é‡è¯•åJSONè§£ææˆåŠŸ")
                            
                            # é‡æ–°æ£€æŸ¥åè®®ï¼ˆç®€åŒ–ç‰ˆï¼‰
                            retry_violations = []
                            if "expert_handoff_response" not in parsed_result:
                                retry_violations.append("expert_handoff_response")
                            if not any(k in parsed_result for k in ["design_rationale", "decision_rationale", "decision_logic"]):
                                retry_violations.append("design_rationale")
                            
                            if retry_violations:
                                logger.error(f"âŒ [v3.5 RETRY] {role_id} é‡è¯•åä»è¿è§„: {retry_violations}")
                            else:
                                logger.info(f"âœ… [v3.5 RETRY] {role_id} é‡è¯•æˆåŠŸï¼Œåè®®å®Œå…¨éµå®ˆï¼")
                        except Exception as retry_error:
                            logger.error(f"âŒ [v3.5 RETRY] {role_id} é‡è¯•è§£æå¤±è´¥: {retry_error}")
                else:
                    logger.info(f"âœ… [v3.5 COMPLIANCE] {role_id} å®Œå…¨éµå®ˆv3.5ä¸“å®¶ä¸»åŠ¨æ€§åè®®")
                    
            except (json.JSONDecodeError, ValueError, KeyError) as e:
                logger.error(f"âŒ [v3.5 PROTOCOL VIOLATION] {role_id} JSONè§£æå¤±è´¥: {e}")
                logger.error(f"   è¾“å‡ºé¢„è§ˆ: {result_content[:200]}...")
                json_parse_failed = True
                parsed_result = {}  # è§£æå¤±è´¥æ—¶ä½¿ç”¨ç©ºå­—å…¸

            # ğŸ”§ P0ä¿®å¤ï¼šè¿”å›parsed_resultä¾›æŒ‘æˆ˜æ£€æµ‹ä½¿ç”¨
            return {
                "messages": [response],
                "role_results": [{
                    "role_id": role_id,
                    "role_name": role_config.get("name", "æœªçŸ¥è§’è‰²"),
                    "result": response.content,
                    "parsed_result": parsed_result  # ğŸ†• ä¿å­˜è§£æåçš„ç»“æ„åŒ–æ•°æ®
                }]
            }
        
        return simple_agent_node


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    from langchain_openai import ChatOpenAI
    from intelligent_project_analyzer.core.role_manager import RoleManager
    import os

    # åˆå§‹åŒ– - ä½¿ç”¨ OpenAI Official API
    llm = ChatOpenAI(
        model="gpt-4.1",
        temperature=0,
        api_key=os.getenv("OPENAI_API_KEY"),
    )
    role_manager = RoleManager()
    
    # è·å–ä¸€ä¸ªè§’è‰²é…ç½®
    role_config = role_manager.get_role_config("V2_è®¾è®¡æ€»ç›‘", "2-1")
    
    if role_config:
        # åˆ›å»ºæ™ºèƒ½ä½“èŠ‚ç‚¹
        agent_node = SpecializedAgentFactory.create_simple_agent_node(
            "V2_è®¾è®¡æ€»ç›‘_2-1",
            role_config,
            llm
        )
        
        # æµ‹è¯•èŠ‚ç‚¹
        test_state = {
            "messages": [
                {"role": "user", "content": "è¯·åˆ†æè¿™ä¸ªå»ºç­‘è®¾è®¡é¡¹ç›®çš„ç©ºé—´è§„åˆ’"}
            ]
        }
        
        result = agent_node(test_state)
        print("æ™ºèƒ½ä½“å“åº”:")
        print(result["role_results"][0]["result"])

