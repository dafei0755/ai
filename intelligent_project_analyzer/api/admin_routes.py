"""
ç®¡ç†å‘˜åå° API è·¯ç”±
æä¾›ç³»ç»Ÿç›‘æ§ã€é…ç½®ç®¡ç†ã€ä¼šè¯ç®¡ç†ã€æ—¥å¿—æŸ¥è¯¢ç­‰åŠŸèƒ½

ä»…é™ç®¡ç†å‘˜è®¿é—®
"""

import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional

import psutil
from cachetools import TTLCache
from fastapi import APIRouter, Depends, HTTPException, Query
from fastapi.responses import JSONResponse
from loguru import logger

from ..services.redis_session_manager import RedisSessionManager
from ..utils.config_manager import config_manager
from .auth_middleware import require_admin
from .performance_monitor import performance_monitor

router = APIRouter(prefix="/api/admin", tags=["admin"])

# APIå“åº”ç¼“å­˜ï¼ˆ5ç§’TTLï¼Œé¿å…é¢‘ç¹è®¡ç®—ï¼‰
metrics_cache = TTLCache(maxsize=10, ttl=5)

# å…¨å±€ä¼šè¯ç®¡ç†å™¨å®ä¾‹ - åœ¨è·¯ç”±å‡½æ•°ä¸­åŠ¨æ€è·å–
session_manager = None  # å°†åœ¨è¯·æ±‚æ—¶ä» server.py è·å–


# ============================================================================
# ç³»ç»Ÿç›‘æ§ API
# ============================================================================


@router.get("/metrics/summary")
async def get_metrics_summary(admin: dict = Depends(require_admin)):
    """
    è·å–ç³»ç»Ÿç›‘æ§æ‘˜è¦

    è¿”å›ï¼š
    - CPUä½¿ç”¨ç‡
    - å†…å­˜ä½¿ç”¨ç‡
    - æ´»è·ƒä¼šè¯æ•°
    - è¯·æ±‚ç»Ÿè®¡

    ç¼“å­˜ç­–ç•¥ï¼š5ç§’å†…é‡å¤è¯·æ±‚è¿”å›ç¼“å­˜
    """
    cache_key = "metrics_summary"

    # æ£€æŸ¥ç¼“å­˜
    if cache_key in metrics_cache:
        logger.debug("ğŸ“¦ è¿”å›ç¼“å­˜çš„ç›‘æ§æ•°æ®")
        return metrics_cache[cache_key]

    try:
        # è®¡ç®—å®æ—¶æŒ‡æ ‡ - å¢åŠ å®¹é”™å¤„ç†å’Œè¯Šæ–­æ—¥å¿—
        cpu_percent = 0.0
        try:
            cpu_percent = psutil.cpu_percent(interval=0.1)
            logger.debug(f"âœ… CPU: {cpu_percent}%")
        except Exception as e:
            logger.warning(f"âš ï¸ CPUç›‘æ§å¤±è´¥: {type(e).__name__}")

        memory_info = {"percent": 0, "used": 0, "total": 0}
        try:
            mem = psutil.virtual_memory()
            memory_info = {"percent": mem.percent, "used": mem.used, "total": mem.total}
            logger.debug(f"âœ… å†…å­˜: {mem.percent}%")
        except Exception as e:
            logger.warning(f"âš ï¸ å†…å­˜ç›‘æ§å¤±è´¥: {type(e).__name__}")

        disk_info = {"percent": 0, "used": 0, "total": 0}
        try:
            # ğŸ”§ v7.122: å¢å¼ºWindowså…¼å®¹æ€§ï¼ˆPython 3.13 + psutilä¿®å¤ï¼‰
            import os
            import platform

            if platform.system() == "Windows":
                # å°è¯•å¤šç§è·¯å¾„æ ¼å¼
                disk_path_candidates = [
                    os.path.abspath(os.sep),  # å½“å‰é©±åŠ¨å™¨æ ¹ç›®å½•
                    "C:\\",  # Windowsç³»ç»Ÿç›˜
                    "C:/",  # å¤‡ç”¨æ ¼å¼
                ]

                disk = None
                for path in disk_path_candidates:
                    try:
                        disk = psutil.disk_usage(path)
                        logger.debug(f"âœ… ç£ç›˜è·¯å¾„æˆåŠŸ: {path}")
                        break
                    except Exception:
                        continue

                if disk is None:
                    raise RuntimeError("æ‰€æœ‰ç£ç›˜è·¯å¾„å°è¯•å‡å¤±è´¥")
            else:
                disk = psutil.disk_usage("/")

            disk_info = {"percent": disk.percent, "used": disk.used, "total": disk.total}
            logger.debug(f"âœ… ç£ç›˜: {disk.percent}%")
        except Exception as e:
            logger.warning(f"âš ï¸ ç£ç›˜ç›‘æ§å¤±è´¥: {type(e).__name__} - {str(e)[:50]}")
            # é™çº§å¤„ç†ï¼šè¿”å›Noneï¼Œå‰ç«¯æ˜¾ç¤º"ä¸å¯ç”¨"
            disk_info = None

        # æ´»è·ƒä¼šè¯æ•° (ä»Redisè·å–æ‰€æœ‰ä¼šè¯)
        active_sessions = 0
        try:
            # ä» server.py å¯¼å…¥å…¨å±€ session_manager
            from ..api.server import session_manager as global_session_manager

            if global_session_manager:
                all_sessions = await global_session_manager.get_all_sessions()
                active_sessions = len([s for s in all_sessions if s.get("status") == "active"])
                logger.debug(f"âœ… æ´»è·ƒä¼šè¯: {active_sessions}")
            else:
                logger.warning("âš ï¸ session_manager æœªåˆå§‹åŒ–")
        except Exception as e:
            logger.warning(f"âš ï¸ ä¼šè¯ç›‘æ§å¤±è´¥: {type(e).__name__}")

        # æ€§èƒ½æŒ‡æ ‡
        perf_stats = performance_monitor.get_stats_summary()
        logger.debug(f"âœ… æ€§èƒ½ç»Ÿè®¡: {perf_stats}")

        result = {
            "system": {
                "cpu_percent": round(cpu_percent, 2),
                "memory_percent": round(memory_info["percent"], 2),
                "memory_used_gb": round(memory_info["used"] / (1024**3), 2),
                "memory_total_gb": round(memory_info["total"] / (1024**3), 2),
                "disk_percent": round(disk_info["percent"], 2),
                "disk_used_gb": round(disk_info["used"] / (1024**3), 2),
                "disk_total_gb": round(disk_info["total"] / (1024**3), 2),
            },
            "sessions": {
                "active_count": active_sessions,
            },
            "performance": {
                "total_requests": perf_stats.get("total_requests", 0),
                "avg_response_time": perf_stats.get("avg_response_time", 0),
                "requests_per_minute": perf_stats.get("requests_per_minute", 0),
                "error_count": perf_stats.get("error_count", 0),
            },
            "timestamp": datetime.now().isoformat(),
        }

        # å†™å…¥ç¼“å­˜
        metrics_cache[cache_key] = result
        return result

    except Exception as e:
        # æåº¦ä¿å®ˆçš„é”™è¯¯å¤„ç†ï¼Œé¿å… SystemError å¯¼è‡´ 500
        logger.error("è·å–ç›‘æ§æ•°æ®å‘ç”ŸæœªçŸ¥é”™è¯¯")
        return {
            "system": {
                "cpu_percent": 0,
                "memory_percent": 0,
                "memory_used_gb": 0,
                "memory_total_gb": 0,
                "disk_percent": 0,
                "disk_used_gb": 0,
                "disk_total_gb": 0,
            },
            "sessions": {"active_count": 0},
            "performance": {"total_requests": 0, "avg_response_time": 0, "requests_per_minute": 0, "error_count": 0},
            "timestamp": datetime.now().isoformat(),
            "error": "Internal Monitoring Error",
        }


@router.get("/metrics/performance/details")
async def get_performance_details(hours: int = Query(default=1, ge=1, le=24), admin: dict = Depends(require_admin)):
    """
    è·å–è¯¦ç»†æ€§èƒ½æŒ‡æ ‡

    Args:
        hours: æŸ¥è¯¢æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰
    """
    try:
        details = performance_monitor.get_detailed_stats(hours=hours)
        return {"time_range_hours": hours, "data": details, "timestamp": datetime.now().isoformat()}
    except Exception as e:
        logger.error(f"âŒ è·å–æ€§èƒ½è¯¦æƒ…å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/metrics/slow-requests")
async def get_slow_requests(limit: int = Query(default=20, ge=1, le=100), admin: dict = Depends(require_admin)):
    """
    è·å–æ…¢è¯·æ±‚åˆ—è¡¨

    Args:
        limit: è¿”å›æ•°é‡é™åˆ¶
    """
    try:
        slow_requests = performance_monitor.get_slow_requests(limit=limit)
        return {"slow_requests": slow_requests, "count": len(slow_requests), "timestamp": datetime.now().isoformat()}
    except Exception as e:
        logger.error(f"âŒ è·å–æ…¢è¯·æ±‚å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/tools/stats")
async def get_tools_stats(hours: int = Query(default=24, ge=1, le=168), admin: dict = Depends(require_admin)):
    """
    è·å–æœç´¢å·¥å…·è°ƒç”¨ç»Ÿè®¡

    ç»Ÿè®¡æŒ‡æ ‡ï¼š
    - å„å·¥å…·è°ƒç”¨æ¬¡æ•°ã€æˆåŠŸç‡ã€å¹³å‡å“åº”æ—¶é—´
    - TopæŸ¥è¯¢ã€é”™è¯¯åˆ†å¸ƒ

    Args:
        hours: ç»Ÿè®¡æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼Œé»˜è®¤24å°æ—¶ï¼‰

    Returns:
        å·¥å…·ç»Ÿè®¡æ•°æ®
    """
    cache_key = f"tools_stats_{hours}"

    # æ£€æŸ¥ç¼“å­˜ï¼ˆ60ç§’TTLï¼‰
    if cache_key in metrics_cache:
        logger.debug("ğŸ“¦ è¿”å›ç¼“å­˜çš„å·¥å…·ç»Ÿè®¡æ•°æ®")
        return metrics_cache[cache_key]

    try:
        # è¯»å–å·¥å…·è°ƒç”¨æ—¥å¿—
        log_file = Path("logs/tool_calls.jsonl")
        if not log_file.exists():
            return {
                "tools": [],
                "total_calls": 0,
                "time_range_hours": hours,
                "timestamp": datetime.now().isoformat(),
                "message": "æš‚æ— å·¥å…·è°ƒç”¨è®°å½•",
            }

        # è®¡ç®—æ—¶é—´èŒƒå›´
        cutoff_time = datetime.now() - timedelta(hours=hours)

        # è§£ææ—¥å¿—
        tool_data = {}
        total_calls = 0

        with open(log_file, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    timestamp = datetime.fromisoformat(entry["timestamp"])

                    # è¿‡æ»¤æ—¶é—´èŒƒå›´
                    if timestamp < cutoff_time:
                        continue

                    tool_name = entry["tool_name"]
                    if tool_name not in tool_data:
                        tool_data[tool_name] = {
                            "name": tool_name,
                            "total_calls": 0,
                            "success_count": 0,
                            "fail_count": 0,
                            "total_duration_ms": 0,
                            "queries": [],
                        }

                    tool_data[tool_name]["total_calls"] += 1
                    total_calls += 1

                    if entry["status"] == "completed":
                        tool_data[tool_name]["success_count"] += 1
                    else:
                        tool_data[tool_name]["fail_count"] += 1

                    duration = entry.get("duration_ms", 0)
                    tool_data[tool_name]["total_duration_ms"] += duration

                    # è®°å½•æŸ¥è¯¢ï¼ˆç”¨äºTopæŸ¥è¯¢ç»Ÿè®¡ï¼‰
                    if entry.get("input_query"):
                        tool_data[tool_name]["queries"].append(entry["input_query"])

                except json.JSONDecodeError:
                    continue
                except Exception as e:
                    logger.warning(f"è§£ææ—¥å¿—è¡Œå¤±è´¥: {e}")
                    continue

        # æ±‡æ€»ç»Ÿè®¡
        tools_summary = []
        for tool_name, data in tool_data.items():
            total = data["total_calls"]
            success_rate = (data["success_count"] / total * 100) if total > 0 else 0
            avg_duration = (data["total_duration_ms"] / total) if total > 0 else 0

            # Top 5 æŸ¥è¯¢ï¼ˆç®€å•è®¡æ•°ï¼‰
            from collections import Counter

            query_counter = Counter(data["queries"])
            top_queries = [{"query": q[:100], "count": c} for q, c in query_counter.most_common(5)]

            tools_summary.append(
                {
                    "tool_name": tool_name,
                    "total_calls": total,
                    "success_count": data["success_count"],
                    "fail_count": data["fail_count"],
                    "success_rate": round(success_rate, 2),
                    "avg_duration_ms": round(avg_duration, 2),
                    "top_queries": top_queries,
                }
            )

        # æŒ‰è°ƒç”¨æ¬¡æ•°é™åºæ’åº
        tools_summary.sort(key=lambda x: x["total_calls"], reverse=True)

        result = {
            "tools": tools_summary,
            "total_calls": total_calls,
            "time_range_hours": hours,
            "timestamp": datetime.now().isoformat(),
        }

        # å†™å…¥ç¼“å­˜
        metrics_cache[cache_key] = result
        return result

    except Exception as e:
        logger.error(f"âŒ è·å–å·¥å…·ç»Ÿè®¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/concept-maps/stats")
async def get_concept_maps_stats(days: int = Query(default=7, ge=1, le=30), admin: dict = Depends(require_admin)):
    """
    è·å–æ¦‚å¿µå›¾ç”Ÿæˆç»Ÿè®¡

    ç»Ÿè®¡æŒ‡æ ‡ï¼š
    - æ€»ç”Ÿæˆæ•°é‡ã€ä¼šè¯åˆ†å¸ƒã€ä¸“å®¶åˆ†å¸ƒ
    - å¹³å‡å°ºå¯¸ã€å®½é«˜æ¯”åˆ†å¸ƒ
    - å­˜å‚¨å ç”¨ã€æ—¶é—´è¶‹åŠ¿

    Args:
        days: ç»Ÿè®¡æ—¶é—´èŒƒå›´ï¼ˆå¤©ï¼Œé»˜è®¤7å¤©ï¼‰

    Returns:
        æ¦‚å¿µå›¾ç»Ÿè®¡æ•°æ®
    """
    cache_key = f"concept_maps_stats_{days}"

    # æ£€æŸ¥ç¼“å­˜ï¼ˆ60ç§’TTLï¼‰
    if cache_key in metrics_cache:
        logger.debug("ğŸ“¦ è¿”å›ç¼“å­˜çš„æ¦‚å¿µå›¾ç»Ÿè®¡æ•°æ®")
        return metrics_cache[cache_key]

    try:
        from collections import Counter, defaultdict

        from intelligent_project_analyzer.services.image_storage_manager import ImageStorageManager

        # è®¡ç®—æ—¶é—´èŒƒå›´
        cutoff_time = datetime.now() - timedelta(days=days)

        # æ‰«ææ‰€æœ‰ä¼šè¯ç›®å½•
        base_dir = ImageStorageManager.BASE_DIR
        if not base_dir.exists():
            return {
                "total_images": 0,
                "total_sessions": 0,
                "total_storage_mb": 0,
                "time_range_days": days,
                "timestamp": datetime.now().isoformat(),
                "message": "æš‚æ— æ¦‚å¿µå›¾æ•°æ®",
            }

        # ç»Ÿè®¡æ•°æ®å®¹å™¨
        total_images = 0
        total_size_bytes = 0
        sessions_with_images = []
        images_by_expert = Counter()
        images_by_aspect_ratio = Counter()
        images_by_date = defaultdict(int)
        all_sessions = []

        # éå†æ‰€æœ‰ä¼šè¯ç›®å½•
        for session_dir in base_dir.iterdir():
            if not session_dir.is_dir():
                continue

            metadata_file = session_dir / "metadata.json"
            if not metadata_file.exists():
                continue

            try:
                with open(metadata_file, "r", encoding="utf-8") as f:
                    metadata = json.load(f)

                session_id = metadata.get("session_id", session_dir.name)
                session_created = datetime.fromisoformat(metadata.get("created_at", cutoff_time.isoformat()))

                # è¿‡æ»¤æ—¶é—´èŒƒå›´
                if session_created < cutoff_time:
                    continue

                images = metadata.get("images", [])
                if not images:
                    continue

                # ç»Ÿè®¡ä¼šè¯ä¿¡æ¯
                session_image_count = len(images)
                session_size = sum(img.get("file_size_bytes", 0) for img in images)

                sessions_with_images.append(
                    {
                        "session_id": session_id,
                        "image_count": session_image_count,
                        "total_size_mb": round(session_size / (1024 * 1024), 2),
                        "created_at": session_created.isoformat(),
                    }
                )

                all_sessions.append(session_id)

                # ç»Ÿè®¡å›¾ç‰‡è¯¦æƒ…
                for img in images:
                    total_images += 1
                    total_size_bytes += img.get("file_size_bytes", 0)

                    # æŒ‰ä¸“å®¶ç»Ÿè®¡
                    expert = img.get("owner_role", "unknown")
                    images_by_expert[expert] += 1

                    # æŒ‰å®½é«˜æ¯”ç»Ÿè®¡
                    aspect_ratio = img.get("aspect_ratio", "16:9")
                    images_by_aspect_ratio[aspect_ratio] += 1

                    # æŒ‰æ—¥æœŸç»Ÿè®¡
                    created_at = datetime.fromisoformat(img.get("created_at", session_created.isoformat()))
                    date_key = created_at.strftime("%Y-%m-%d")
                    images_by_date[date_key] += 1

            except Exception as e:
                logger.warning(f"âš ï¸ è§£æä¼šè¯ {session_dir.name} å¤±è´¥: {e}")
                continue

        # æŒ‰å›¾ç‰‡æ•°é‡æ’åºä¼šè¯
        sessions_with_images.sort(key=lambda x: x["image_count"], reverse=True)

        # è½¬æ¢ä¸“å®¶ç»Ÿè®¡ä¸ºåˆ—è¡¨
        expert_stats = [{"expert_role": role, "image_count": count} for role, count in images_by_expert.most_common()]

        # è½¬æ¢å®½é«˜æ¯”ç»Ÿè®¡
        aspect_ratio_stats = [
            {"aspect_ratio": ratio, "count": count} for ratio, count in images_by_aspect_ratio.most_common()
        ]

        # è½¬æ¢æ—¥æœŸè¶‹åŠ¿ï¼ˆæœ€è¿‘7å¤©ï¼‰
        date_trend = [
            {"date": date, "count": count} for date, count in sorted(images_by_date.items(), reverse=True)[:7]
        ]

        result = {
            "total_images": total_images,
            "total_sessions": len(all_sessions),
            "total_storage_mb": round(total_size_bytes / (1024 * 1024), 2),
            "avg_images_per_session": round(total_images / len(all_sessions), 2) if all_sessions else 0,
            "expert_distribution": expert_stats,
            "aspect_ratio_distribution": aspect_ratio_stats,
            "date_trend": date_trend,
            "top_sessions": sessions_with_images[:10],  # Top 10 ä¼šè¯
            "time_range_days": days,
            "timestamp": datetime.now().isoformat(),
        }

        # å†™å…¥ç¼“å­˜
        metrics_cache[cache_key] = result
        return result

    except Exception as e:
        logger.error(f"âŒ è·å–æ¦‚å¿µå›¾ç»Ÿè®¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/conversations/analytics")
async def get_conversations_analytics(
    days: int = Query(default=30, ge=1, le=365), admin: dict = Depends(require_admin)
):
    """
    è·å–å¯¹è¯åˆ†æç»Ÿè®¡

    ç»Ÿè®¡æŒ‡æ ‡ï¼š
    - æ—¶é—´è¶‹åŠ¿ï¼šæ¯å¤©ã€æ¯å‘¨ã€æ¯æœˆçš„å¯¹è¯æ•°é‡
    - ç±»å‹åˆ†å¸ƒï¼šé¡¹ç›®ç±»å‹åˆ†å¸ƒ
    - çŠ¶æ€åˆ†å¸ƒï¼šcompleted/failed/active
    - å…³é”®è¯åˆ†æï¼šç”¨æˆ·è¾“å…¥çš„é«˜é¢‘å…³é”®è¯

    Args:
        days: ç»Ÿè®¡æ—¶é—´èŒƒå›´ï¼ˆå¤©ï¼Œé»˜è®¤30å¤©ï¼‰

    Returns:
        å¯¹è¯åˆ†ææ•°æ®
    """
    cache_key = f"conversations_analytics_{days}"

    # æ£€æŸ¥ç¼“å­˜ï¼ˆ60ç§’TTLï¼‰
    if cache_key in metrics_cache:
        logger.debug("ğŸ“¦ è¿”å›ç¼“å­˜çš„å¯¹è¯åˆ†ææ•°æ®")
        return metrics_cache[cache_key]

    try:
        # è·å–æ‰€æœ‰ä¼šè¯
        from intelligent_project_analyzer.services.redis_session_manager import RedisSessionManager

        redis_manager = RedisSessionManager()

        # ç¡®ä¿Redisè¿æ¥
        if not redis_manager.is_connected:
            await redis_manager.connect()

        all_sessions = await redis_manager.get_all_sessions()

        # è®¡ç®—æ—¶é—´èŒƒå›´
        cutoff_time = datetime.now() - timedelta(days=days)

        # ç»Ÿè®¡æ•°æ®å®¹å™¨
        import re
        from collections import Counter, defaultdict

        daily_counts = defaultdict(int)
        weekly_counts = defaultdict(int)
        monthly_counts = defaultdict(int)
        yearly_counts = defaultdict(int)

        type_distribution = Counter()
        status_distribution = Counter()
        all_keywords = []

        total_conversations = 0

        for session in all_sessions:
            # è§£æåˆ›å»ºæ—¶é—´
            created_at_str = session.get("created_at", "")
            if not created_at_str:
                continue

            try:
                created_at = datetime.fromisoformat(created_at_str)
            except (ValueError, TypeError):
                continue

            # è¿‡æ»¤æ—¶é—´èŒƒå›´
            if created_at < cutoff_time:
                continue

            total_conversations += 1

            # æ—¶é—´è¶‹åŠ¿ç»Ÿè®¡
            date_key = created_at.strftime("%Y-%m-%d")
            week_key = created_at.strftime("%Y-W%W")
            month_key = created_at.strftime("%Y-%m")
            year_key = created_at.strftime("%Y")

            daily_counts[date_key] += 1
            weekly_counts[week_key] += 1
            monthly_counts[month_key] += 1
            yearly_counts[year_key] += 1

            # ç±»å‹åˆ†å¸ƒ
            project_type = session.get("project_type", "æœªåˆ†ç±»")
            type_distribution[project_type] += 1

            # çŠ¶æ€åˆ†å¸ƒ
            status = session.get("status", "unknown")
            status_distribution[status] += 1

            # å…³é”®è¯æå–ï¼ˆä»user_inputï¼‰
            user_input = session.get("user_input", "")
            if user_input:
                # ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼ˆæå–2-4å­—çš„è¯æ±‡ï¼‰
                words = re.findall(r"[\u4e00-\u9fff]{2,4}", user_input)
                all_keywords.extend(words)

        # å…³é”®è¯ç»Ÿè®¡ï¼ˆTop 50ï¼‰
        keyword_counter = Counter(all_keywords)
        top_keywords = [{"word": word, "count": count} for word, count in keyword_counter.most_common(50)]

        # è½¬æ¢ä¸ºå‰ç«¯å‹å¥½çš„æ ¼å¼
        daily_trend = [
            {"date": date, "count": count} for date, count in sorted(daily_counts.items(), reverse=True)[:30]  # æœ€è¿‘30å¤©
        ]

        weekly_trend = [
            {"week": week, "count": count} for week, count in sorted(weekly_counts.items(), reverse=True)[:12]  # æœ€è¿‘12å‘¨
        ]

        monthly_trend = [
            {"month": month, "count": count}
            for month, count in sorted(monthly_counts.items(), reverse=True)[:12]  # æœ€è¿‘12æœˆ
        ]

        yearly_trend = [{"year": year, "count": count} for year, count in sorted(yearly_counts.items(), reverse=True)]

        type_dist = [{"type": type_name, "count": count} for type_name, count in type_distribution.most_common()]

        status_dist = [{"status": status, "count": count} for status, count in status_distribution.most_common()]

        result = {
            "total_conversations": total_conversations,
            "time_range_days": days,
            "daily_trend": daily_trend,
            "weekly_trend": weekly_trend,
            "monthly_trend": monthly_trend,
            "yearly_trend": yearly_trend,
            "type_distribution": type_dist,
            "status_distribution": status_dist,
            "top_keywords": top_keywords,
            "timestamp": datetime.now().isoformat(),
        }

        # å†™å…¥ç¼“å­˜
        metrics_cache[cache_key] = result
        return result

    except Exception as e:
        logger.error(f"âŒ è·å–å¯¹è¯åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# é…ç½®ç®¡ç† API
# ============================================================================


@router.get("/config/current")
async def get_current_config(admin: dict = Depends(require_admin)):
    """
    è·å–å½“å‰é…ç½®ï¼ˆè„±æ•ï¼‰

    ä¸è¿”å›æ•æ„Ÿä¿¡æ¯ï¼ˆAPI Keyã€å¯†ç ç­‰ï¼‰
    """
    try:
        sanitized_config = config_manager.get_sanitized_config()
        return {"config": sanitized_config, "timestamp": datetime.now().isoformat()}
    except Exception as e:
        logger.error(f"âŒ è·å–é…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/config/reload")
async def reload_config(admin: dict = Depends(require_admin)):
    """
    æ‰‹åŠ¨è§¦å‘é…ç½®é‡è½½

    é‡æ–°åŠ è½½ .env æ–‡ä»¶å’Œé…ç½®å¯¹è±¡
    """
    try:
        success = config_manager.reload()

        if success:
            logger.info(f"âœ… ç®¡ç†å‘˜ {admin.get('username')} è§¦å‘é…ç½®é‡è½½")
            return {"status": "success", "message": "é…ç½®å·²é‡è½½", "timestamp": datetime.now().isoformat()}
        else:
            raise HTTPException(status_code=500, detail="é…ç½®é‡è½½å¤±è´¥")

    except Exception as e:
        logger.error(f"âŒ é…ç½®é‡è½½å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/config/env")
async def get_env_content(admin: dict = Depends(require_admin)):
    """
    è·å– .env æ–‡ä»¶å†…å®¹ï¼ˆè„±æ•ï¼‰

    ç”¨äºé…ç½®ç¼–è¾‘å™¨å±•ç¤º
    """
    try:
        env_path = Path(__file__).parent.parent.parent.parent / ".env"

        if not env_path.exists():
            raise HTTPException(status_code=404, detail=".env æ–‡ä»¶ä¸å­˜åœ¨")

        with open(env_path, "r", encoding="utf-8") as f:
            content = f.read()

        # è„±æ•å¤„ç†ï¼šéšè—æ•æ„Ÿå€¼
        lines = []
        for line in content.split("\n"):
            if "=" in line and not line.strip().startswith("#"):
                key, value = line.split("=", 1)
                # éšè— API Keyã€å¯†ç ç­‰æ•æ„Ÿä¿¡æ¯
                if any(sensitive in key.upper() for sensitive in ["KEY", "SECRET", "PASSWORD", "TOKEN"]):
                    lines.append(f"{key}=***HIDDEN***")
                else:
                    lines.append(line)
            else:
                lines.append(line)

        return {"content": "\n".join(lines), "file_path": str(env_path), "timestamp": datetime.now().isoformat()}

    except Exception as e:
        logger.error(f"âŒ è¯»å– .env æ–‡ä»¶å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# ä¼šè¯ç®¡ç† API
# ============================================================================


@router.get("/sessions")
async def list_all_sessions(
    page: int = Query(default=1, ge=1),
    page_size: int = Query(default=20, ge=1, le=100),
    status: Optional[str] = Query(default=None),
    search: Optional[str] = Query(default=None),
    admin: dict = Depends(require_admin),
):
    """
    è·å–æ‰€æœ‰ç”¨æˆ·çš„ä¼šè¯åˆ—è¡¨ï¼ˆç®¡ç†å‘˜è§†å›¾ï¼‰

    Args:
        page: é¡µç 
        page_size: æ¯é¡µæ•°é‡
        status: ç­›é€‰çŠ¶æ€
        search: æœç´¢å…³é”®è¯
    """
    try:
        # ä» server.py å¯¼å…¥å…¨å±€ session_manager
        from ..api.server import session_manager as global_session_manager

        if not global_session_manager:
            return {"sessions": [], "total": 0, "page": page, "page_size": page_size, "total_pages": 0}

        # è·å–æ‰€æœ‰ä¼šè¯ï¼ˆä¸é™ç”¨æˆ·ï¼‰
        all_sessions = await global_session_manager.get_all_sessions()

        # å¢å¼ºç”¨æˆ·ä¿¡æ¯æ˜¾ç¤º
        for session in all_sessions:
            # å¦‚æœä¼šè¯ä¸­å·²ç»æœ‰ username/display_nameï¼Œç›´æ¥ä½¿ç”¨
            if "username" not in session or "display_name" not in session:
                user_id = session.get("user_id", "")
                # ä» session_id æå–ç”¨æˆ·æ ‡è¯†ä½œä¸ºfallback
                session_parts = session.get("session_id", "").split("-")
                username_fallback = session_parts[0] if session_parts else user_id

                session["username"] = session.get(
                    "username", username_fallback if username_fallback != "web_user" else "åŒ¿åç”¨æˆ·"
                )
                session["display_name"] = session.get("display_name", session["username"])

        # çŠ¶æ€è¿‡æ»¤
        if status:
            all_sessions = [s for s in all_sessions if s.get("status") == status]

        # æœç´¢è¿‡æ»¤
        if search:
            search_lower = search.lower()
            all_sessions = [
                s
                for s in all_sessions
                if search_lower in str(s.get("session_id", "")).lower()
                or search_lower in str(s.get("user_id", "")).lower()
                or search_lower in str(s.get("input_text", "")).lower()
            ]

        # æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        all_sessions.sort(key=lambda x: x.get("created_at", ""), reverse=True)

        # åˆ†é¡µ
        total = len(all_sessions)
        start = (page - 1) * page_size
        end = start + page_size
        paginated_sessions = all_sessions[start:end]

        return {
            "sessions": paginated_sessions,
            "total": total,
            "page": page,
            "page_size": page_size,
            "total_pages": (total + page_size - 1) // page_size,
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        logger.error(f"âŒ è·å–ä¼šè¯åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/sessions/{session_id}")
async def get_session_detail(session_id: str, admin: dict = Depends(require_admin)):
    """
    è·å–ä¼šè¯è¯¦ç»†ä¿¡æ¯

    Args:
        session_id: ä¼šè¯ID

    Returns:
        dict: å®Œæ•´çš„ä¼šè¯æ•°æ®ï¼ˆåŒ…æ‹¬çŠ¶æ€ã€è¾“å…¥ã€è¾“å‡ºç­‰ï¼‰
    """
    try:
        logger.info(f"ğŸ” ç®¡ç†å‘˜ {admin.get('username')} è¯·æ±‚æŸ¥çœ‹ä¼šè¯è¯¦æƒ…: {session_id}")

        # ä» server.py å¯¼å…¥å…¨å±€ session_manager
        from ..api.server import session_manager as global_session_manager

        if not global_session_manager:
            logger.error("âŒ session_manager æœªåˆå§‹åŒ–")
            raise HTTPException(status_code=503, detail="ä¼šè¯ç®¡ç†å™¨æœªåˆå§‹åŒ–")

        # è·å–ä¼šè¯æ•°æ®
        logger.debug(f"ğŸ“¦ æ­£åœ¨ä» Redis è·å–ä¼šè¯: {session_id}")
        session_data = await global_session_manager.get(session_id)

        if not session_data:
            logger.warning(f"âš ï¸ ä¼šè¯ä¸å­˜åœ¨: {session_id}")
            raise HTTPException(status_code=404, detail=f"ä¼šè¯ {session_id} ä¸å­˜åœ¨")

        logger.success(f"âœ… æˆåŠŸè·å–ä¼šè¯è¯¦æƒ…: {session_id} (æ•°æ®å¤§å°: {len(str(session_data))} bytes)")

        return session_data

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ è·å–ä¼šè¯è¯¦æƒ…å¤±è´¥ ({session_id}): {type(e).__name__}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å†…éƒ¨é”™è¯¯: {str(e)}")


@router.post("/sessions/{session_id}/force-stop")
async def force_stop_session(session_id: str, admin: dict = Depends(require_admin)):
    """
    å¼ºåˆ¶ç»ˆæ­¢ä¼šè¯

    Args:
        session_id: ä¼šè¯ID
    """
    try:
        # ä» server.py å¯¼å…¥å…¨å±€ session_manager
        from ..api.server import session_manager as global_session_manager

        if not global_session_manager:
            raise HTTPException(status_code=503, detail="ä¼šè¯ç®¡ç†å™¨æœªåˆå§‹åŒ–")

        # è·å–ä¼šè¯
        session_data = await global_session_manager.get(session_id)

        if not session_data:
            raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")

        # æ›´æ–°ä¼šè¯çŠ¶æ€ä¸ºç»ˆæ­¢
        updates = {
            "status": "terminated",
            "terminated_at": datetime.now().isoformat(),
            "terminated_by": admin.get("username"),
        }

        await global_session_manager.update(session_id, updates)

        logger.warning(f"âš ï¸ ç®¡ç†å‘˜ {admin.get('username')} å¼ºåˆ¶ç»ˆæ­¢ä¼šè¯: {session_id}")
        return {"status": "success", "message": f"ä¼šè¯ {session_id} å·²å¼ºåˆ¶ç»ˆæ­¢", "timestamp": datetime.now().isoformat()}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ å¼ºåˆ¶ç»ˆæ­¢ä¼šè¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/sessions/batch")
async def batch_delete_sessions(session_ids: List[str], admin: dict = Depends(require_admin)):
    """
    æ‰¹é‡åˆ é™¤ä¼šè¯

    Args:
        session_ids: ä¼šè¯IDåˆ—è¡¨
    """
    try:
        # ä» server.py å¯¼å…¥å…¨å±€ session_manager
        from ..api.server import session_manager as global_session_manager

        if not global_session_manager:
            raise HTTPException(status_code=503, detail="ä¼šè¯ç®¡ç†å™¨æœªåˆå§‹åŒ–")

        deleted_count = 0
        failed_count = 0

        for session_id in session_ids:
            try:
                await global_session_manager.delete_session(session_id)
                deleted_count += 1
            except Exception as e:
                logger.error(f"âŒ åˆ é™¤ä¼šè¯ {session_id} å¤±è´¥: {e}")
                failed_count += 1

        logger.warning(f"âš ï¸ ç®¡ç†å‘˜ {admin.get('username')} æ‰¹é‡åˆ é™¤ä¼šè¯: " f"{deleted_count} æˆåŠŸ, {failed_count} å¤±è´¥")

        return {
            "status": "success",
            "deleted_count": deleted_count,
            "failed_count": failed_count,
            "total_requested": len(session_ids),
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡åˆ é™¤ä¼šè¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# æ—¥å¿—æŸ¥è¯¢ API
# ============================================================================


@router.get("/logs")
async def query_logs(
    log_type: str = Query(default="server", regex="^(server|auth|errors|performance|admin_operations)$"),
    lines: int = Query(default=100, ge=1, le=1000),
    search: Optional[str] = Query(default=None),
    admin: dict = Depends(require_admin),
):
    """
    æŸ¥è¯¢æ—¥å¿—

    Args:
        log_type: æ—¥å¿—ç±»å‹ï¼ˆserver/auth/errors/performance/admin_operationsï¼‰
        lines: è¿”å›è¡Œæ•°
        search: æœç´¢å…³é”®è¯
    """
    try:
        log_dir = Path(__file__).parent.parent.parent.parent / "logs"
        log_file_map = {
            "server": "server.log",
            "auth": "auth_service.log",
            "errors": "errors.log",
            "performance": "performance.log",
            "admin_operations": "admin_operations.log",
        }

        log_file = log_dir / log_file_map.get(log_type, "server.log")

        if not log_file.exists():
            return {
                "logs": [],
                "count": 0,
                "message": f"æ—¥å¿—æ–‡ä»¶ {log_file.name} ä¸å­˜åœ¨",
                "timestamp": datetime.now().isoformat(),
            }

        # è¯»å–æ—¥å¿—æ–‡ä»¶ï¼ˆå°¾éƒ¨Nè¡Œï¼‰
        with open(log_file, "r", encoding="utf-8", errors="ignore") as f:
            all_lines = f.readlines()

        # è·å–å°¾éƒ¨Nè¡Œ
        log_lines = all_lines[-lines:] if len(all_lines) > lines else all_lines

        # æœç´¢è¿‡æ»¤
        if search:
            log_lines = [line for line in log_lines if search.lower() in line.lower()]

        return {
            "logs": log_lines,
            "count": len(log_lines),
            "log_type": log_type,
            "file_path": str(log_file),
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        logger.error(f"âŒ æŸ¥è¯¢æ—¥å¿—å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/logs/files")
async def list_log_files(admin: dict = Depends(require_admin)):
    """
    åˆ—å‡ºæ‰€æœ‰æ—¥å¿—æ–‡ä»¶
    """
    try:
        log_dir = Path(__file__).parent.parent.parent.parent / "logs"

        if not log_dir.exists():
            return {"files": [], "count": 0}

        log_files = []
        for log_file in log_dir.glob("*.log"):
            stat = log_file.stat()
            log_files.append(
                {
                    "name": log_file.name,
                    "size_mb": round(stat.st_size / (1024**2), 2),
                    "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                }
            )

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        log_files.sort(key=lambda x: x["modified"], reverse=True)

        return {"files": log_files, "count": len(log_files), "timestamp": datetime.now().isoformat()}

    except Exception as e:
        logger.error(f"âŒ åˆ—å‡ºæ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# ä¸»åŠ¨å­¦ä¹ æ•°æ®åˆ†æ API
# ============================================================================


@router.get("/dimension-learning/stats")
async def get_dimension_learning_stats(admin: dict = Depends(require_admin)):
    """
    è·å–ä¸»åŠ¨å­¦ä¹ æ•°æ®ç»Ÿè®¡

    è¿”å›ç»´åº¦é€‰æ‹©æ•ˆæœã€ç”¨æˆ·åé¦ˆç­‰
    """
    try:
        # TODO: ä» Redis è¯»å–ç»´åº¦å­¦ä¹ å†å²æ•°æ®
        # dimension_history = await redis_client.get("dimension:history:*")

        # å ä½å®ç°
        return {
            "status": "success",
            "message": "ç»´åº¦å­¦ä¹ ç»Ÿè®¡åŠŸèƒ½å¼€å‘ä¸­",
            "placeholder_data": {"total_feedbacks": 0, "avg_score": 0, "top_dimensions": []},
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        logger.error(f"âŒ è·å–ç»´åº¦å­¦ä¹ ç»Ÿè®¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# ç³»ç»Ÿå¥åº·æ£€æŸ¥
# ============================================================================


@router.get("/health")
async def admin_health_check(admin: dict = Depends(require_admin)):
    """
    ç®¡ç†å‘˜ç³»ç»Ÿå¥åº·æ£€æŸ¥ï¼ˆè¯¦ç»†ç‰ˆï¼‰

    è¿”å›æ›´å¤šå†…éƒ¨çŠ¶æ€ä¿¡æ¯
    """
    try:
        return {
            "status": "healthy",
            "admin": admin.get("username"),
            "components": {
                "api": "up",
                "redis": "up",  # TODO: å®é™…æ£€æŸ¥
                "config_manager": "up",
            },
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        logger.error(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# ç”¨æˆ·åˆ†æ API
# ============================================================================


@router.get("/users/analytics")
async def get_users_analytics(
    time_range: str = Query("7d", description="æ—¶é—´èŒƒå›´: 1d/7d/30d/365d"), admin: dict = Depends(require_admin)
):
    """
    è·å–ç”¨æˆ·åˆ†ææ•°æ®

    è¿”å›ï¼š
    - åœ¨çº¿ç”¨æˆ·æ•°é‡ï¼ˆæŒ‰å¤©/å‘¨/æœˆ/å¹´ç»Ÿè®¡ï¼‰
    - ç”¨æˆ·åœ°åŒºåˆ†å¸ƒï¼ˆåŸºäºsessionä¸­çš„åŸå¸‚ä¿¡æ¯ï¼‰
    - ç”¨æˆ·å¯¹è¯æ•°é‡æ’è¡Œï¼ˆå¯æŒ‰æ—¶é—´ç­›é€‰ï¼‰

    ç¼“å­˜ç­–ç•¥ï¼š60ç§’TTL
    """
    cache_key = f"users_analytics_{time_range}"

    # æ£€æŸ¥ç¼“å­˜
    if cache_key in metrics_cache:
        logger.debug(f"ğŸ“¦ è¿”å›ç¼“å­˜çš„ç”¨æˆ·åˆ†ææ•°æ®: {time_range}")
        return metrics_cache[cache_key]

    try:
        # è®¡ç®—æ—¶é—´èŒƒå›´
        time_ranges = {"1d": 1, "7d": 7, "30d": 30, "365d": 365}
        days = time_ranges.get(time_range, 7)
        cutoff_date = datetime.now() - timedelta(days=days)

        # ä» server.py å¯¼å…¥å…¨å±€ session_manager
        from ..api.server import session_manager as global_session_manager

        if not global_session_manager:
            raise HTTPException(status_code=503, detail="Session manager æœªåˆå§‹åŒ–")

        # è·å–æ‰€æœ‰ä¼šè¯
        all_sessions = await global_session_manager.get_all_sessions()

        # è¿‡æ»¤æ—¶é—´èŒƒå›´å†…çš„ä¼šè¯
        filtered_sessions = []
        for session in all_sessions:
            created_str = session.get("created_at")
            if created_str:
                try:
                    created_at = datetime.fromisoformat(created_str)
                    if created_at >= cutoff_date:
                        filtered_sessions.append(session)
                except:
                    continue

        # 1. ç»Ÿè®¡åœ¨çº¿ç”¨æˆ·æ•°é‡ï¼ˆæŒ‰æ—¥æœŸåˆ†ç»„ï¼‰
        daily_users = {}  # {date: set(user_ids)}
        weekly_users = {}  # {week_number: set(user_ids)}
        monthly_users = {}  # {month: set(user_ids)}
        yearly_users = {}  # {year: set(user_ids)}

        for session in filtered_sessions:
            user_id = session.get("user_id") or session.get("username", "guest")
            created_str = session.get("created_at")

            if not created_str:
                continue

            try:
                created_at = datetime.fromisoformat(created_str)

                # æŒ‰æ—¥ç»Ÿè®¡
                date_key = created_at.strftime("%Y-%m-%d")
                if date_key not in daily_users:
                    daily_users[date_key] = set()
                daily_users[date_key].add(user_id)

                # æŒ‰å‘¨ç»Ÿè®¡
                week_key = f"{created_at.year}-W{created_at.isocalendar()[1]:02d}"
                if week_key not in weekly_users:
                    weekly_users[week_key] = set()
                weekly_users[week_key].add(user_id)

                # æŒ‰æœˆç»Ÿè®¡
                month_key = created_at.strftime("%Y-%m")
                if month_key not in monthly_users:
                    monthly_users[month_key] = set()
                monthly_users[month_key].add(user_id)

                # æŒ‰å¹´ç»Ÿè®¡
                year_key = str(created_at.year)
                if year_key not in yearly_users:
                    yearly_users[year_key] = set()
                yearly_users[year_key].add(user_id)
            except:
                continue

        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        daily_trend = [{"date": k, "count": len(v)} for k, v in sorted(daily_users.items())]
        weekly_trend = [{"week": k, "count": len(v)} for k, v in sorted(weekly_users.items())]
        monthly_trend = [{"month": k, "count": len(v)} for k, v in sorted(monthly_users.items())]
        yearly_trend = [{"year": k, "count": len(v)} for k, v in sorted(yearly_users.items())]

        # 2. åœ°åŒºåˆ†å¸ƒï¼ˆä¼˜å…ˆä½¿ç”¨IPå®šä½çš„metadataæ•°æ®ï¼‰
        region_distribution = {}
        region_coords = {}  # å­˜å‚¨ç»çº¬åº¦ç”¨äºåœ°å›¾å¯è§†åŒ–

        for session in filtered_sessions:
            metadata = session.get("metadata", {})

            # ğŸŒ ä¼˜å…ˆä½¿ç”¨IPå®šä½çš„åŸå¸‚ä¿¡æ¯
            location = metadata.get("location")
            geo_info = metadata.get("geo_info", {})

            if not location or location == "æœªçŸ¥":
                # å›é€€ï¼šä»ç”¨æˆ·è¾“å…¥ä¸­æ£€æµ‹åŸå¸‚ï¼ˆä¸­å›½ä¸»è¦åŸå¸‚åˆ—è¡¨ï¼‰
                user_input = session.get("user_input", "")
                cities = [
                    "åŒ—äº¬",
                    "ä¸Šæµ·",
                    "å¹¿å·",
                    "æ·±åœ³",
                    "æ­å·",
                    "æˆéƒ½",
                    "é‡åº†",
                    "æ­¦æ±‰",
                    "è¥¿å®‰",
                    "å¤©æ´¥",
                    "å—äº¬",
                    "è‹å·",
                    "é•¿æ²™",
                    "éƒ‘å·",
                    "æµå—",
                    "é’å²›",
                    "å¦é—¨",
                    "ç¦å·",
                    "æ˜†æ˜",
                    "å—å®",
                    "æµ·å£",
                    "å…°å·",
                    "é“¶å·",
                    "è¥¿å®",
                    "ä¹Œé²æœ¨é½",
                    "æ‹‰è¨",
                    "å“ˆå°”æ»¨",
                    "é•¿æ˜¥",
                    "æ²ˆé˜³",
                    "å¤§è¿",
                    "çŸ³å®¶åº„",
                    "å¤ªåŸ",
                    "å‘¼å’Œæµ©ç‰¹",
                    "åˆè‚¥",
                    "å—æ˜Œ",
                    "è´µé˜³",
                ]

                detected_city = None
                for city in cities:
                    if city in user_input:
                        detected_city = city
                        break

                location = detected_city or "æœªçŸ¥åœ°åŒº"

            # ç»Ÿè®¡åœ°åŒºåˆ†å¸ƒ
            region_distribution[location] = region_distribution.get(location, 0) + 1

            # æ”¶é›†ç»çº¬åº¦æ•°æ®ï¼ˆç”¨äºå‰ç«¯åœ°å›¾å¯è§†åŒ–ï¼‰
            if geo_info.get("latitude") and geo_info.get("longitude"):
                if location not in region_coords:
                    region_coords[location] = {
                        "lat": geo_info["latitude"],
                        "lng": geo_info["longitude"],
                        "country": geo_info.get("country", ""),
                        "province": geo_info.get("province", ""),
                    }

        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼å¹¶æ’åºï¼ˆé™„å¸¦ç»çº¬åº¦ä¿¡æ¯ï¼‰
        region_list = []
        for region, count in sorted(region_distribution.items(), key=lambda x: x[1], reverse=True):
            item = {"region": region, "count": count}
            if region in region_coords:
                item.update(region_coords[region])
            region_list.append(item)

        # 3. ç”¨æˆ·å¯¹è¯æ•°é‡æ’è¡Œ
        user_conversation_counts = {}

        for session in filtered_sessions:
            user_id = session.get("user_id") or session.get("username", "guest")
            user_conversation_counts[user_id] = user_conversation_counts.get(user_id, 0) + 1

        # æ’åºå¹¶å–Top 50
        user_rankings = sorted(
            [{"user_id": k, "conversation_count": v} for k, v in user_conversation_counts.items()],
            key=lambda x: x["conversation_count"],
            reverse=True,
        )[:50]

        # æ„å»ºå“åº”
        result = {
            "status": "success",
            "time_range": time_range,
            "total_users": len(set(s.get("user_id") or s.get("username", "guest") for s in filtered_sessions)),
            "total_sessions": len(filtered_sessions),
            "date_range": {"start": cutoff_date.strftime("%Y-%m-%d"), "end": datetime.now().strftime("%Y-%m-%d")},
            "online_users": {
                "daily": daily_trend[-30:],  # æœ€è¿‘30å¤©
                "weekly": weekly_trend[-12:],  # æœ€è¿‘12å‘¨
                "monthly": monthly_trend[-12:],  # æœ€è¿‘12ä¸ªæœˆ
                "yearly": yearly_trend[-5:],  # æœ€è¿‘5å¹´
            },
            "region_distribution": region_list,
            "user_rankings": user_rankings,
            "timestamp": datetime.now().isoformat(),
        }

        # ç¼“å­˜ç»“æœï¼ˆ60ç§’ï¼‰
        metrics_cache[cache_key] = result

        logger.info(f"âœ… ç”¨æˆ·åˆ†ææ•°æ®ç”Ÿæˆå®Œæˆ: {len(filtered_sessions)} sessions, {result['total_users']} users")
        return result

    except Exception as e:
        logger.error(f"âŒ è·å–ç”¨æˆ·åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))
