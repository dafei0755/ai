"""
æ€§èƒ½ç›‘æ§ä¸­é—´ä»¶ - è®°å½• API å“åº”æ—¶é—´å’Œæ€§èƒ½æŒ‡æ ‡

åŠŸèƒ½ï¼š
1. è®°å½•æ¯ä¸ªè¯·æ±‚çš„å“åº”æ—¶é—´
2. è®°å½•æ…¢è¯·æ±‚ï¼ˆ>1ç§’ï¼‰
3. ç»Ÿè®¡ API è°ƒç”¨æ¬¡æ•°
4. æ€§èƒ½æŒ‡æ ‡å¯¼å‡º
"""

import asyncio
import json
import threading
import time
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Callable

from fastapi import Request, Response
from loguru import logger


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§ç®¡ç†å™¨"""

    def __init__(self):
        self.metrics = defaultdict(list)
        self.lock = threading.Lock()
        self.slow_request_threshold = 1.0  # ç§’
        self.metrics_file = Path(__file__).parent.parent.parent / "logs" / "performance_metrics.jsonl"
        self.metrics_file.parent.mkdir(exist_ok=True)

    def record_request(self, path: str, method: str, duration: float, status_code: int):
        """è®°å½•è¯·æ±‚æ€§èƒ½"""
        with self.lock:
            self.metrics[path].append(
                {
                    "method": method,
                    "duration": duration,
                    "status_code": status_code,
                    "timestamp": datetime.now().isoformat(),
                }
            )

            # æ…¢è¯·æ±‚è­¦å‘Š
            if duration > self.slow_request_threshold:
                logger.warning(f"ğŸŒ æ…¢è¯·æ±‚æ£€æµ‹: {method} {path} è€—æ—¶ {duration:.2f}ç§’")

            # å†™å…¥æŒä¹…åŒ–æ—¥å¿—
            self._write_metric(path, method, duration, status_code)

    def _write_metric(self, path: str, method: str, duration: float, status_code: int):
        """å†™å…¥æ€§èƒ½æŒ‡æ ‡åˆ°æ–‡ä»¶"""
        # ğŸ”¥ v7.105: æ·»åŠ æ–‡ä»¶é”å®šä¿æŠ¤ï¼Œé¿å…Permission deniedé”™è¯¯
        try:
            metric = {
                "timestamp": datetime.now().isoformat(),
                "path": path,
                "method": method,
                "duration": round(duration, 3),
                "status_code": status_code,
            }

            # å°è¯•å†™å…¥ï¼Œå¦‚æœå¤±è´¥åˆ™é™é»˜å¿½ç•¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰
            try:
                with open(self.metrics_file, "a", encoding="utf-8") as f:
                    f.write(json.dumps(metric, ensure_ascii=False) + "\n")
            except (PermissionError, OSError) as file_error:
                # æ–‡ä»¶é”å®šæˆ–æƒé™é—®é¢˜ï¼Œé™é»˜å¿½ç•¥ï¼ˆæ¯10æ¬¡è®°å½•ä¸€æ¬¡è­¦å‘Šï¼‰
                if not hasattr(self, "_write_error_count"):
                    self._write_error_count = 0
                self._write_error_count += 1
                if self._write_error_count % 10 == 1:
                    logger.debug(f"âš ï¸ æ€§èƒ½æŒ‡æ ‡å†™å…¥è·³è¿‡ (æ–‡ä»¶è¢«å ç”¨ï¼Œå·²è·³è¿‡{self._write_error_count}æ¬¡)")
        except Exception as e:
            # å…¶ä»–å¼‚å¸¸ä¹Ÿä¸å½±å“ä¸»æµç¨‹
            pass

    def get_stats(self, path: str = None):
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        with self.lock:
            if path:
                metrics = self.metrics.get(path, [])
            else:
                metrics = [m for ms in self.metrics.values() for m in ms]

            if not metrics:
                return {}

            durations = [m["duration"] for m in metrics]
            return {
                "count": len(metrics),
                "avg_duration": sum(durations) / len(durations),
                "max_duration": max(durations),
                "min_duration": min(durations),
                "slow_requests": len([d for d in durations if d > self.slow_request_threshold]),
            }

    def get_stats_summary(self):
        """
        è·å–æ€§èƒ½ç»Ÿè®¡æ‘˜è¦ï¼ˆç”¨äºç®¡ç†åå°ä»ªè¡¨æ¿ï¼‰

        Returns:
            dict: åŒ…å«æ€»è¯·æ±‚æ•°ã€å¹³å‡å“åº”æ—¶é—´ã€æ¯åˆ†é’Ÿè¯·æ±‚æ•°ã€é”™è¯¯æ•°
        """
        with self.lock:
            all_metrics = [m for ms in self.metrics.values() for m in ms]

            if not all_metrics:
                return {"total_requests": 0, "avg_response_time": 0, "requests_per_minute": 0, "error_count": 0}

            # è®¡ç®—ç»Ÿè®¡æ•°æ®
            total_requests = len(all_metrics)
            durations = [m["duration"] for m in all_metrics]
            avg_response_time = (sum(durations) / len(durations)) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’

            # ç»Ÿè®¡é”™è¯¯ï¼ˆ5xxçŠ¶æ€ç ï¼‰
            error_count = len([m for m in all_metrics if m.get("status_code", 200) >= 500])

            # è®¡ç®—æ¯åˆ†é’Ÿè¯·æ±‚æ•°ï¼ˆåŸºäºæœ€è¿‘çš„æ—¶é—´çª—å£ï¼‰
            now = datetime.now()
            recent_requests = [
                m for m in all_metrics if (now - datetime.fromisoformat(m["timestamp"])).total_seconds() <= 60
            ]
            requests_per_minute = len(recent_requests)

            return {
                "total_requests": total_requests,
                "avg_response_time": round(avg_response_time, 2),
                "requests_per_minute": requests_per_minute,
                "error_count": error_count,
            }

    def get_slow_requests(self, limit: int = 20):
        """
        è·å–æ…¢è¯·æ±‚åˆ—è¡¨

        Args:
            limit: è¿”å›æ•°é‡é™åˆ¶

        Returns:
            list: æ…¢è¯·æ±‚åˆ—è¡¨
        """
        with self.lock:
            all_metrics = [m for ms in self.metrics.values() for m in ms]

            # ç­›é€‰æ…¢è¯·æ±‚å¹¶æŒ‰è€—æ—¶æ’åº
            slow_requests = [
                {**m, "path": path}
                for path, ms in self.metrics.items()
                for m in ms
                if m["duration"] > self.slow_request_threshold
            ]

            # æŒ‰è€—æ—¶é™åºæ’åº
            slow_requests.sort(key=lambda x: x["duration"], reverse=True)

            return slow_requests[:limit]

    def get_detailed_stats(self, hours: int = 1):
        """
        è·å–è¯¦ç»†æ€§èƒ½ç»Ÿè®¡ï¼ˆæŒ‰è·¯å¾„åˆ†ç»„ï¼‰

        Args:
            hours: ç»Ÿè®¡æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰

        Returns:
            dict: æŒ‰è·¯å¾„åˆ†ç»„çš„è¯¦ç»†ç»Ÿè®¡
        """
        with self.lock:
            now = datetime.now()
            cutoff_time = now.timestamp() - (hours * 3600)

            result = {}
            for path, metrics in self.metrics.items():
                # ç­›é€‰æ—¶é—´èŒƒå›´å†…çš„è¯·æ±‚
                recent_metrics = [
                    m for m in metrics if datetime.fromisoformat(m["timestamp"]).timestamp() > cutoff_time
                ]

                if not recent_metrics:
                    continue

                durations = [m["duration"] for m in recent_metrics]
                result[path] = {
                    "count": len(recent_metrics),
                    "avg_duration": sum(durations) / len(durations),
                    "max_duration": max(durations),
                    "min_duration": min(durations),
                    "p95_duration": sorted(durations)[int(len(durations) * 0.95)] if len(durations) > 0 else 0,
                    "error_count": len([m for m in recent_metrics if m.get("status_code", 200) >= 500]),
                }

            return result


# å…¨å±€æ€§èƒ½ç›‘æ§å®ä¾‹
performance_monitor = PerformanceMonitor()


async def performance_monitoring_middleware(request: Request, call_next: Callable):
    """
    æ€§èƒ½ç›‘æ§ä¸­é—´ä»¶

    è®°å½•æ‰€æœ‰ API è¯·æ±‚çš„å“åº”æ—¶é—´
    """
    # è·³è¿‡é™æ€æ–‡ä»¶å’Œ WebSocket
    if request.url.path.startswith("/static") or request.url.path.startswith("/ws"):
        return await call_next(request)

    start_time = time.time()

    try:
        response = await call_next(request)
        duration = time.time() - start_time

        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        performance_monitor.record_request(
            path=request.url.path, method=request.method, duration=duration, status_code=response.status_code
        )

        # æ·»åŠ æ€§èƒ½å“åº”å¤´
        response.headers["X-Response-Time"] = f"{duration:.3f}s"

        # æ­£å¸¸è¯·æ±‚è®°å½•ï¼ˆINFO çº§åˆ«ï¼‰
        if duration < 0.5:
            logger.info(f"âš¡ {request.method} {request.url.path} - {response.status_code} - {duration:.3f}s")
        elif duration < 1.0:
            logger.info(f"ğŸ”¶ {request.method} {request.url.path} - {response.status_code} - {duration:.3f}s")
        else:
            logger.warning(f"ğŸŒ {request.method} {request.url.path} - {response.status_code} - {duration:.3f}s")

        return response

    except Exception as e:
        duration = time.time() - start_time
        logger.error(f"âŒ {request.method} {request.url.path} - ERROR - {duration:.3f}s - {str(e)}")

        # è®°å½•å¤±è´¥è¯·æ±‚
        performance_monitor.record_request(
            path=request.url.path, method=request.method, duration=duration, status_code=500
        )

        raise


class LLMPerformanceTracker:
    """LLM è°ƒç”¨æ€§èƒ½è¿½è¸ªå™¨"""

    def __init__(self):
        self.llm_metrics_file = Path(__file__).parent.parent.parent / "logs" / "llm_metrics.jsonl"
        self.llm_metrics_file.parent.mkdir(exist_ok=True)

    def record_llm_call(self, model: str, operation: str, duration: float, tokens: int = 0, success: bool = True):
        """è®°å½• LLM è°ƒç”¨æ€§èƒ½"""
        try:
            metric = {
                "timestamp": datetime.now().isoformat(),
                "model": model,
                "operation": operation,
                "duration": round(duration, 3),
                "tokens": tokens,
                "success": success,
            }

            with open(self.llm_metrics_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(metric, ensure_ascii=False) + "\n")

            # è®°å½•æ…¢ LLM è°ƒç”¨
            if duration > 5.0:
                logger.warning(f"ğŸŒ LLM æ…¢è°ƒç”¨: {model} - {operation} - {duration:.2f}ç§’")
            else:
                logger.debug(f"ğŸ¤– LLM è°ƒç”¨: {model} - {operation} - {duration:.2f}ç§’ - {tokens} tokens")

        except Exception as e:
            logger.error(f"âŒ è®°å½• LLM æ€§èƒ½å¤±è´¥: {e}")


# å…¨å±€ LLM æ€§èƒ½è¿½è¸ªå™¨
llm_tracker = LLMPerformanceTracker()
