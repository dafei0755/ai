"""æˆ˜ç•¥æ ¡å‡†é—®å·èŠ‚ç‚¹"""

import json
import os
from datetime import datetime
from typing import Dict, Any, Literal, Optional, Tuple, List
from loguru import logger
from langgraph.types import interrupt, Command
from langgraph.store.base import BaseStore

from ...core.state import ProjectAnalysisState
from ...core.workflow_flags import WorkflowFlagManager
from ..questionnaire import (
    QuestionContext,
    FallbackQuestionGenerator,
    PhilosophyQuestionGenerator,
    BiddingStrategyGenerator,
    ConflictQuestionGenerator,
    QuestionAdjuster,
    AnswerParser
)

# ğŸ†• v7.18: ç¯å¢ƒå˜é‡æ§åˆ¶æ˜¯å¦ä½¿ç”¨ QuestionnaireAgent
USE_V718_QUESTIONNAIRE_AGENT = os.getenv("USE_V718_QUESTIONNAIRE_AGENT", "false").lower() == "true"


class CalibrationQuestionnaireNode:
    """æˆ˜ç•¥æ ¡å‡†é—®å·èŠ‚ç‚¹"""

    @staticmethod
    def _identify_scenario_type(user_input: str, structured_data: Dict[str, Any]) -> str:
        """
        ğŸš€ P0ä¼˜åŒ–ï¼šè¯†åˆ«é¡¹ç›®åœºæ™¯ç±»å‹ï¼Œé¿å…ç”Ÿæˆæ— å…³é—®é¢˜

        åœºæ™¯ç±»å‹ï¼š
        - bidding_strategy: ç«æ ‡/ç­–ç•¥å’¨è¯¢åœºæ™¯ï¼ˆB2Bä¸“ä¸šäººå£«ï¼‰
        - design_execution: è®¾è®¡æ‰§è¡Œ/æ–½å·¥åœºæ™¯ï¼ˆå®é™…é¡¹ç›®è½åœ°ï¼‰
        - concept_exploration: æ¦‚å¿µæ¢ç´¢åœºæ™¯ï¼ˆCç«¯ç”¨æˆ·ï¼‰
        - unknown: æœªçŸ¥åœºæ™¯ï¼ˆä½¿ç”¨ä¿å®ˆç­–ç•¥ï¼‰
        """
        # å…³é”®è¯ç°‡å®šä¹‰
        bidding_keywords = ["ç«æ ‡", "æŠ•æ ‡", "ç­–ç•¥", "å¯¹æ‰‹", "å¦‚ä½•å–èƒœ", "å·®å¼‚åŒ–", "çªå›´", "è¯„å§”", "æ–¹æ¡ˆç«äº‰"]
        execution_keywords = ["æ–½å·¥", "å·¥æœŸ", "ææ–™", "é¢„ç®—", "è½åœ°", "å®æ–½", "å»ºé€ ", "è£…ä¿®"]

        # è®¡ç®—æ¿€æ´»åº¦
        bidding_score = sum(1 for kw in bidding_keywords if kw in user_input)
        execution_score = sum(1 for kw in execution_keywords if kw in user_input)

        # åˆ¤æ–­åœºæ™¯
        if bidding_score >= 2:
            logger.info(f"ğŸ¯ åœºæ™¯è¯†åˆ«ï¼šç«æ ‡ç­–ç•¥åœºæ™¯ï¼ˆbidding_score={bidding_score}ï¼‰")
            return "bidding_strategy"
        elif execution_score >= 2:
            logger.info(f"ğŸ¯ åœºæ™¯è¯†åˆ«ï¼šè®¾è®¡æ‰§è¡Œåœºæ™¯ï¼ˆexecution_score={execution_score}ï¼‰")
            return "design_execution"
        else:
            logger.info(f"ğŸ¯ åœºæ™¯è¯†åˆ«ï¼šæœªçŸ¥åœºæ™¯")
            return "unknown"

    # ğŸ”§ ä¿®å¤: ç§»é™¤é‡å¤çš„ _build_conflict_questions æ–¹æ³•
    # è¯¥æ–¹æ³•ä¸ ConflictQuestionGenerator.generate() é‡å¤
    # ç°åœ¨ç»Ÿä¸€ä½¿ç”¨ ConflictQuestionGenerator.generate()

    @staticmethod
    def _extract_raw_answers(user_response: Any) -> Tuple[Optional[Any], str]:
        """ä»ç”¨æˆ·å“åº”ä¸­æå–é—®å·ç­”æ¡ˆåŸå§‹ç»“æ„å’Œè¡¥å……è¯´æ˜."""
        if user_response is None:
            return None, ""

        additional_notes = ""
        raw_answers: Optional[Any] = None

        if isinstance(user_response, dict):
            additional_notes = str(user_response.get("additional_info") or user_response.get("notes") or "").strip()
            raw_answers = (
                user_response.get("answers")
                or user_response.get("entries")
                or user_response.get("responses")
            )
        elif isinstance(user_response, list):
            raw_answers = user_response
        elif isinstance(user_response, str):
            stripped = user_response.strip()
            if not stripped:
                return None, ""
            try:
                parsed = json.loads(stripped)
            except json.JSONDecodeError:
                return None, ""
            if isinstance(parsed, dict):
                additional_notes = str(parsed.get("additional_info") or parsed.get("notes") or "").strip()
                raw_answers = parsed.get("answers") or parsed.get("entries") or parsed
            elif isinstance(parsed, list):
                raw_answers = parsed

        return raw_answers, additional_notes

    @staticmethod
    def _build_answer_entries(
        questionnaire: Dict[str, Any],
        raw_answers: Optional[Any]
    ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """å°†åŸå§‹ç­”æ¡ˆä¸é—®å·å…ƒæ•°æ®åˆå¹¶ä¸ºç»“æ„åŒ–æ‘˜è¦."""
        if not raw_answers:
            return [], {}

        answer_lookup: Dict[str, Any] = {}

        if isinstance(raw_answers, dict):
            answer_lookup = {str(key): value for key, value in raw_answers.items()}
        elif isinstance(raw_answers, list):
            for idx, item in enumerate(raw_answers, 1):
                if not isinstance(item, dict):
                    continue
                q_id = item.get("question_id") or item.get("id") or f"Q{idx}"
                answer_value = (
                    item.get("answer")
                    or item.get("value")
                    or item.get("response")
                    or item.get("selected")
                    or item.get("answers")
                )
                if answer_value is None:
                    continue
                key = str(q_id)
                answer_lookup[key] = answer_value
                question_label = item.get("question")
                if question_label:
                    answer_lookup[str(question_label)] = answer_value
        else:
            return [], {}

        questions = questionnaire.get("questions", []) if questionnaire else []
        entries: List[Dict[str, Any]] = []
        compact_answers: Dict[str, Any] = {}

        for idx, question in enumerate(questions, 1):
            q_id = question.get("id") or f"Q{idx}"
            potential_keys = [
                str(q_id),
                f"q{idx}",
                question.get("question"),
                str(idx)
            ]

            answer_value = None
            for key in potential_keys:
                if key is None:
                    continue
                key_str = str(key)
                if key_str in answer_lookup:
                    answer_value = answer_lookup[key_str]
                    break
                if key in answer_lookup:  # å…¼å®¹åŸå§‹é”®
                    answer_value = answer_lookup[key]
                    break

            if answer_value is None:
                continue

            normalized_value = CalibrationQuestionnaireNode._normalize_answer_value(question, answer_value)
            if normalized_value is None:
                continue

            entry = {
                "id": q_id,
                "question": question.get("question", ""),
                "value": normalized_value,
                "type": question.get("type"),
                "context": question.get("context", "")
            }
            entries.append(entry)
            compact_answers[q_id] = normalized_value

        return entries, compact_answers

    @staticmethod
    def _normalize_answer_value(question: Dict[str, Any], answer: Any) -> Optional[Any]:
        """æ ¹æ®é¢˜å‹å¯¹ç­”æ¡ˆè¿›è¡Œå½’ä¸€åŒ–ï¼Œä¾¿äºåç»­å¤„ç†."""
        if answer is None:
            return None

        q_type = (question.get("type") or "open_ended").lower()

        if q_type == "multiple_choice":
            if isinstance(answer, str):
                values = [item.strip() for item in answer.split(",") if item.strip()]
                return values or None
            if isinstance(answer, (list, tuple, set)):
                values = [str(item).strip() for item in answer if str(item).strip()]
                return values or None
            coerced = str(answer).strip()
            return [coerced] if coerced else None

        if q_type == "single_choice":
            if isinstance(answer, (list, tuple, set)):
                for item in answer:
                    candidate = str(item).strip()
                    if candidate:
                        return candidate
                return None
            return str(answer).strip() or None

        if isinstance(answer, (list, tuple, set)):
            values = [str(item).strip() for item in answer if str(item).strip()]
            return "ã€".join(values) if values else None

        if isinstance(answer, dict):
            try:
                serialized = json.dumps(answer, ensure_ascii=False)
            except (TypeError, ValueError):
                serialized = str(answer)
            return serialized.strip() or None

        return str(answer).strip() or None

    @staticmethod
    def execute(
        state: ProjectAnalysisState,
        store: Optional[BaseStore] = None
    ) -> Command[Literal["requirements_confirmation", "requirements_analyst"]]:
        """
        æ‰§è¡Œæˆ˜ç•¥æ ¡å‡†é—®å·äº¤äº’

        æ ¹æ®éœ€æ±‚åˆ†æå¸ˆæ–‡æ¡£v1.0çš„è¦æ±‚ï¼š
        åœ¨å®Œæˆéœ€æ±‚åˆ†æåï¼Œå¿…é¡»ç”Ÿæˆ"æˆ˜ç•¥æ ¡å‡†é—®å·"å¹¶ç­‰å¾…ç”¨æˆ·å›ç­”ã€‚
        è¿™æ˜¯ç¡®ä¿æˆ˜ç•¥æ–¹å‘æ­£ç¡®æ— è¯¯çš„å…³é”®æ­¥éª¤ã€‚

        Args:
            state: é¡¹ç›®åˆ†æçŠ¶æ€
            store: å­˜å‚¨æ¥å£

        Returns:
            Commandå¯¹è±¡ï¼ŒæŒ‡å‘ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
        """
        logger.info("=" * 80)
        logger.info("ğŸ¯ Starting calibration questionnaire interaction")
        logger.info("=" * 80)

        # ğŸ” è¯Šæ–­æ—¥å¿—ï¼šæ£€æŸ¥ skip_calibration æ ‡å¿—
        logger.info(f"ğŸ” [DEBUG] skip_calibration æ ‡å¿—: {state.get('skip_calibration')}")

        # ğŸ†• v7.4: é—®å·ä¸å¯è·³è¿‡
        # åŸ v3.7 é€»è¾‘å·²ç§»é™¤ï¼šå³ä½¿æ˜¯ä¸­ç­‰å¤æ‚åº¦ä»»åŠ¡ï¼Œä¹Ÿå¿…é¡»å®Œæˆé—®å·
        # ä½†å¯ä»¥å‡å°‘é—®é¢˜æ•°é‡ï¼ˆé€šè¿‡ QuestionAdjuster åŠ¨æ€è°ƒæ•´ï¼‰
        if state.get("skip_calibration"):
            logger.warning("âš ï¸ [v7.4] skip_calibration æ ‡å¿—å·²è®¾ç½®ï¼Œä½†é—®å·ä¸å¯è·³è¿‡ã€‚å°†ç”Ÿæˆç²¾ç®€ç‰ˆé—®å·ã€‚")
            # ä¸å†è·³è¿‡ï¼Œç»§ç»­æ‰§è¡Œé—®å·ç”Ÿæˆ
            # åç»­é€šè¿‡ QuestionAdjuster å‡å°‘é—®é¢˜æ•°é‡

        # âœ… è¿½é—®æ¨¡å¼ä¸‹ç›´æ¥è·³è¿‡
        if state.get("is_followup"):
            logger.info("â© Follow-up session detected, skipping calibration questionnaire")
            # è‡ªåŠ¨ä¿ç•™æŒä¹…åŒ–æ ‡å¿—
            update_dict = {"calibration_processed": True, "calibration_skipped": True}
            update_dict = WorkflowFlagManager.preserve_flags(state, update_dict)

            return Command(
                update=update_dict,
                goto="requirements_confirmation"
            )

        # âœ… æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡é—®å·ï¼ˆé¿å…æ­»å¾ªç¯ï¼‰
        calibration_processed = state.get("calibration_processed")
        calibration_answers = state.get("calibration_answers")
        questionnaire_summary = state.get("questionnaire_summary")
        
        logger.info(f"ğŸ” [DEBUG] calibration_processed æ ‡å¿—: {calibration_processed}")
        logger.info(f"ğŸ” [DEBUG] calibration_answers å­˜åœ¨: {bool(calibration_answers)}")
        logger.info(f"ğŸ” [DEBUG] questionnaire_summary å­˜åœ¨: {bool(questionnaire_summary)}")
        
        # ğŸ›¡ï¸ v7.24 å¢å¼ºé˜²å¾¡ï¼šæ£€æŸ¥å¤šä¸ªä¿¡å·æºåˆ¤æ–­é—®å·æ˜¯å¦å·²å¤„ç†
        # ä¿¡å·æºä¼˜å…ˆçº§ï¼šcalibration_processed > calibration_answers > questionnaire_summary
        if not calibration_processed:
            if calibration_answers:
                logger.warning("âš ï¸ v7.24: calibration_processed=False ä½† calibration_answers å­˜åœ¨ï¼Œè§†ä¸ºå·²å¤„ç†")
                calibration_processed = True
            elif questionnaire_summary and questionnaire_summary.get("answers"):
                logger.warning("âš ï¸ v7.24: calibration_processed=False ä½† questionnaire_summary.answers å­˜åœ¨ï¼Œè§†ä¸ºå·²å¤„ç†")
                calibration_processed = True
        
        if calibration_processed:
            logger.info("âœ… Calibration already processed, skipping to requirements confirmation")
            logger.info("ğŸ”„ [DEBUG] Returning Command(goto='requirements_confirmation')")
            # è‡ªåŠ¨ä¿ç•™æŒä¹…åŒ–æ ‡å¿—
            update_dict = {"calibration_processed": True}  # ğŸ”§ v7.24: æ˜¾å¼è®¾ç½®ï¼Œç¡®ä¿ä¼ é€’
            update_dict = WorkflowFlagManager.preserve_flags(state, update_dict)
            return Command(
                update=update_dict,
                goto="requirements_confirmation"
            )

        # è·å–éœ€æ±‚åˆ†æç»“æœ
        agent_results = state.get("agent_results", {})
        requirements_result = agent_results.get("requirements_analyst")

        logger.info(f"ğŸ“Š Debug - requirements_result exists: {bool(requirements_result)}")
        if requirements_result:
            logger.info(f"ğŸ“Š Debug - requirements_result keys: {list(requirements_result.keys())}")

        if not requirements_result:
            logger.warning("âš ï¸ No requirements analysis found, returning to requirements analyst")
            return Command(
                update={"error": "No requirements analysis found"},
                goto="requirements_analyst"
            )

        # ğŸ†• v7.3: é—®å·ç”Ÿæˆæ¶æ„è°ƒæ•´
        # è·å–åˆ†æç»“æœï¼Œç”¨äºåŠ¨æ€ç”Ÿæˆé—®å·
        structured_data = requirements_result.get("structured_data") or {}

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é¢„ç”Ÿæˆçš„é—®å·ï¼ˆå‘åå…¼å®¹æ—§æ•°æ®ï¼‰
        questionnaire_from_agent = structured_data.get("calibration_questionnaire") or {}
        state_questionnaire = state.get("calibration_questionnaire") or {}

        logger.info(f"ğŸ“Š åˆ†æç»“æœå­—æ®µ: {list(structured_data.keys())}")

        # ğŸ”¥ v7.4.3: è·å– user_inputï¼ˆåœ¨æ‰€æœ‰ä»£ç å—ä¹‹å‰å®šä¹‰ï¼Œç¡®ä¿å…¨å±€å¯ç”¨ï¼‰
        user_input = state.get("user_input", "")

        # ğŸ†• v7.3: æ–°æ¶æ„ - é—®å·åŠ¨æ€ç”Ÿæˆé€»è¾‘
        # ä¼˜å…ˆä½¿ç”¨åŸºäºåˆ†æç»“æœçš„åŠ¨æ€ç”Ÿæˆï¼Œè€Œéä¾èµ–é¢„ç”Ÿæˆé—®å·
        questionnaire = None
        generation_source = None

        # Step 1: æ£€æŸ¥æ˜¯å¦æœ‰é¢„ç”Ÿæˆé—®å·ï¼ˆå‘åå…¼å®¹ï¼‰
        if questionnaire_from_agent.get("questions") and questionnaire_from_agent.get("source") != "to_be_regenerated":
            questionnaire = questionnaire_from_agent
            generation_source = "llm_pregenerated"
            logger.info(f"â„¹ï¸ ä½¿ç”¨LLMé¢„ç”Ÿæˆçš„é—®å·ï¼ˆå‘åå…¼å®¹æ—§æ¶æ„ï¼‰ï¼š{len(questionnaire.get('questions', []))} ä¸ªé—®é¢˜")
        elif state_questionnaire.get("questions"):
            questionnaire = state_questionnaire
            generation_source = "state_persisted"
            logger.info(f"â„¹ï¸ ä½¿ç”¨stateä¸­æŒä¹…åŒ–çš„é—®å·ï¼š{len(questionnaire.get('questions', []))} ä¸ªé—®é¢˜")

        # Step 2: å¦‚æœæ²¡æœ‰é¢„ç”Ÿæˆé—®å·ï¼Œæˆ–æ ‡è®°ä¸ºéœ€è¦é‡æ–°ç”Ÿæˆï¼Œåˆ™åŠ¨æ€ç”Ÿæˆ
        if not questionnaire or not questionnaire.get("questions"):
            logger.info("ğŸš€ v7.5æ–°æ¶æ„ï¼šLLMé©±åŠ¨çš„æ™ºèƒ½é—®å·ç”Ÿæˆï¼ˆæå‡é—®é¢˜ä¸ç”¨æˆ·éœ€æ±‚çš„ç»“åˆåº¦ï¼‰")

            # ğŸ†• v7.18: ä¼˜å…ˆä½¿ç”¨ QuestionnaireAgent (StateGraph) ç”Ÿæˆé—®å·
            if USE_V718_QUESTIONNAIRE_AGENT:
                try:
                    from ...agents.questionnaire_agent import QuestionnaireAgent
                    
                    logger.info("ğŸ¤– [v7.18] ä½¿ç”¨ QuestionnaireAgent (StateGraph) ç”Ÿæˆé—®å·...")
                    questionnaire_agent = QuestionnaireAgent(llm_model=None)
                    base_questions, generation_method = questionnaire_agent.generate(
                        user_input=user_input,
                        structured_data=structured_data
                    )
                    
                    if base_questions and generation_method in ("llm_generated", "regenerated"):
                        logger.info(f"âœ… [v7.18] QuestionnaireAgent ç”ŸæˆæˆåŠŸï¼š{len(base_questions)} ä¸ªé—®é¢˜")
                        questionnaire = {
                            "introduction": "ä»¥ä¸‹é—®é¢˜åŸºäºæ‚¨çš„å…·ä½“éœ€æ±‚å®šåˆ¶ï¼Œæ—¨åœ¨æ·±å…¥ç†è§£æ‚¨çš„æœŸæœ›å’Œåå¥½",
                            "questions": base_questions,
                            "note": "è¿™äº›é—®é¢˜ç›´æ¥é’ˆå¯¹æ‚¨æåˆ°çš„å…·ä½“å†…å®¹ï¼Œå¸®åŠ©æˆ‘ä»¬æä¾›æ›´ç²¾å‡†çš„è®¾è®¡å»ºè®®",
                            "source": generation_method,
                            "generation_method": "stategraph_agent"
                        }
                        generation_source = generation_method
                    else:
                        logger.warning(f"âš ï¸ [v7.18] QuestionnaireAgent è¿”å›å›é€€æ–¹æ¡ˆï¼Œå°†ä½¿ç”¨è§„åˆ™ç”Ÿæˆ")
                        raise Exception("QuestionnaireAgent è¿”å›å›é€€æ–¹æ¡ˆ")
                        
                except Exception as agent_error:
                    logger.warning(f"âš ï¸ [v7.18] QuestionnaireAgent å¤±è´¥: {agent_error}ï¼Œå›é€€åˆ° LLMQuestionGenerator")
                    # å›é€€åˆ°åŸæœ‰ v7.5 é€»è¾‘
                    USE_V718_QUESTIONNAIRE_AGENT_FALLBACK = True
            else:
                USE_V718_QUESTIONNAIRE_AGENT_FALLBACK = True
            
            # v7.5 åŸæœ‰é€»è¾‘ï¼ˆä½œä¸º v7.18 çš„å›é€€æˆ–ç‹¬ç«‹ä½¿ç”¨ï¼‰
            if not questionnaire or not questionnaire.get("questions"):
                try:
                    from ..questionnaire.llm_generator import LLMQuestionGenerator
                    
                    logger.info("ğŸ¤– [v7.5] å°è¯•ä½¿ç”¨ LLM ç”Ÿæˆé—®å·...")
                    base_questions, generation_method = LLMQuestionGenerator.generate(
                        user_input=user_input,
                        structured_data=structured_data,
                        llm_model=None,  # ä½¿ç”¨é»˜è®¤LLMå®ä¾‹
                        timeout=30
                    )
                    
                    if base_questions and generation_method == "llm_generated":
                        logger.info(f"âœ… [v7.5] LLMç”ŸæˆæˆåŠŸï¼š{len(base_questions)} ä¸ªå®šåˆ¶é—®é¢˜")
                        questionnaire = {
                            "introduction": "ä»¥ä¸‹é—®é¢˜åŸºäºæ‚¨çš„å…·ä½“éœ€æ±‚å®šåˆ¶ï¼Œæ—¨åœ¨æ·±å…¥ç†è§£æ‚¨çš„æœŸæœ›å’Œåå¥½",
                            "questions": base_questions,
                            "note": "è¿™äº›é—®é¢˜ç›´æ¥é’ˆå¯¹æ‚¨æåˆ°çš„å…·ä½“å†…å®¹ï¼Œå¸®åŠ©æˆ‘ä»¬æä¾›æ›´ç²¾å‡†çš„è®¾è®¡å»ºè®®",
                            "source": "llm_generated",
                            "generation_method": "llm_driven"
                        }
                        generation_source = "llm_generated"
                    else:
                        logger.warning(f"âš ï¸ [v7.5] LLMç”Ÿæˆè¿”å›å›é€€æ–¹æ¡ˆï¼Œå°†ä½¿ç”¨è§„åˆ™ç”Ÿæˆ")
                        raise Exception("LLMè¿”å›å›é€€æ–¹æ¡ˆ")
                        
                except Exception as llm_error:
                    logger.warning(f"âš ï¸ [v7.5] LLMç”Ÿæˆå¤±è´¥: {llm_error}ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ")
                    
                    # ğŸ”„ å›é€€åˆ°åŸæœ‰çš„ FallbackQuestionGenerator
                    logger.info("ğŸ”„ [v7.5] å›é€€åˆ°è§„åˆ™é©±åŠ¨çš„é—®å·ç”Ÿæˆ...")
                    
                    # æ™ºèƒ½æå–å…³é”®ä¿¡æ¯
                    from ..questionnaire.context import KeywordExtractor
                    import sys

                    try:
                        extracted_info = KeywordExtractor.extract(user_input, structured_data)
                        logger.info(f"ğŸ” å…³é”®è¯æå–å®Œæˆï¼Œæå–äº† {len(extracted_info)} ä¸ªå­—æ®µ")
                    except Exception as e:
                        logger.error(f"âŒ KeywordExtractor.extract() å¤±è´¥: {e}")
                        extracted_info = KeywordExtractor._empty_result()

                    # ä½¿ç”¨ FallbackQuestionGenerator ç”ŸæˆåŸºç¡€é—®é¢˜é›†
                    base_questions = FallbackQuestionGenerator.generate(
                        structured_data,
                        user_input=user_input,
                        extracted_info=extracted_info
                    )
                    logger.info(f"âœ… è§„åˆ™ç”Ÿæˆå®Œæˆï¼š{len(base_questions)} ä¸ªé—®é¢˜")

                    questionnaire = {
                        "introduction": "ä»¥ä¸‹é—®é¢˜æ—¨åœ¨æ·±å…¥ç†è§£æ‚¨çš„éœ€æ±‚å’ŒæœŸæœ›ï¼Œå¸®åŠ©æˆ‘ä»¬æä¾›æ›´ç²¾å‡†çš„è®¾è®¡å»ºè®®",
                        "questions": base_questions,
                        "note": "åŸºäºæ‚¨çš„éœ€æ±‚æ·±åº¦åˆ†æç»“æœç”Ÿæˆçš„å®šåˆ¶é—®å·",
                        "source": "dynamic_generation",
                        "generation_method": "rule_based_fallback"
                    }
                    generation_source = "dynamic_generation"

        logger.info(f"ğŸ” é—®å·æº: {generation_source}, é—®é¢˜æ•°: {len(questionnaire.get('questions', []))}")

        # ğŸ†• V1é›†æˆï¼šåŸºäºæˆ˜ç•¥æ´å¯Ÿæ³¨å…¥ç†å¿µæ¢ç´¢é—®é¢˜ï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
        logger.info(f"ğŸ” [DEBUG] Step 2: æ„å»ºç†å¿µæ¢ç´¢é—®é¢˜...")
        try:
            philosophy_questions = PhilosophyQuestionGenerator.generate(structured_data)
            logger.info(f"ğŸ” [DEBUG] Step 2 å®Œæˆ: ç”Ÿæˆ {len(philosophy_questions)} ä¸ªç†å¿µé—®é¢˜")
        except Exception as e:
            logger.error(f"âŒ [DEBUG] Step 2 å¼‚å¸¸: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            philosophy_questions = []
        if philosophy_questions:
            logger.info(f"ğŸ¨ V1æˆ˜ç•¥æ´å¯Ÿç”Ÿæˆ {len(philosophy_questions)} ä¸ªç†å¿µæ¢ç´¢é—®é¢˜")

        # ğŸš€ P0ä¼˜åŒ–ï¼šè¯†åˆ«åœºæ™¯ç±»å‹ï¼Œé¿å…ç”Ÿæˆæ— å…³é—®é¢˜
        logger.info(f"ğŸ” [DEBUG] Step 2.5: è¯†åˆ«åœºæ™¯ç±»å‹...")
        # user_input å·²åœ¨ç¬¬305è¡Œå®šä¹‰
        scenario_type = CalibrationQuestionnaireNode._identify_scenario_type(user_input, structured_data)
        logger.info(f"ğŸ” [DEBUG] Step 2.5 å®Œæˆ: scenario_type={scenario_type}")

        # ğŸš€ P1ä¼˜åŒ–ï¼šç«æ ‡ç­–ç•¥åœºæ™¯ç”Ÿæˆä¸“ç”¨é—®é¢˜
        bidding_strategy_questions = []
        if scenario_type == "bidding_strategy":
            logger.info(f"ğŸ” [DEBUG] Step 2.6: æ„å»ºç«æ ‡ç­–ç•¥ä¸“ç”¨é—®é¢˜...")
            try:
                bidding_strategy_questions = BiddingStrategyGenerator.generate(user_input, structured_data)
                logger.info(f"ğŸ” [DEBUG] Step 2.6 å®Œæˆ: ç”Ÿæˆ {len(bidding_strategy_questions)} ä¸ªç«æ ‡ç­–ç•¥é—®é¢˜")
            except Exception as e:
                logger.error(f"âŒ [DEBUG] Step 2.6 å¼‚å¸¸: {type(e).__name__}: {e}")
                import traceback
                traceback.print_exc()
                bidding_strategy_questions = []
            if bidding_strategy_questions:
                logger.info(f"ğŸ¯ [P1] ç«æ ‡ç­–ç•¥åœºæ™¯æ³¨å…¥ {len(bidding_strategy_questions)} ä¸ªä¸“ç”¨é—®é¢˜")

        # ğŸ†• V1.5é›†æˆï¼šåˆ©ç”¨å¯è¡Œæ€§åˆ†æç»“æœæ³¨å…¥èµ„æºå†²çªé—®é¢˜ï¼ˆä»·å€¼ä½“ç°ç‚¹1ï¼‰
        # ğŸ†• v7.4: å†²çªé—®é¢˜å¿…é¡»ç”±ç”¨æˆ·çº¦æŸæ¿€æ´»
        logger.info(f"ğŸ” [DEBUG] Step 3: æ„å»ºèµ„æºå†²çªé—®é¢˜...")
        feasibility = state.get("feasibility_assessment", {})
        conflict_questions = []

        # ğŸ†• v7.4: è·å–ç”¨æˆ·æåŠçš„çº¦æŸï¼ˆä» extracted_info æˆ–é‡æ–°æå–ï¼‰
        if 'extracted_info' not in dir() or extracted_info is None:
            from ..questionnaire.context import KeywordExtractor
            extracted_info = KeywordExtractor.extract(user_input, structured_data)
        user_mentioned_constraints = extracted_info.get("user_mentioned_constraints", [])

        if feasibility:
            try:
                # ğŸš€ v7.4ä¼˜åŒ–ï¼šä¼ å…¥ user_mentioned_constraints å‚æ•°
                conflict_questions = ConflictQuestionGenerator.generate(
                    feasibility,
                    scenario_type,
                    user_mentioned_constraints=user_mentioned_constraints
                )
                logger.info(f"ğŸ” [DEBUG] Step 3 å®Œæˆ: ç”Ÿæˆ {len(conflict_questions)} ä¸ªå†²çªé—®é¢˜")
            except Exception as e:
                logger.error(f"âŒ [DEBUG] Step 3 å¼‚å¸¸: {type(e).__name__}: {e}")
                import traceback
                traceback.print_exc()
                conflict_questions = []
            if conflict_questions:
                logger.info(f"ğŸ” V1.5å¯è¡Œæ€§åˆ†ææ£€æµ‹åˆ°å†²çªï¼Œæ³¨å…¥ {len(conflict_questions)} ä¸ªèµ„æºçº¦æŸé—®é¢˜")
            elif user_mentioned_constraints:
                logger.info(f"ğŸ” [v7.4] ç”¨æˆ·æåŠçº¦æŸ {user_mentioned_constraints}ï¼Œä½†æ— å¯¹åº”å†²çªæ£€æµ‹")
        else:
            logger.info(f"ğŸ” [DEBUG] Step 3 è·³è¿‡: feasibility ä¸ºç©º")

        # ğŸ†• P2åŠŸèƒ½ï¼šåŠ¨æ€è°ƒæ•´é—®é¢˜æ•°é‡
        # æ ¹æ®é—®å·é•¿åº¦ã€å†²çªä¸¥é‡æ€§ã€V1è¾“å‡ºä¸°å¯Œåº¦æ™ºèƒ½è£å‰ª
        logger.info(f"ğŸ” [DEBUG] Step 4: åŠ¨æ€è°ƒæ•´é—®é¢˜æ•°é‡...")
        original_questions = questionnaire.get("questions", [])
        try:
            adjusted_philosophy_questions, adjusted_conflict_questions = QuestionAdjuster.adjust(
                philosophy_questions=philosophy_questions,
                conflict_questions=conflict_questions,
                original_question_count=len(original_questions),
                feasibility_data=feasibility
            )
            logger.info(f"ğŸ” [DEBUG] Step 4 å®Œæˆ: è°ƒæ•´åç†å¿µ={len(adjusted_philosophy_questions)}, å†²çª={len(adjusted_conflict_questions)}")
        except Exception as e:
            logger.error(f"âŒ [DEBUG] Step 4 å¼‚å¸¸: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            adjusted_philosophy_questions = philosophy_questions
            adjusted_conflict_questions = conflict_questions

        # åˆå¹¶è°ƒæ•´åçš„ç†å¿µé—®é¢˜å’Œå†²çªé—®é¢˜
        # ğŸš€ P1ä¼˜åŒ–ï¼šç«æ ‡ç­–ç•¥é—®é¢˜ä¼˜å…ˆçº§æœ€é«˜ï¼Œæ”¾åœ¨æœ€å‰é¢
        all_injected_questions = bidding_strategy_questions + adjusted_philosophy_questions + adjusted_conflict_questions

        if all_injected_questions:
            if bidding_strategy_questions:
                logger.info(f"ğŸ¯ [P1] ç«æ ‡ç­–ç•¥é—®é¢˜ {len(bidding_strategy_questions)} ä¸ªå·²åŠ å…¥æ³¨å…¥é˜Ÿåˆ—")
            logger.info(f"âœ¨ æ€»è®¡æ³¨å…¥ {len(all_injected_questions)} ä¸ªåŠ¨æ€é—®é¢˜ï¼ˆç†å¿µ{len(adjusted_philosophy_questions)}ä¸ª + èµ„æº{len(adjusted_conflict_questions)}ä¸ªï¼‰")
            if len(adjusted_philosophy_questions) < len(philosophy_questions) or len(adjusted_conflict_questions) < len(conflict_questions):
                logger.info(f"ğŸ“Š åŠ¨æ€è°ƒæ•´: ç†å¿µé—®é¢˜ {len(philosophy_questions)}â†’{len(adjusted_philosophy_questions)}, å†²çªé—®é¢˜ {len(conflict_questions)}â†’{len(adjusted_conflict_questions)}")

            # å°†é—®é¢˜æ’å…¥åˆ°é—®å·ä¸­ï¼ˆå•é€‰é¢˜ä¹‹åï¼‰
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªéå•é€‰é¢˜çš„ä½ç½®
            insert_position = 0
            for i, q in enumerate(original_questions):
                if q.get("type") != "single_choice":
                    insert_position = i
                    break
            # æ’å…¥æ‰€æœ‰åŠ¨æ€é—®é¢˜
            updated_questions = (
                original_questions[:insert_position] +
                all_injected_questions +
                original_questions[insert_position:]
            )
            questionnaire["questions"] = updated_questions
            logger.info(f"âœ… å·²å°†åŠ¨æ€é—®é¢˜æ’å…¥åˆ°ä½ç½® {insert_position}ï¼Œæ€»é—®é¢˜æ•°: {len(updated_questions)}")

        structured_data["calibration_questionnaire"] = questionnaire
        requirements_result["structured_data"] = structured_data
        agent_results["requirements_analyst"] = requirements_result
        state["calibration_questionnaire"] = questionnaire

        logger.info(f"âœ… Active questionnaire contains {len(questionnaire.get('questions', []))} questions")

        # ğŸ”§ ä¿®å¤é—®å·é¢˜å‹é¡ºåºï¼ˆç¡®ä¿ï¼šå•é€‰â†’å¤šé€‰â†’æ–‡å­—è¾“å…¥ï¼‰
        questions = questionnaire.get("questions", [])
        original_order = [q.get("type", "") for q in questions]
        
        single_choice = [q for q in questions if q.get("type") == "single_choice"]
        multiple_choice = [q for q in questions if q.get("type") == "multiple_choice"]
        open_ended = [q for q in questions if q.get("type") == "open_ended"]
        
        fixed_questions = single_choice + multiple_choice + open_ended
        fixed_order = [q.get("type", "") for q in fixed_questions]
        
        if original_order != fixed_order:
            logger.warning(f"âš ï¸ é—®å·é¢˜å‹é¡ºåºä¸æ­£ç¡®ï¼Œå·²è‡ªåŠ¨ä¿®å¤ï¼š")
            logger.warning(f"   åŸå§‹: {original_order}")
            logger.warning(f"   ä¿®å¤: {fixed_order}")
            logger.info(f"   ğŸ“Š ç»Ÿè®¡: {len(single_choice)}ä¸ªå•é€‰ + {len(multiple_choice)}ä¸ªå¤šé€‰ + {len(open_ended)}ä¸ªæ–‡å­—è¾“å…¥")
            questionnaire["questions"] = fixed_questions
        else:
            logger.info(f"âœ… é—®å·é¢˜å‹é¡ºåºæ­£ç¡®ï¼š{len(single_choice)}ä¸ªå•é€‰ â†’ {len(multiple_choice)}ä¸ªå¤šé€‰ â†’ {len(open_ended)}ä¸ªæ–‡å­—è¾“å…¥")

        warning_message = state.get("calibration_warning")
        skip_attempts = int(state.get("calibration_skip_attempts", 0) or 0)

        intent = ""
        content = ""
        additional_info = ""
        modifications = ""
        entries: List[Dict[str, Any]] = []
        answers_map: Dict[str, Any] = {}
        notes = ""

        skip_tokens = {"skip", "close", "cancel", "é€€å‡º", "å…³é—­", "å–æ¶ˆ", "æ”¾å¼ƒ"}

        from ...utils.intent_parser import parse_user_intent

        while True:
            message_text = "è¯·å›ç­”ä»¥ä¸‹æˆ˜ç•¥æ ¡å‡†é—®é¢˜ï¼Œä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£æ‚¨çš„éœ€æ±‚ï¼š"
            if warning_message:
                message_text = f"{warning_message}\n\n{message_text}"

            questionnaire_payload = {
                "interaction_type": "calibration_questionnaire",
                "message": message_text,
                "questionnaire": {
                    "introduction": questionnaire.get("introduction", "ä»¥ä¸‹é—®é¢˜æ—¨åœ¨ç²¾å‡†æ•æ‰æ‚¨åœ¨æˆ˜æœ¯æ‰§è¡Œå’Œç¾å­¦è¡¨è¾¾å±‚é¢çš„ä¸ªäººåå¥½"),
                    "questions": questionnaire.get("questions", []),
                    "note": questionnaire.get("note") or "è¿™äº›é—®é¢˜ä¸ä¼šæ”¹å˜æ ¸å¿ƒæˆ˜ç•¥æ–¹å‘ï¼Œåªæ˜¯å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°å®ç°æ—¢å®šç›®æ ‡"
                },
                "options": {
                    "submit": "æäº¤é—®å·ç­”æ¡ˆ"
                }
            }

            logger.info(f"ğŸ›‘ [QUESTIONNAIRE] å³å°†è°ƒç”¨ interrupt()ï¼Œç­‰å¾…ç”¨æˆ·è¾“å…¥...")
            logger.info(f"ğŸ›‘ [QUESTIONNAIRE] payload keys: {list(questionnaire_payload.keys())}")
            logger.info(f"ğŸ›‘ [QUESTIONNAIRE] questions count: {len(questionnaire_payload['questionnaire']['questions'])}")
            
            user_response = interrupt(questionnaire_payload)
            logger.info(f"Received questionnaire response: {type(user_response)}")

            intent_result = parse_user_intent(
                user_response,
                context="æˆ˜ç•¥æ ¡å‡†é—®å·",
                stage="calibration_questionnaire"
            )

            intent = intent_result["intent"]
            content = intent_result.get("content", "")
            additional_info = intent_result.get("additional_info", "")
            modifications = intent_result.get("modifications", "")

            logger.info(f"ğŸ’¬ ç”¨æˆ·æ„å›¾è§£æ: {intent} (æ–¹æ³•: {intent_result['method']})")

            normalized_response = ""
            if isinstance(user_response, str):
                normalized_response = user_response.strip().lower()
            elif isinstance(user_response, dict):
                possible_keys = [
                    str(user_response.get("intent", "")),
                    str(user_response.get("action", "")),
                    str(user_response.get("value", "")),
                    str(user_response.get("resume_value", ""))
                ]
                normalized_response = next((val.strip().lower() for val in possible_keys if val), "")

            raw_answers_payload, implicit_notes = AnswerParser.extract_raw_answers(user_response)
            answers_payload = intent_result.get("answers") or raw_answers_payload
            entries, answers_map = AnswerParser.build_answer_entries(questionnaire, answers_payload)

            notes = (additional_info or implicit_notes or content).strip()

            skip_detected = (intent in {"skip"}) or (normalized_response in skip_tokens)
            if skip_detected:
                logger.info("â­ï¸ User chose to skip questionnaire, proceeding without answers")
                # ğŸ”¥ åˆå§‹åŒ– updated_stateï¼ˆåœ¨ skip åˆ†æ”¯ä¸­æå‰è¿”å›éœ€è¦åˆå§‹åŒ–ï¼‰
                updated_state: Dict[str, Any] = {}
                updated_state["calibration_processed"] = True
                updated_state["calibration_skipped"] = True
                updated_state["calibration_skip_attempts"] = skip_attempts
                updated_state["agent_results"] = agent_results

                # ä¿ç•™ skip_unified_review æ ‡å¿—
                if state.get("skip_unified_review"):
                    updated_state["skip_unified_review"] = True
                    logger.info("ğŸ” [DEBUG] ä¿ç•™ skip_unified_review=True")

                # è®°å½•è·³è¿‡äº¤äº’
                skip_entry = {
                    "type": "calibration_questionnaire",
                    "intent": "skip",
                    "timestamp": datetime.now().isoformat(),
                    "question_count": len(questionnaire.get("questions", []))
                }
                history = state.get("interaction_history", [])
                updated_state["interaction_history"] = history + [skip_entry]

                # è‡ªåŠ¨ä¿ç•™æŒä¹…åŒ–æ ‡å¿—
                updated_state = WorkflowFlagManager.preserve_flags(state, updated_state)

                logger.info(f"ğŸ” [DEBUG] Command.update åŒ…å«çš„é”®: {list(updated_state.keys())}")
                return Command(update=updated_state, goto="requirements_confirmation")

            if intent not in {"modify", "add", "revise"} and not answers_map:
                skip_attempts += 1
                warning_message = "è¯·è‡³å°‘å›ç­”ä¸€ä¸ªé—®é¢˜ä»¥ç»§ç»­ã€‚"
                logger.warning("âš ï¸ No valid answers captured, prompting questionnaire again")
                continue

            if intent == "add" and not (answers_map or notes):
                skip_attempts += 1
                warning_message = "è¯·è¡¥å……è‡³å°‘ä¸€ä¸ªç­”æ¡ˆæˆ–è¯´æ˜ï¼Œæ–¹ä¾¿æˆ‘ä»¬ç»§ç»­ã€‚"
                logger.warning("âš ï¸ Additional info intent received without content, prompting again")
                continue

            warning_message = None
            break

        timestamp = datetime.now().isoformat()

        updated_state: Dict[str, Any] = {
            "calibration_questionnaire": questionnaire,
            "calibration_skip_attempts": skip_attempts,
            "calibration_warning": warning_message,
            "agent_results": agent_results
        }

        # è‡ªåŠ¨ä¿ç•™æŒä¹…åŒ–æ ‡å¿—
        updated_state = WorkflowFlagManager.preserve_flags(state, updated_state)

        interaction_entry = {
            "type": "calibration_questionnaire",
            "intent": intent,
            "timestamp": timestamp,
            "question_count": len(questionnaire.get("questions", []))
        }
        if answers_map:
            interaction_entry["answers"] = answers_map
        if notes:
            interaction_entry["notes"] = notes

        history = state.get("interaction_history", [])
        updated_state["interaction_history"] = history + [interaction_entry]

        next_node = "requirements_confirmation"
        should_reanalyze = False

        if intent == "modify":
            logger.info(f"âš ï¸ User requested modifications: {content[:100]}")
            modification_text = modifications or content
            updated_state["calibration_modifications"] = modification_text
            updated_state["user_feedback"] = notes or modification_text
            original_input = state.get("user_input", "")
            updated_state["user_input"] = f"{original_input}\n\nã€ç”¨æˆ·ä¿®æ”¹æ„è§ã€‘\n{modification_text}"
            next_node = "requirements_analyst"
            should_reanalyze = True

        elif intent == "add":
            logger.info(f"ğŸ“ User provided additional information: {content[:100]}")
            supplement_text = notes or content
            
            # âœ… ä¿®å¤: æ— è®ºæ˜¯å¦æœ‰è¡¥å……æ–‡æœ¬ï¼Œéƒ½è¦ä¿å­˜é—®å·ç­”æ¡ˆï¼ˆç”¨äºåç»­èšåˆï¼‰
            if answers_map:
                updated_state["calibration_answers"] = answers_map
                
                # ğŸ”§ æ–°å¢: æ„å»ºå¹¶ä¿å­˜ questionnaire_summary/responsesï¼ˆä¸ approve åˆ†æ”¯ä¸€è‡´ï¼‰
                summary_entries = entries
                summary_payload = {
                    "entries": summary_entries,
                    "answers": answers_map,
                    "submitted_at": timestamp,
                    "timestamp": timestamp,
                    "notes": notes,
                    "source": "calibration_questionnaire"
                }
                updated_state["questionnaire_summary"] = summary_payload
                updated_state["questionnaire_responses"] = summary_payload
                # ğŸ”¥ v7.13: ä¿®å¤ - add æ„å›¾ä¹Ÿéœ€è¦æ ‡è®°é—®å·å·²å¤„ç†ï¼Œé˜²æ­¢ resume åé‡å¤ç”Ÿæˆ
                updated_state["calibration_processed"] = True
                logger.info(f"âœ… [add æ„å›¾] å·²ä¿å­˜ {len(answers_map)} ä¸ªé—®å·ç­”æ¡ˆåˆ° questionnaire_summary")
            
            if supplement_text:
                updated_state["additional_requirements"] = supplement_text

            if supplement_text and len(str(supplement_text).strip()) > 10:
                logger.info("ğŸ”„ Significant additional info detected, returning to requirements analyst")
                original_input = state.get("user_input", "")
                updated_state["user_input"] = f"{original_input}\n\nã€ç”¨æˆ·è¡¥å……ä¿¡æ¯ã€‘\n{supplement_text}"
                next_node = "requirements_analyst"
                should_reanalyze = True
            else:
                logger.info("âœ… Minor additions, proceeding to confirmation")

        elif intent == "revise":
            logger.info("ğŸ”„ User requested re-analysis")
            next_node = "requirements_analyst"
            should_reanalyze = True

        else:
            logger.info("âœ… User approved questionnaire")
            if answers_map:
                logger.info(f"ğŸ“ Integrating {len(answers_map)} questionnaire answers into requirements")

                summary_entries = entries
                summary_payload = {
                    "entries": summary_entries,
                    "answers": answers_map,
                    "submitted_at": timestamp,
                    "timestamp": timestamp,
                    "notes": notes,
                    "source": "calibration_questionnaire"
                }

                updated_state["calibration_answers"] = answers_map
                updated_state["questionnaire_summary"] = summary_payload
                updated_state["questionnaire_responses"] = summary_payload
                updated_state["calibration_processed"] = True

                # æ„å»ºç®€è¦æ–‡æœ¬å¹¶åˆå¹¶åˆ°ç”¨æˆ·éœ€æ±‚æè¿°ä¸­
                summary_lines = []
                for entry in summary_entries:
                    value = entry.get("value")
                    if isinstance(value, (list, tuple, set)):
                        value_text = "ã€".join(str(v) for v in value if str(v).strip())
                    else:
                        value_text = str(value)
                    summary_lines.append(f"- {entry.get('question', 'åå¥½')}: {value_text}")

                if notes:
                    summary_lines.append(f"- è¡¥å……è¯´æ˜: {notes}")

                if summary_lines:
                    original_input = state.get("user_input", "")
                    insight_block = "\n".join(summary_lines)
                    updated_state["user_input"] = f"{original_input}\n\nã€é—®å·æ´å¯Ÿã€‘\n{insight_block}"

                # æ›´æ–°ç»“æ„åŒ–éœ€æ±‚ï¼ŒåµŒå…¥é—®å·æ´å¯Ÿ
                existing_requirements = state.get("structured_requirements") or {}
                questionnaire_insights = {
                    "entries": summary_entries,
                    "insights": {entry["id"]: entry["value"] for entry in summary_entries},
                    "submitted_at": timestamp,
                    "notes": notes,
                    "source": "calibration_questionnaire"
                }
                updated_state["structured_requirements"] = {
                    **existing_requirements,
                    "questionnaire_insights": questionnaire_insights
                }

                next_node = "requirements_analyst"
                should_reanalyze = True
            else:
                logger.warning("âš ï¸ No valid answers captured, re-prompting questionnaire")
                updated_state["calibration_skip_attempts"] = skip_attempts + 1
                updated_state["calibration_warning"] = "è¯·è‡³å°‘å›ç­”ä¸€ä¸ªé—®é¢˜ä»¥ç»§ç»­ã€‚"
                next_node = "calibration_questionnaire"

        if should_reanalyze:
            logger.info(f"ğŸ”„ Routing back to {next_node} for re-analysis")
        else:
            logger.info(f"âœ… Proceeding to {next_node}")

        logger.info(f"ğŸ” [DEBUG] Command.update åŒ…å«çš„é”®: {list(updated_state.keys())}")
        logger.info(f"ğŸ” [DEBUG] calibration_processed å€¼: {updated_state.get('calibration_processed')}")
        return Command(update=updated_state, goto=next_node)
