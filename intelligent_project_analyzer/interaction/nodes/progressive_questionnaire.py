"""
ä¸‰æ­¥é€’è¿›å¼é—®å·èŠ‚ç‚¹

v7.80: å°†é—®å·å‡çº§ä¸ºã€Œæ ¸å¿ƒä»»åŠ¡ç¡®è®¤ â†’ é›·è¾¾å›¾å¤šç»´åº¦ç”»åƒ â†’ å¯†åº¦è¡¥é½è¿½é—®ã€çš„ä¸‰æ­¥é€’è¿›å¼ä½“éªŒ
v7.80.1: Step 1 å‡çº§ä¸º LLM é©±åŠ¨çš„æ™ºèƒ½ä»»åŠ¡æ‹†è§£ï¼ˆä»å¤è¿° â†’ æ˜ç¡®ï¼‰

èŠ‚ç‚¹æµç¨‹ï¼š
    requirements_analyst
        â†’ progressive_step1_core_task
        â†’ progressive_step2_radar
        â†’ progressive_step3_gap_filling
        â†’ requirements_confirmation
"""

import asyncio
import json
from datetime import datetime
from typing import Any, Dict, List, Literal, Optional

from langgraph.store.base import BaseStore
from langgraph.types import Command, interrupt
from loguru import logger

from ...core.state import ProjectAnalysisState
from ...core.workflow_flags import WorkflowFlagManager
from ...services.core_task_decomposer import _simple_fallback_decompose, decompose_core_tasks
from ...services.dimension_selector import DimensionSelector, RadarGapAnalyzer, select_dimensions_for_state


class ProgressiveQuestionnaireNode:
    """ä¸‰æ­¥é€’è¿›å¼é—®å·èŠ‚ç‚¹"""

    # ==========================================================================
    # Step 1: æ ¸å¿ƒä»»åŠ¡ç¡®è®¤ï¼ˆv7.80.1 æ™ºèƒ½æ‹†è§£ç‰ˆï¼‰
    # ==========================================================================

    @staticmethod
    def step1_core_task(
        state: ProjectAnalysisState, store: Optional[BaseStore] = None
    ) -> Command[Literal["progressive_step2_radar", "requirements_analyst"]]:
        """
        Step 1: æ ¸å¿ƒä»»åŠ¡æ™ºèƒ½æ‹†è§£ä¸ç¡®è®¤

        v7.80.1: ä½¿ç”¨ LLM å°†ç”¨æˆ·è¾“å…¥æ‹†è§£ä¸ºç»“æ„åŒ–ä»»åŠ¡åˆ—è¡¨ï¼Œè®©ç”¨æˆ·ç¡®è®¤/è°ƒæ•´/è¡¥å……ã€‚

        Args:
            state: é¡¹ç›®åˆ†æçŠ¶æ€
            store: å­˜å‚¨æ¥å£

        Returns:
            Commandå¯¹è±¡ï¼ŒæŒ‡å‘ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
        """
        logger.info("=" * 80)
        logger.info("ğŸ¯ [v7.80.1 Step 1] æ ¸å¿ƒä»»åŠ¡æ™ºèƒ½æ‹†è§£")
        logger.info("=" * 80)

        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆæ­¤æ­¥éª¤ï¼ˆä½¿ç”¨æ–°å­—æ®µï¼‰
        if state.get("progressive_questionnaire_step", 0) >= 1 and state.get("confirmed_core_tasks"):
            logger.info("âœ… Step 1 å·²å®Œæˆï¼Œè·³è¿‡")
            return Command(update={"progressive_questionnaire_step": 1}, goto="progressive_step2_radar")

        # è¿½é—®æ¨¡å¼è·³è¿‡
        if state.get("is_followup"):
            logger.info("â© Follow-up session, skipping progressive questionnaire")
            logger.info("   - ä¸‹ä¸€æ­¥: requirements_confirmation")
            update_dict = {"progressive_questionnaire_completed": True, "progressive_questionnaire_step": 3}
            update_dict = WorkflowFlagManager.preserve_flags(state, update_dict)
            return Command(update=update_dict, goto="requirements_confirmation")

        # ğŸ†• v7.80.1: ä½¿ç”¨ LLM æ‹†è§£æ ¸å¿ƒä»»åŠ¡
        user_input = state.get("user_input", "")
        structured_data = state.get("agent_results", {}).get("requirements_analyst", {}).get("structured_data", {})

        # ğŸ†• v7.80.15 (P1.1): è¯—æ„è§£è¯»å­æµç¨‹
        poetic_metadata = None
        if _contains_poetic_expression(user_input):
            logger.info("ğŸ¨ [è¯—æ„è§£è¯»] æ£€æµ‹åˆ°è¯—æ„/å“²å­¦è¡¨è¾¾ï¼Œå¯åŠ¨è¯—æ„è§£è¯»å­æµç¨‹")
            try:
                import functools
                from concurrent.futures import ThreadPoolExecutor

                def _run_async_poetic_interpret(user_input: str):
                    """åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œè¯—æ„è§£è¯»"""
                    return asyncio.run(_llm_interpret_poetry(user_input))

                with ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(_run_async_poetic_interpret, user_input)
                    poetic_metadata = future.result(timeout=30)  # 30ç§’è¶…æ—¶

                logger.info(f"âœ… [è¯—æ„è§£è¯»] è§£è¯»å®Œæˆ: {poetic_metadata.get('metaphor_explanation', '')[:50]}...")
            except Exception as e:
                logger.warning(f"âš ï¸ [è¯—æ„è§£è¯»] è§£è¯»å¤±è´¥: {e}ï¼Œç»§ç»­æ­£å¸¸æµç¨‹")
                poetic_metadata = None

        # æ‰§è¡Œä»»åŠ¡æ‹†è§£
        # v7.80.1.2: ä½¿ç”¨ ThreadPoolExecutor åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œ LLM å¼‚æ­¥è°ƒç”¨
        # è§£å†³ LangGraph å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸ asyncio.run ä¸å…¼å®¹çš„é—®é¢˜
        try:
            import functools
            from concurrent.futures import ThreadPoolExecutor

            def _run_async_decompose(user_input: str, structured_data: dict):
                """åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥ä»»åŠ¡æ‹†è§£"""
                return asyncio.run(decompose_core_tasks(user_input, structured_data))

            logger.info("ğŸ”„ [v7.80.1.2] ä½¿ç”¨ ThreadPoolExecutor æ‰§è¡Œ LLM ä»»åŠ¡æ‹†è§£")
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(_run_async_decompose, user_input, structured_data)
                extracted_tasks = future.result(timeout=60)  # 60ç§’è¶…æ—¶

            if not extracted_tasks:
                logger.warning("âš ï¸ LLM ä»»åŠ¡æ‹†è§£è¿”å›ç©ºåˆ—è¡¨ï¼Œä½¿ç”¨å›é€€ç­–ç•¥")
                extracted_tasks = _simple_fallback_decompose(user_input)

        except Exception as e:
            logger.error(f"âŒ LLM ä»»åŠ¡æ‹†è§£å¤±è´¥: {e}")
            logger.info("âš ï¸ ä½¿ç”¨å›é€€ç­–ç•¥è¿›è¡Œå…³é”®è¯åŒ¹é…æ‹†è§£")
            extracted_tasks = _simple_fallback_decompose(user_input)

        logger.info(f"ğŸ“ [v7.80.1] æ‹†è§£å‡º {len(extracted_tasks)} ä¸ªæ ¸å¿ƒä»»åŠ¡")
        for i, task in enumerate(extracted_tasks):
            logger.info(f"   {i+1}. {task.get('title', 'æœªå‘½å')}")

        # åŒæ—¶ä¿ç•™æ—§æ ¼å¼ï¼ˆå…¼å®¹æ€§ï¼‰
        old_format_task = ProgressiveQuestionnaireNode._extract_core_task(state)

        # ç”Ÿæˆç”¨æˆ·è¾“å…¥æ‘˜è¦
        user_input_summary = user_input[:100] + ("..." if len(user_input) > 100 else "")

        # ğŸ†• v7.80.1: æ„å»ºæ–°çš„ interrupt payload
        payload = {
            "interaction_type": "progressive_questionnaire_step1",
            "step": 1,
            "total_steps": 3,
            "title": "æ˜ç¡®æ ¸å¿ƒä»»åŠ¡",
            "message": "ç³»ç»Ÿå·²ä»æ‚¨çš„æè¿°ä¸­è¯†åˆ«ä»¥ä¸‹æ ¸å¿ƒä»»åŠ¡ï¼Œè¯·ç¡®è®¤ã€è°ƒæ•´æˆ–è¡¥å……",
            # ğŸ†• æ–°å­—æ®µï¼šä»»åŠ¡åˆ—è¡¨
            "extracted_tasks": extracted_tasks,
            "user_input_summary": user_input_summary,
            # æ—§å­—æ®µï¼šä¿ç•™å…¼å®¹
            "extracted_task": old_format_task,
            "editable": True,
            "options": {"confirm": "ç¡®è®¤ä»»åŠ¡åˆ—è¡¨", "skip": "è·³è¿‡é—®å·"},
        }

        logger.info("ğŸ›‘ [Step 1] å³å°†è°ƒç”¨ interrupt()ï¼Œç­‰å¾…ç”¨æˆ·è¾“å…¥...")
        user_response = interrupt(payload)
        logger.info(f"âœ… [Step 1] æ”¶åˆ°ç”¨æˆ·å“åº”: {type(user_response)}")

        # ğŸ†• v7.80.1: è§£æç”¨æˆ·å“åº”ï¼ˆæ”¯æŒæ–°æ—§æ ¼å¼ï¼‰
        confirmed_tasks = extracted_tasks
        confirmed_task = old_format_task
        skip_requested = False

        if isinstance(user_response, dict):
            if user_response.get("action") == "skip" or user_response.get("intent") == "skip":
                skip_requested = True
            else:
                # ä¼˜å…ˆä½¿ç”¨æ–°æ ¼å¼
                if "confirmed_tasks" in user_response:
                    confirmed_tasks = user_response["confirmed_tasks"]
                elif "tasks" in user_response:
                    confirmed_tasks = user_response["tasks"]

                # å…¼å®¹æ—§æ ¼å¼
                if "confirmed_task" in user_response:
                    confirmed_task = user_response["confirmed_task"]
                elif "task" in user_response:
                    confirmed_task = user_response["task"]

        elif isinstance(user_response, str):
            if user_response.strip().lower() in {"skip", "è·³è¿‡", "å–æ¶ˆ"}:
                skip_requested = True
            elif user_response.strip():
                confirmed_task = user_response.strip()

        if skip_requested:
            logger.info("â­ï¸ ç”¨æˆ·è·³è¿‡é—®å·")

            # ğŸ”§ v7.87 P0: è®¾ç½®é»˜è®¤ questionnaire_summary
            timestamp_now = datetime.now().isoformat()
            default_questionnaire_summary = {
                "skipped": True,
                "reason": "user_skip_step1",
                "entries": [],
                "answers": {},
                "timestamp": timestamp_now,
                "submitted_at": timestamp_now,
                "source": "progressive_step1_skip",
            }

            update_dict = {
                "progressive_questionnaire_completed": True,
                "progressive_questionnaire_step": 3,
                # æ–°å­—æ®µ
                "extracted_core_tasks": extracted_tasks,
                "confirmed_core_tasks": extracted_tasks,
                "core_task_summary": ProgressiveQuestionnaireNode._build_task_summary(extracted_tasks),
                # æ—§å­—æ®µï¼ˆå…¼å®¹ï¼‰
                "extracted_core_task": old_format_task,
                "confirmed_core_task": old_format_task,
                # ğŸ”§ v7.87 P0: é»˜è®¤ questionnaire_summary
                "questionnaire_summary": default_questionnaire_summary,
                "questionnaire_responses": default_questionnaire_summary,
            }
            logger.info(f"â­ï¸ [v7.87 P0] å·²è®¾ç½®é»˜è®¤ questionnaire_summaryï¼ˆStep 1è·³è¿‡ï¼‰")
            logger.info("   - ä¸‹ä¸€æ­¥: requirements_confirmation")

            update_dict = WorkflowFlagManager.preserve_flags(state, update_dict)
            return Command(update=update_dict, goto="requirements_confirmation")

        # æ„å»ºä»»åŠ¡æ‘˜è¦
        task_summary = ProgressiveQuestionnaireNode._build_task_summary(confirmed_tasks)
        logger.info(f"âœ… [Step 1] ç¡®è®¤ {len(confirmed_tasks)} ä¸ªæ ¸å¿ƒä»»åŠ¡")

        # ğŸ†• v7.80.15 (P1.2): æ£€æµ‹ç‰¹æ®Šåœºæ™¯
        from ...services.task_completeness_analyzer import TaskCompletenessAnalyzer

        analyzer = TaskCompletenessAnalyzer()
        special_scenarios = analyzer.detect_special_scenarios(user_input, task_summary)

        special_scene_metadata = None
        if special_scenarios:
            # æ„å»ºåœºæ™¯å…ƒæ•°æ®
            scene_tags = list(special_scenarios.keys())
            matched_keywords = {}
            for scene_id, scene_info in special_scenarios.items():
                matched_keywords[scene_id] = scene_info.get("matched_keywords", [])

            special_scene_metadata = {
                "scene_tags": scene_tags,
                "matched_keywords": matched_keywords,
                "trigger_messages": {
                    scene_id: info.get("trigger_message", "") for scene_id, info in special_scenarios.items()
                },
            }
            logger.info(f"ğŸ¯ [Step 1] è¯†åˆ«ç‰¹æ®Šåœºæ™¯: {scene_tags}")

        update_dict = {
            # ğŸ†• v7.80.1 æ–°å­—æ®µ
            "extracted_core_tasks": extracted_tasks,
            "confirmed_core_tasks": confirmed_tasks,
            "core_task_summary": task_summary,
            # æ—§å­—æ®µï¼ˆå…¼å®¹ï¼‰
            "extracted_core_task": old_format_task,
            "confirmed_core_task": confirmed_task if confirmed_task != old_format_task else task_summary,
            "progressive_questionnaire_step": 1,
            # ğŸ†• v7.80.15 (P1.1): è¯—æ„è§£è¯»å…ƒæ•°æ®
            "poetic_metadata": poetic_metadata,
            # ğŸ†• v7.80.15 (P1.2): ç‰¹æ®Šåœºæ™¯å…ƒæ•°æ®
            "special_scene_metadata": special_scene_metadata,
        }
        update_dict = WorkflowFlagManager.preserve_flags(state, update_dict)

        return Command(update=update_dict, goto="progressive_step2_radar")

    # ==========================================================================
    # Step 2: é›·è¾¾å›¾ç»´åº¦
    # ==========================================================================

    @staticmethod
    def step2_radar(
        state: ProjectAnalysisState, store: Optional[BaseStore] = None
    ) -> Command[Literal["progressive_step3_gap_filling", "requirements_confirmation"]]:
        """
        Step 2: é›·è¾¾å›¾å¤šç»´åº¦åå¥½æ”¶é›†

        å±•ç¤º9-12ä¸ªç»´åº¦æ»‘å—ï¼Œè®©ç”¨æˆ·è¡¨è¾¾è®¾è®¡åå¥½ã€‚

        Args:
            state: é¡¹ç›®åˆ†æçŠ¶æ€
            store: å­˜å‚¨æ¥å£

        Returns:
            Commandå¯¹è±¡ï¼ŒæŒ‡å‘ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
        """
        logger.info("=" * 80)
        logger.info("ğŸ“Š [v7.80 Step 2] é›·è¾¾å›¾ç»´åº¦æ”¶é›†")
        logger.info("=" * 80)

        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆæ­¤æ­¥éª¤
        if state.get("progressive_questionnaire_step", 0) >= 2 and state.get("radar_dimension_values"):
            logger.info("âœ… Step 2 å·²å®Œæˆï¼Œè·³è¿‡")
            return Command(update={"progressive_questionnaire_step": 2}, goto="progressive_step3_gap_filling")

        # ğŸ†• v7.80.4: åŠ¨æ€ç»´åº¦é€‰æ‹© + æ™ºèƒ½ç”Ÿæˆ
        # ğŸ”§ v7.80.16: æ€§èƒ½ä¼˜åŒ– - é»˜è®¤ç¦ç”¨LLMåŠ¨æ€ç”Ÿæˆï¼Œä¾èµ–P0.3åœºæ™¯æ³¨å…¥
        import os

        from ...services.dynamic_dimension_generator import DynamicDimensionGenerator

        # ğŸ†• v7.80.5: å¼ºåˆ¶ç”Ÿæˆæ¨¡å¼ï¼ˆç”¨äºæµ‹è¯•/æ¼”ç¤ºï¼‰
        FORCE_GENERATE = os.getenv("FORCE_GENERATE_DIMENSIONS", "false").lower() == "true"

        # ğŸ”§ v7.80.16: åŠ¨æ€ç”Ÿæˆå¼€å…³ï¼ˆé»˜è®¤å…³é—­ï¼Œé¿å…LLMè°ƒç”¨å»¶è¿Ÿï¼‰
        USE_DYNAMIC_GENERATION = os.getenv("USE_DYNAMIC_GENERATION", "false").lower() == "true"

        # Step 2.1: å…ˆé€‰æ‹©ç°æœ‰ç»´åº¦
        existing_dimensions = select_dimensions_for_state(state)
        logger.info(f"ğŸ“Š å·²é€‰æ‹© {len(existing_dimensions)} ä¸ªç°æœ‰ç»´åº¦")

        # Step 2.2: åˆ†æè¦†ç›–åº¦ï¼Œå¿…è¦æ—¶ç”Ÿæˆæ–°ç»´åº¦
        user_input = state.get("user_input", "")
        agent_results = state.get("agent_results", {})
        requirements_result = agent_results.get("requirements_analyst", {})
        structured_data = requirements_result.get("structured_data", {})

        generator = DynamicDimensionGenerator()
        dimensions = existing_dimensions
        generated_count = 0

        if FORCE_GENERATE:
            # ğŸš€ å¼ºåˆ¶ç”Ÿæˆæ¨¡å¼ï¼šè·³è¿‡è¦†ç›–åº¦æ£€æŸ¥ï¼Œç›´æ¥ç”Ÿæˆ
            logger.info("ğŸš€ [åŠ¨æ€ç»´åº¦] å¼ºåˆ¶ç”Ÿæˆæ¨¡å¼å·²å¯ç”¨")
            missing_aspects = ["ç”¨æˆ·ç‹¬ç‰¹éœ€æ±‚", "é¡¹ç›®ç‰¹è‰²è¦æ±‚"]

            new_dimensions = generator.generate_dimensions(
                user_input,
                structured_data,
                missing_aspects,
                target_count=min(2, 12 - len(existing_dimensions)),  # å¼ºåˆ¶æ¨¡å¼ç”Ÿæˆ2ä¸ª
            )

            if new_dimensions:
                dimensions = existing_dimensions + new_dimensions
                generated_count = len(new_dimensions)
                logger.info(f"âœ… [åŠ¨æ€ç»´åº¦] å¼ºåˆ¶æ–°å¢ {generated_count} ä¸ªå®šåˆ¶ç»´åº¦")
                for dim in new_dimensions:
                    logger.info(f"   + {dim['name']}: {dim['left_label']} â† â†’ {dim['right_label']}")
        elif USE_DYNAMIC_GENERATION:
            # æ­£å¸¸æ¨¡å¼ï¼šåŸºäºè¦†ç›–åº¦åˆ†æï¼ˆä»…åœ¨ç¯å¢ƒå˜é‡å¯ç”¨æ—¶ï¼‰
            logger.info("ğŸ” [åŠ¨æ€ç»´åº¦] LLMè¦†ç›–åº¦åˆ†æå·²å¯ç”¨")
            coverage = generator.analyze_coverage(user_input, structured_data, existing_dimensions)

            if coverage.get("should_generate", False) and coverage.get("missing_aspects"):
                logger.info(f"ğŸ¤– [åŠ¨æ€ç»´åº¦] æ£€æµ‹åˆ°è¦†ç›–ä¸è¶³ (è¯„åˆ†: {coverage.get('coverage_score', 0):.2f})")
                logger.info(f"   ç¼ºå¤±æ–¹é¢: {coverage.get('missing_aspects', [])}")

                # ç”Ÿæˆæ–°ç»´åº¦
                new_dimensions = generator.generate_dimensions(
                    user_input,
                    structured_data,
                    coverage.get("missing_aspects", []),
                    target_count=min(3, 12 - len(existing_dimensions)),  # ç¡®ä¿æ€»æ•°ä¸è¶…è¿‡12
                )

                if new_dimensions:
                    dimensions = existing_dimensions + new_dimensions
                    generated_count = len(new_dimensions)
                    logger.info(f"âœ… [åŠ¨æ€ç»´åº¦] æ–°å¢ {generated_count} ä¸ªå®šåˆ¶ç»´åº¦")
                    for dim in new_dimensions:
                        logger.info(f"   + {dim['name']}: {dim['left_label']} â† â†’ {dim['right_label']}")
        else:
            # ğŸ”§ v7.80.16: é»˜è®¤ç¦ç”¨LLMè°ƒç”¨ï¼Œä»…ä½¿ç”¨P0.3åœºæ™¯æ³¨å…¥
            logger.info("âš¡ [æ€§èƒ½ä¼˜åŒ–] è·³è¿‡LLMåŠ¨æ€ç”Ÿæˆï¼Œä½¿ç”¨P0.3åœºæ™¯æ³¨å…¥æœºåˆ¶")

        # ğŸ†• v7.80.15 (P1.3): åŸºäºç‰¹æ®Šåœºæ™¯æ³¨å…¥ä¸“ç”¨ç»´åº¦
        special_scene_metadata = state.get("special_scene_metadata")
        confirmed_tasks = state.get("confirmed_core_tasks", [])

        if special_scene_metadata or confirmed_tasks:
            from ...services.dimension_selector import DimensionSelector

            selector = DimensionSelector()

            # è°ƒç”¨åœºæ™¯æ£€æµ‹å’Œç»´åº¦æ³¨å…¥
            dimensions = selector.detect_and_inject_specialized_dimensions(
                user_input=user_input,
                confirmed_tasks=confirmed_tasks,
                current_dimensions=dimensions,
                special_scene_metadata=special_scene_metadata,
            )
            logger.info(f"ğŸ¯ [ç‰¹æ®Šåœºæ™¯] ç»´åº¦æ³¨å…¥å®Œæˆ: æœ€ç»ˆ {len(dimensions)} ä¸ªç»´åº¦")

        logger.info(f"ğŸ“Š æœ€ç»ˆç»´åº¦æ•°é‡: {len(dimensions)} ({len(existing_dimensions)} ç°æœ‰ + {generated_count} åŠ¨æ€ç”Ÿæˆ)")

        # è·å–ç¡®è®¤çš„æ ¸å¿ƒä»»åŠ¡
        confirmed_task = state.get("confirmed_core_task", "")

        # æ„å»ºinterrupt payload
        payload = {
            "interaction_type": "progressive_questionnaire_step2",
            "step": 2,
            "total_steps": 3,
            "title": "å¤šç»´åº¦åå¥½è®¾ç½®",
            "message": "è¯·é€šè¿‡æ‹–åŠ¨æ»‘å—è¡¨è¾¾æ‚¨çš„è®¾è®¡åå¥½ã€‚æ¯ä¸ªç»´åº¦ä»£è¡¨ä¸¤ç§ä¸åŒçš„è®¾è®¡æ–¹å‘ã€‚",
            "core_task": confirmed_task,
            "dimensions": dimensions,
            "instructions": "æ‹–åŠ¨æ»‘å—åˆ°æ‚¨åå¥½çš„ä½ç½®ï¼ˆ0-100ï¼‰",
            "options": {"confirm": "ç¡®è®¤åå¥½è®¾ç½®", "back": "è¿”å›ä¿®æ”¹æ ¸å¿ƒä»»åŠ¡"},
        }

        logger.info("ğŸ›‘ [Step 2] å³å°†è°ƒç”¨ interrupt()ï¼Œç­‰å¾…ç”¨æˆ·è¾“å…¥...")
        user_response = interrupt(payload)
        logger.info(f"âœ… [Step 2] æ”¶åˆ°ç”¨æˆ·å“åº”: {type(user_response)}")

        # è§£æç”¨æˆ·å“åº”
        dimension_values = {}
        # ğŸ”§ v7.87: ç§»é™¤è¿”å›ä¸Šä¸€æ­¥åŠŸèƒ½

        if isinstance(user_response, dict):
            # ğŸ”§ v7.87: ç§»é™¤ action == "back" å¤„ç†
            dimension_values = user_response.get("values") or user_response.get("dimension_values") or {}
            # å¦‚æœæ²¡æœ‰valueså­—æ®µï¼Œå°è¯•ç›´æ¥ä»å“åº”ä¸­æå–
            if not dimension_values:
                for key, value in user_response.items():
                    if key in [d["id"] for d in dimensions] and isinstance(value, (int, float)):
                        dimension_values[key] = int(value)

        # å¦‚æœç”¨æˆ·æ²¡æœ‰è®¾ç½®ä»»ä½•å€¼ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if not dimension_values:
            logger.warning("âš ï¸ ç”¨æˆ·æœªè®¾ç½®ä»»ä½•ç»´åº¦å€¼ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            dimension_values = {d["id"]: d.get("default_value", 50) for d in dimensions}

        logger.info(f"ğŸ“Š [Step 2] æ”¶é›†åˆ° {len(dimension_values)} ä¸ªç»´åº¦å€¼")

        # åˆ†æé›·è¾¾å›¾
        analyzer = RadarGapAnalyzer()
        analysis = analyzer.analyze(dimension_values, dimensions)

        update_dict = {
            "selected_radar_dimensions": dimensions,
            "radar_dimension_values": dimension_values,
            "radar_analysis_summary": analysis,
            "progressive_questionnaire_step": 2,
        }
        update_dict = WorkflowFlagManager.preserve_flags(state, update_dict)

        # ğŸ”§ v7.80.18: æ€»æ˜¯æ‰§è¡Œ Step 3ï¼ˆä»»åŠ¡ä¿¡æ¯å®Œæ•´æ€§åˆ†æï¼‰
        # v7.80.6 é©æ–°ï¼šStep 3 ä¸å†åŸºäºé›·è¾¾å›¾çŸ­æ¿ï¼Œè€Œæ˜¯åŸºäºä»»åŠ¡å®Œæ•´æ€§
        logger.info("ğŸ“Š [Step 2] é›·è¾¾å›¾ç»´åº¦æ”¶é›†å®Œæˆï¼Œå‡†å¤‡è¿›å…¥ Step 3ï¼ˆä»»åŠ¡å®Œæ•´æ€§åˆ†æï¼‰")

        # ä¿å­˜é›·è¾¾å›¾åˆ†æç»“æœï¼ˆä¾› Step 3 å‚è€ƒï¼Œä½†ä¸ä½œä¸ºæ˜¯å¦æ‰§è¡Œçš„ä¾æ®ï¼‰
        gap_dimensions = analysis.get("gap_dimensions", [])
        if gap_dimensions:
            logger.info(f"   é›·è¾¾å›¾çŸ­æ¿ç»´åº¦: {gap_dimensions}")

        # âœ… æ€»æ˜¯è¿›å…¥ Step 3ï¼Œç”± Step 3 å†…éƒ¨å†³å®šæ˜¯å¦éœ€è¦è¡¥å……é—®é¢˜
        return Command(update=update_dict, goto="progressive_step3_gap_filling")

    # ==========================================================================
    # Step 3: å¯†åº¦è¡¥é½è¿½é—®
    # ==========================================================================

    @staticmethod
    def step3_gap_filling(
        state: ProjectAnalysisState, store: Optional[BaseStore] = None
    ) -> Command[Literal["requirements_confirmation"]]:
        """
        Step 3: æ ¸å¿ƒä»»åŠ¡ä¿¡æ¯å®Œæ•´æ€§æŸ¥æ¼è¡¥ç¼º

        v7.80.6: ä»"é›·è¾¾å›¾è¡¥å……"è½¬å˜ä¸º"ä»»åŠ¡ä¿¡æ¯å®Œæ•´æ€§æ£€æŸ¥"
        - åˆ†ææ ¸å¿ƒä»»åŠ¡æ˜¯å¦åŒ…å«è¶³å¤Ÿä¿¡æ¯
        - è¯†åˆ«ç¼ºå¤±çš„å…³é”®ç»´åº¦ï¼ˆ6å¤§ç»´åº¦ï¼‰
        - ç”Ÿæˆé’ˆå¯¹æ€§ã€å¯¼å‘æ€§ã€æ•æ„Ÿæ€§çš„è¡¥å……é—®é¢˜

        Args:
            state: é¡¹ç›®åˆ†æçŠ¶æ€
            store: å­˜å‚¨æ¥å£

        Returns:
            Commandå¯¹è±¡ï¼ŒæŒ‡å‘ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
        """
        logger.info("=" * 80)
        logger.info("â“ [v7.80.6 Step 3] æ ¸å¿ƒä»»åŠ¡ä¿¡æ¯å®Œæ•´æ€§æŸ¥æ¼è¡¥ç¼º")
        logger.info("=" * 80)

        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆæ­¤æ­¥éª¤
        if state.get("progressive_questionnaire_completed"):
            logger.info("âœ… é—®å·å·²å®Œæˆï¼Œè·³è¿‡ Step 3")
            logger.info("   - ä¸‹ä¸€æ­¥: requirements_confirmation")
            return Command(update={"progressive_questionnaire_step": 3}, goto="requirements_confirmation")

        # ğŸ†• v7.80.6: é’ˆå¯¹æ ¸å¿ƒä»»åŠ¡è¿›è¡Œä¿¡æ¯å®Œæ•´æ€§åˆ†æ
        from ...services.task_completeness_analyzer import TaskCompletenessAnalyzer

        # è·å–æ ¸å¿ƒä»»åŠ¡å’Œç›¸å…³æ•°æ®
        confirmed_tasks = state.get("confirmed_core_tasks", [])
        user_input = state.get("user_input", "")
        agent_results = state.get("agent_results", {})
        requirements_result = agent_results.get("requirements_analyst", {})
        structured_data = requirements_result.get("structured_data", {})

        # æ‰§è¡Œä»»åŠ¡å®Œæ•´æ€§åˆ†æ
        analyzer = TaskCompletenessAnalyzer()
        completeness = analyzer.analyze(confirmed_tasks, user_input, structured_data)

        logger.info(f"ğŸ“Š ä»»åŠ¡ä¿¡æ¯å®Œæ•´æ€§è¯„åˆ†: {completeness.get('completeness_score', 0):.2f}")
        logger.info(f"   å·²è¦†ç›–ç»´åº¦: {completeness.get('covered_dimensions', [])}")
        logger.info(f"   ç¼ºå¤±ç»´åº¦: {completeness.get('missing_dimensions', [])}")
        logger.info(f"   å…³é”®ç¼ºå¤±ç‚¹: {completeness.get('critical_gaps', [])}")

        # åˆ¤æ–­æ˜¯å¦éœ€è¦è¡¥å……é—®é¢˜
        critical_gaps = completeness.get("critical_gaps", [])
        if not critical_gaps:
            logger.info("âœ… ä»»åŠ¡ä¿¡æ¯å®Œæ•´ï¼Œæ— éœ€è¡¥å……ï¼Œè·³è¿‡ Step 3")
            logger.info("   - ä¸‹ä¸€æ­¥: requirements_confirmation")
            update_dict = {
                "progressive_questionnaire_completed": True,
                "progressive_questionnaire_step": 3,
                "task_completeness_analysis": completeness,  # ä¿å­˜åˆ†æç»“æœ
            }
            update_dict = WorkflowFlagManager.preserve_flags(state, update_dict)
            return Command(update=update_dict, goto="requirements_confirmation")

        # ğŸ”§ v7.80.8: ç”Ÿæˆé’ˆå¯¹æ€§è¡¥å……é—®é¢˜ï¼ˆç›®æ ‡æ•°é‡ä»5æå‡è‡³10ï¼‰
        existing_info_summary = ProgressiveQuestionnaireNode._build_existing_info_summary(structured_data)
        questions = analyzer.generate_gap_questions(
            missing_dimensions=completeness.get("missing_dimensions", []),
            critical_gaps=critical_gaps,
            confirmed_tasks=confirmed_tasks,
            existing_info_summary=existing_info_summary,
            target_count=10,  # ğŸ”§ v7.80.8: ä»5æ”¹ä¸º10
        )
        logger.info(f"ğŸ“ [v7.80.8] ç”Ÿæˆ {len(questions)} ä¸ªé’ˆå¯¹æ€§è¡¥å……é—®é¢˜ï¼ˆç›®æ ‡10ä¸ªï¼‰")

        # ğŸ†• v7.80.17: åº”ç”¨é—®é¢˜æ’åºï¼ˆå¿…ç­”é—®é¢˜åœ¨å‰ï¼ŒæŒ‰priorityæ’åºï¼‰
        questions = sorted(
            questions,
            key=lambda q: (
                0 if q.get("is_required", False) else 1,  # å¿…ç­”é—®é¢˜åœ¨å‰
                q.get("priority", 999),  # ä¼˜å…ˆçº§æ•°å­—å°çš„åœ¨å‰
                -q.get("weight", 5),  # æƒé‡å¤§çš„åœ¨å‰ï¼ˆè´Ÿå·åè½¬ï¼‰
            ),
        )
        logger.info(
            f"ğŸ“Š [v7.80.17] é—®é¢˜æ’åºå®Œæˆï¼š{len([q for q in questions if q.get('is_required')])}ä¸ªå¿…ç­”ï¼Œ{len([q for q in questions if not q.get('is_required')])}ä¸ªé€‰ç­”"
        )

        # è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯
        confirmed_task = state.get("confirmed_core_task", "")
        task_summary = ProgressiveQuestionnaireNode._build_task_summary(confirmed_tasks)

        # ğŸ†• v7.80.6: æ„å»ºæ–°çš„ interrupt payloadï¼ˆä»»åŠ¡å®Œæ•´æ€§å¯¼å‘ï¼‰
        payload = {
            "interaction_type": "progressive_questionnaire_step3",
            "step": 3,
            "total_steps": 3,
            "title": "è¡¥å……å…³é”®ä¿¡æ¯",
            "message": "ä¸ºäº†æ›´ç²¾å‡†åœ°ç†è§£æ‚¨çš„é¡¹ç›®éœ€æ±‚ï¼Œè¯·è¡¥å……ä»¥ä¸‹å…³é”®ä¿¡æ¯ï¼š",
            "core_task": confirmed_task,
            "task_summary": task_summary,
            # ğŸ†• ä»»åŠ¡å®Œæ•´æ€§ä¿¡æ¯
            "completeness_score": completeness.get("completeness_score", 0),
            "covered_dimensions": completeness.get("covered_dimensions", []),
            "missing_dimensions": completeness.get("missing_dimensions", []),
            "critical_gaps": critical_gaps,
            "questionnaire": {
                "introduction": f"å·²å®Œæ•´åº¦: {int(completeness.get('completeness_score', 0) * 100)}% | ç¼ºå¤±ç»´åº¦: {', '.join(completeness.get('missing_dimensions', []))}",
                "questions": questions,
                "note": "è¿™äº›é—®é¢˜æ¶‰åŠé¢„ç®—ã€æ—¶é—´ã€äº¤ä»˜ç­‰å…³é”®å†³ç­–ç‚¹ï¼Œè¯·æ ¹æ®å®é™…æƒ…å†µä½œç­”",
            },
            "options": {"submit": "æäº¤é—®å·", "back": "è¿”å›ä¿®æ”¹æ ¸å¿ƒä»»åŠ¡"},
        }

        logger.info("ğŸ›‘ [Step 3] å³å°†è°ƒç”¨ interrupt()ï¼Œç­‰å¾…ç”¨æˆ·è¾“å…¥...")
        user_response = interrupt(payload)
        logger.info(f"âœ… [Step 3] æ”¶åˆ°ç”¨æˆ·å“åº”: {type(user_response)}")

        # è§£æç”¨æˆ·å“åº”
        answers = {}
        # ğŸ”§ v7.87: ç§»é™¤è¿”å›ä¸Šä¸€æ­¥åŠŸèƒ½

        if isinstance(user_response, dict):
            # ğŸ”§ v7.87: ç§»é™¤ action == "back" å¤„ç†
            answers = user_response.get("answers") or {}
            # å°è¯•ä»å…¶ä»–æ ¼å¼æå–ç­”æ¡ˆ
            if not answers and "responses" in user_response:
                answers = user_response["responses"]

        logger.info(f"âœ… [Step 3] æ”¶é›†åˆ° {len(answers)} ä¸ªè¡¥å……ç­”æ¡ˆ")

        # æ„å»ºé—®å·æ‘˜è¦
        questionnaire_summary = ProgressiveQuestionnaireNode._build_questionnaire_summary(state, answers)

        # ğŸ†• v7.80.6: ä¿å­˜ä»»åŠ¡å®Œæ•´æ€§åˆ†æå’Œè¡¥å……ç­”æ¡ˆ
        update_dict = {
            "task_completeness_analysis": completeness,  # å®Œæ•´æ€§åˆ†æ
            "task_gap_filling_questionnaire": {
                "questions": questions,
                "missing_dimensions": completeness.get("missing_dimensions", []),
                "critical_gaps": critical_gaps,
            },
            "gap_filling_answers": answers,
            "progressive_questionnaire_completed": True,
            "progressive_questionnaire_step": 3,
            "questionnaire_summary": questionnaire_summary,  # å…¼å®¹æ—§å­—æ®µ
            "calibration_processed": True,  # å…¼å®¹æ—§å­—æ®µ
        }
        update_dict = WorkflowFlagManager.preserve_flags(state, update_dict)

        logger.info("âœ… [Step 3] é—®å·å®Œæˆï¼Œè·¯ç”±åˆ°éœ€æ±‚ç¡®è®¤èŠ‚ç‚¹")
        logger.info(f"   - æ”¶é›†åˆ° {len(answers)} ä¸ªè¡¥å……ç­”æ¡ˆ")
        logger.info(f"   - ä¸‹ä¸€æ­¥: requirements_confirmation")
        return Command(update=update_dict, goto="requirements_confirmation")

    # ==========================================================================
    # è¾…åŠ©æ–¹æ³•
    # ==========================================================================

    @staticmethod
    def _extract_core_task(state: ProjectAnalysisState) -> str:
        """
        ä»éœ€æ±‚åˆ†æç»“æœä¸­æå–æ ¸å¿ƒä»»åŠ¡

        ä¼˜å…ˆçº§ï¼š
        1. structured_data.project_task
        2. structured_data.core_objectives
        3. structured_data.project_overview
        4. user_input çš„æ‘˜è¦
        """
        agent_results = state.get("agent_results", {})
        requirements_result = agent_results.get("requirements_analyst", {})
        structured_data = requirements_result.get("structured_data", {})

        # å°è¯•ä»ä¸åŒå­—æ®µæå–
        project_task = structured_data.get("project_task") or structured_data.get("project_tasks")
        if project_task:
            if isinstance(project_task, list):
                return "ï¼›".join(project_task[:3])
            return str(project_task)

        core_objectives = structured_data.get("core_objectives", [])
        if core_objectives:
            if isinstance(core_objectives, list):
                return "æ ¸å¿ƒç›®æ ‡ï¼š" + "ï¼›".join(str(obj) for obj in core_objectives[:3])
            return f"æ ¸å¿ƒç›®æ ‡ï¼š{core_objectives}"

        project_overview = structured_data.get("project_overview", "")
        if project_overview:
            # æˆªå–å‰200å­—ç¬¦
            return project_overview[:200] + ("..." if len(project_overview) > 200 else "")

        # å…œåº•ï¼šä½¿ç”¨ç”¨æˆ·è¾“å…¥
        user_input = state.get("user_input", "")
        if user_input:
            return user_input[:300] + ("..." if len(user_input) > 300 else "")

        return "è¯·æè¿°æ‚¨çš„æ ¸å¿ƒéœ€æ±‚å’ŒæœŸæœ›ç›®æ ‡"

    @staticmethod
    def _build_task_summary(tasks: List[Dict[str, Any]]) -> str:
        """
        å°†ä»»åŠ¡åˆ—è¡¨æ„å»ºä¸ºä¸€å¥è¯æ‘˜è¦

        v7.80.1: ç”¨äºå…¼å®¹æ—§çš„ confirmed_core_task å­—æ®µ

        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨

        Returns:
            ä»»åŠ¡æ‘˜è¦å­—ç¬¦ä¸²
        """
        if not tasks:
            return "è¯·æè¿°æ‚¨çš„æ ¸å¿ƒéœ€æ±‚å’ŒæœŸæœ›ç›®æ ‡"

        # æå–ä»»åŠ¡æ ‡é¢˜
        titles = [task.get("title", "") for task in tasks if task.get("title")]

        if not titles:
            return "è¯·æè¿°æ‚¨çš„æ ¸å¿ƒéœ€æ±‚å’ŒæœŸæœ›ç›®æ ‡"

        # é™åˆ¶æ‘˜è¦é•¿åº¦
        if len(titles) <= 3:
            return "ï¼›".join(titles)
        else:
            return "ï¼›".join(titles[:3]) + f"ç­‰ {len(titles)} é¡¹ä»»åŠ¡"

    @staticmethod
    def _build_existing_info_summary(structured_data: Dict[str, Any]) -> str:
        """
        æ„å»ºå·²æœ‰ä¿¡æ¯æ‘˜è¦

        v7.80.6: ç”¨äºä»»åŠ¡å®Œæ•´æ€§åˆ†æ

        Args:
            structured_data: éœ€æ±‚åˆ†æå™¨çš„ç»“æ„åŒ–æ•°æ®

        Returns:
            å·²æœ‰ä¿¡æ¯æ‘˜è¦å­—ç¬¦ä¸²
        """
        parts = []

        # é¡¹ç›®ç±»å‹
        project_type = structured_data.get("project_type", "")
        if project_type:
            parts.append(f"é¡¹ç›®ç±»å‹: {project_type}")

        # åœ°ç‚¹
        location = structured_data.get("location", "")
        if location:
            parts.append(f"åœ°ç‚¹: {location}")

        # æ ¸å¿ƒç›®æ ‡
        objectives = structured_data.get("core_objectives", [])
        if objectives:
            if isinstance(objectives, list):
                parts.append(f"æ ¸å¿ƒç›®æ ‡: {', '.join(str(obj) for obj in objectives[:3])}")
            else:
                parts.append(f"æ ¸å¿ƒç›®æ ‡: {objectives}")

        # é¢„ç®—
        budget = structured_data.get("budget", "")
        if budget:
            parts.append(f"é¢„ç®—: {budget}")

        # æ—¶é—´
        timeline = structured_data.get("timeline", "")
        if timeline:
            parts.append(f"æ—¶é—´: {timeline}")

        # è§„æ¨¡
        scale = structured_data.get("scale", "") or structured_data.get("area", "")
        if scale:
            parts.append(f"è§„æ¨¡: {scale}")

        if not parts:
            return "ï¼ˆæš‚æ— ç»“æ„åŒ–ä¿¡æ¯ï¼‰"

        return " | ".join(parts)

    @staticmethod
    def _generate_gap_questions(gap_dimensions: List[str], state: ProjectAnalysisState) -> List[Dict[str, Any]]:
        """
        æ ¹æ®çŸ­æ¿ç»´åº¦ç”Ÿæˆè¡¥å……é—®é¢˜

        ä¼˜å…ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡æ¿ï¼Œå›é€€åˆ°é€šç”¨é—®é¢˜ã€‚
        """
        selector = DimensionSelector()
        questions = []

        for i, dim_id in enumerate(gap_dimensions[:5]):  # æœ€å¤š5ä¸ªé—®é¢˜
            # å°è¯•è·å–æ¨¡æ¿
            template = selector.get_gap_question_template(dim_id)

            if template:
                questions.append(
                    {
                        "id": f"gap_{dim_id}",
                        "question": template.get("question", ""),
                        "type": template.get("type", "single_choice"),
                        "options": template.get("options", []),
                        "context": f"å…³äºã€Œ{selector.get_dimension_by_id(dim_id).get('name', dim_id)}ã€ç»´åº¦çš„è¡¥å……",
                        "source_dimension": dim_id,
                    }
                )
            else:
                # ä½¿ç”¨é€šç”¨é—®é¢˜
                dim_config = selector.get_dimension_by_id(dim_id) or {}
                questions.append(
                    {
                        "id": f"gap_{dim_id}",
                        "question": f"å…³äºã€Œ{dim_config.get('name', dim_id)}ã€ï¼Œæ‚¨æœ‰ä»€ä¹ˆå…·ä½“çš„åå¥½æˆ–è¦æ±‚ï¼Ÿ",
                        "type": "open_ended",
                        "options": [],
                        "context": dim_config.get("description", "")[:100] if dim_config.get("description") else "",
                        "source_dimension": dim_id,
                    }
                )

        return questions

    @staticmethod
    def _build_questionnaire_summary(state: ProjectAnalysisState, gap_answers: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ„å»ºé—®å·æ‘˜è¦ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
        """
        timestamp = datetime.now().isoformat()

        # åˆå¹¶æ‰€æœ‰æ•°æ®
        radar_values = state.get("radar_dimension_values", {})
        radar_summary = state.get("radar_analysis_summary", {})
        confirmed_task = state.get("confirmed_core_task", "")

        entries = []

        # æ·»åŠ æ ¸å¿ƒä»»åŠ¡
        if confirmed_task:
            entries.append({"id": "core_task", "question": "æ ¸å¿ƒä»»åŠ¡", "value": confirmed_task, "type": "text"})

        # æ·»åŠ é›·è¾¾å›¾æ•°æ®
        for dim_id, value in radar_values.items():
            dim_detail = radar_summary.get("dimension_details", {}).get(dim_id, {})
            entries.append(
                {
                    "id": f"radar_{dim_id}",
                    "question": dim_detail.get("name", dim_id),
                    "value": value,
                    "type": "slider",
                    "tendency": dim_detail.get("tendency", ""),
                }
            )

        # æ·»åŠ Gapé—®é¢˜ç­”æ¡ˆ
        for q_id, answer in gap_answers.items():
            entries.append({"id": q_id, "question": q_id, "value": answer, "type": "gap_filling"})  # ç®€åŒ–å¤„ç†

        return {
            "entries": entries,
            "answers": {"core_task": confirmed_task, "radar_values": radar_values, "gap_answers": gap_answers},
            "submitted_at": timestamp,
            "timestamp": timestamp,
            "profile_label": radar_summary.get("profile_label", ""),
            "source": "progressive_questionnaire_v780",
            "notes": f"ä¸‰æ­¥é€’è¿›å¼é—®å·ï¼Œé£æ ¼æ ‡ç­¾ï¼š{radar_summary.get('profile_label', 'æœªå®šä¹‰')}",
        }


# ==========================================================================
# ğŸ†• v7.80.15 (P1.1): è¯—æ„è§£è¯»è¾…åŠ©å‡½æ•°
# ==========================================================================


def _contains_poetic_expression(text: str) -> bool:
    """
    æ£€æµ‹æ–‡æœ¬æ˜¯å¦åŒ…å«è¯—æ„/å“²å­¦/éšå–»è¡¨è¾¾

    æ£€æµ‹è§„åˆ™ï¼š
    1. åŒ…å«è‡ªç„¶æ„è±¡å…³é”®è¯ï¼ˆæœˆäº®ã€æ¹–é¢ã€é›ªã€äº‘ã€é£ç­‰ï¼‰
    2. åŒ…å«å“²å­¦/ç²¾ç¥å…³é”®è¯ï¼ˆè™šæ— ã€å¯¹è¯ã€å­˜åœ¨ã€çµé­‚ç­‰ï¼‰
    3. åŒ…å«æƒ…æ„Ÿ/æ°›å›´å…³é”®è¯ï¼ˆå®é™ã€å­¤ç‹¬ã€æ¼‚æµ®ã€æ²‰æµ¸ç­‰ï¼‰
    4. æ–‡æœ¬è¾ƒçŸ­ä½†å¯Œæœ‰æ„å¢ƒï¼ˆ<50å­—ä½†åŒ…å«ä¸Šè¿°å…³é”®è¯ï¼‰
    """
    poetic_keywords = [
        # è‡ªç„¶æ„è±¡
        "æœˆäº®",
        "æ¹–é¢",
        "ç»“å†°",
        "é›ª",
        "äº‘",
        "é£",
        "é›¨",
        "æ˜Ÿç©º",
        "å±±",
        "æ°´",
        "æ ‘",
        "èŠ±",
        "æµ·",
        "å¤©ç©º",
        "æ—¥å‡º",
        "æ—¥è½",
        "æ™¨æ›¦",
        "é»„æ˜",
        "å¤œ",
        "å…‰å½±",
        # å“²å­¦/ç²¾ç¥
        "è™šæ— ",
        "å­˜åœ¨",
        "å¯¹è¯",
        "çµé­‚",
        "ç²¾ç¥",
        "æ„è¯†",
        "è§‰æ‚Ÿ",
        "ç¦…",
        "æ‚Ÿ",
        "é“",
        "æ°¸æ’",
        "ç¬é—´",
        "æ—¶é—´",
        "ç©ºé—´",
        "è‡ªæˆ‘",
        "å†…å¿ƒ",
        "æœ¬è´¨",
        "çœŸå®",
        # æƒ…æ„Ÿ/æ°›å›´
        "å®é™",
        "å­¤ç‹¬",
        "æ¼‚æµ®",
        "æ²‰æµ¸",
        "æ²»æ„ˆ",
        "é‡ç”Ÿ",
        "å›å½’",
        "æ¸¸å­",
        "ä¹¡æ„",
        "è¯—æ„",
        "æ„å¢ƒ",
        "æ°›å›´",
        "æ„Ÿå—",
        "ä½“éªŒ",
        "å¿ƒçµ",
    ]

    text_lower = text.lower()
    matched = sum(1 for kw in poetic_keywords if kw in text_lower)

    # è§„åˆ™1: åŒ¹é…3ä¸ªæˆ–ä»¥ä¸Šå…³é”®è¯
    if matched >= 3:
        return True

    # è§„åˆ™2: çŸ­æ–‡æœ¬(<50å­—) + åŒ¹é…1-2ä¸ªå…³é”®è¯ + åŒ…å«æ¯”å–»æ€§æè¿°
    if len(text) < 50 and matched >= 1:
        metaphor_patterns = ["åƒ", "å¦‚åŒ", "ä»¿ä½›", "ä¼¼ä¹", "çŠ¹å¦‚", "å¥½ä¼¼", "è½åœ¨", "èå…¥", "åŒ–ä½œ"]
        if any(pattern in text for pattern in metaphor_patterns):
            return True

    return False


async def _llm_interpret_poetry(text: str) -> Dict[str, Any]:
    """
    ä½¿ç”¨ LLM è§£è¯»è¯—æ„/éšå–»è¡¨è¾¾

    Args:
        text: ç”¨æˆ·çš„è¯—æ„è¾“å…¥

    Returns:
        {
            "literal_tasks": ["ä»»åŠ¡1", "ä»»åŠ¡2"],  # å­—é¢æ„æ€çš„ä»»åŠ¡æè¿°
            "metaphor_explanation": "æœˆäº®=å®é™ï¼Œæ¹–é¢=é•œåƒï¼Œç»“å†°=å…‹åˆ¶",  # éšå–»è§£é‡Š
            "design_implications": ["å…‰å½±è®¾è®¡", "é•œé¢å…ƒç´ ", "å†·è‰²è°ƒ"]  # è®¾è®¡æŒ‡å‘
        }
    """
    import json

    from langchain_core.messages import HumanMessage, SystemMessage

    from ...services.llm_factory import LLMFactory

    system_prompt = """ä½ æ˜¯ä¸€ä¸ªè®¾è®¡è¯—æ„è¡¨è¾¾è§£è¯»ä¸“å®¶ã€‚ç”¨æˆ·å¯èƒ½ç”¨è¯—æ„ã€éšå–»ã€å“²å­¦æ€§çš„è¯­è¨€æè¿°è®¾è®¡éœ€æ±‚ã€‚

ä½ çš„ä»»åŠ¡ï¼š
1. æå–å­—é¢æ„æ€çš„æ ¸å¿ƒä»»åŠ¡ï¼ˆç”¨ç®€æ´çš„è®¾è®¡è¯­è¨€ï¼‰
2. è§£è¯»éšå–»å’Œæ„è±¡ï¼ˆæ¯ä¸ªå…³é”®è¯çš„å«ä¹‰ï¼‰
3. è½¬åŒ–ä¸ºå…·ä½“çš„è®¾è®¡æŒ‡å‘ï¼ˆææ–™ã€è‰²å½©ã€å…‰å½±ã€æ°›å›´ç­‰ï¼‰

è¾“å‡ºJSONï¼š
{
  "literal_tasks": ["æ ¸å¿ƒä»»åŠ¡1", "æ ¸å¿ƒä»»åŠ¡2"],
  "metaphor_explanation": "å…³é”®è¯A=å«ä¹‰ï¼Œå…³é”®è¯B=å«ä¹‰",
  "design_implications": ["è®¾è®¡å…ƒç´ 1", "è®¾è®¡å…ƒç´ 2", "è®¾è®¡å…ƒç´ 3"]
}"""

    user_prompt = f"""ç”¨æˆ·è¾“å…¥ï¼š

{text}

è¯·è§£è¯»è¿™æ®µè¯—æ„è¡¨è¾¾ï¼Œè¾“å‡ºJSONæ ¼å¼çš„ç»“æœã€‚"""

    try:
        llm = LLMFactory.create_llm(temperature=0.3)  # é™ä½æ¸©åº¦æé«˜ä¸€è‡´æ€§
        messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_prompt)]

        response = await llm.ainvoke(messages)
        response_text = response.content if hasattr(response, "content") else str(response)

        # è§£æJSON
        response_text = response_text.strip()
        if response_text.startswith("```json"):
            response_text = response_text[7:]
        if response_text.startswith("```"):
            response_text = response_text[3:]
        if response_text.endswith("```"):
            response_text = response_text[:-3]
        response_text = response_text.strip()

        result = json.loads(response_text)
        return result

    except Exception as e:
        logger.error(f"âŒ è¯—æ„è§£è¯»LLMè°ƒç”¨å¤±è´¥: {e}")
        # è¿”å›å…œåº•ç»“æœ
        return {
            "literal_tasks": [f"è¯—æ„ç©ºé—´è®¾è®¡ï¼š{text[:30]}"],
            "metaphor_explanation": "ï¼ˆè§£è¯»å¤±è´¥ï¼‰",
            "design_implications": ["æ°›å›´è¥é€ ", "æ„å¢ƒè¡¨è¾¾"],
        }


# ==========================================================================
# å¯¼å‡ºèŠ‚ç‚¹å‡½æ•°ï¼ˆç”¨äºworkflowæ³¨å†Œï¼‰
# ==========================================================================


def progressive_step1_core_task_node(state: ProjectAnalysisState, store: Optional[BaseStore] = None) -> Command:
    """Step 1 èŠ‚ç‚¹å‡½æ•°"""
    return ProgressiveQuestionnaireNode.step1_core_task(state, store)


def progressive_step2_radar_node(state: ProjectAnalysisState, store: Optional[BaseStore] = None) -> Command:
    """Step 2 èŠ‚ç‚¹å‡½æ•°"""
    return ProgressiveQuestionnaireNode.step2_radar(state, store)


def progressive_step3_gap_filling_node(state: ProjectAnalysisState, store: Optional[BaseStore] = None) -> Command:
    """Step 3 èŠ‚ç‚¹å‡½æ•°"""
    return ProgressiveQuestionnaireNode.step3_gap_filling(state, store)
