"""
é—®å·è°ƒæ•´å™¨æ¨¡å—

æä¾›é—®å·é—®é¢˜æ•°é‡çš„åŠ¨æ€è°ƒæ•´é€»è¾‘ã€‚
"""

from typing import Dict, Any, List, Tuple
from loguru import logger


class QuestionAdjuster:
    """
    é—®é¢˜æ•°é‡åŠ¨æ€è°ƒæ•´å™¨
    
    æ ¹æ®é—®å·æ€»é•¿åº¦åŠ¨æ€è°ƒæ•´ï¼šé¿å…é—®å·è¿‡é•¿å¯¼è‡´ç”¨æˆ·ç–²åŠ³ã€‚
    æ ¹æ®å†²çªä¸¥é‡æ€§åŠ¨æ€è°ƒæ•´ï¼šcriticalå†²çªæ—¶ä¼˜å…ˆä¿ç•™å†²çªé—®é¢˜ã€‚
    
    åŸå§‹ä½ç½®: calibration_questionnaire.py L534-709
    """
    
    @staticmethod
    def adjust(
        philosophy_questions: List[Dict[str, Any]],
        conflict_questions: List[Dict[str, Any]],
        original_question_count: int,
        feasibility_data: Dict[str, Any]
    ) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """
        åŠ¨æ€è°ƒæ•´é—®é¢˜æ•°é‡ï¼ˆğŸ†• P2è¿›é˜¶åŠŸèƒ½ï¼‰

        æ ¸å¿ƒç­–ç•¥ï¼š
        1. æ ¹æ®é—®å·æ€»é•¿åº¦åŠ¨æ€è°ƒæ•´ï¼šé¿å…é—®å·è¿‡é•¿å¯¼è‡´ç”¨æˆ·ç–²åŠ³
        2. æ ¹æ®å†²çªä¸¥é‡æ€§åŠ¨æ€è°ƒæ•´ï¼šcriticalå†²çªæ—¶ä¼˜å…ˆä¿ç•™å†²çªé—®é¢˜
        3. ä¼˜å…ˆçº§æ’åºï¼šç¡®ä¿ä¿ç•™æœ€æœ‰ä»·å€¼çš„é—®é¢˜

        è°ƒæ•´è§„åˆ™ï¼š
        - é—®å·æ€»é•¿åº¦ <= 7: ä¿ç•™å…¨éƒ¨é—®é¢˜
        - é—®å·æ€»é•¿åº¦ 8-10: è½»åº¦è£å‰ªï¼ˆä¿ç•™80%ï¼‰
        - é—®å·æ€»é•¿åº¦ 11-13: ä¸­åº¦è£å‰ªï¼ˆä¿ç•™60%ï¼‰
        - é—®å·æ€»é•¿åº¦ >= 14: é‡åº¦è£å‰ªï¼ˆä¿ç•™40%ï¼‰

        ä¼˜å…ˆçº§æ’åºï¼ˆä»é«˜åˆ°ä½ï¼‰ï¼š
        1. criticalå†²çªé—®é¢˜ > ç†å¿µé€‰æ‹©é—®é¢˜
        2. highå†²çªé—®é¢˜ > æ–¹æ¡ˆå€¾å‘é—®é¢˜
        3. ç›®æ ‡ç†å¿µé—®é¢˜ > mediumå†²çªé—®é¢˜
        4. å¼€æ”¾æ¢ç´¢é—®é¢˜ï¼ˆå¯é€‰ï¼‰

        Args:
            philosophy_questions: ç†å¿µæ¢ç´¢é—®é¢˜åˆ—è¡¨
            conflict_questions: èµ„æºå†²çªé—®é¢˜åˆ—è¡¨
            original_question_count: åŸå§‹é—®å·é—®é¢˜æ•°é‡
            feasibility_data: V1.5å¯è¡Œæ€§åˆ†ææ•°æ®ï¼ˆç”¨äºåˆ¤æ–­å†²çªä¸¥é‡æ€§ï¼‰

        Returns:
            è°ƒæ•´åçš„(philosophy_questions, conflict_questions)
        """
        total_injected = len(philosophy_questions) + len(conflict_questions)

        # å¦‚æœæ²¡æœ‰é—®é¢˜éœ€è¦æ³¨å…¥ï¼Œç›´æ¥è¿”å›
        if total_injected == 0:
            return [], []

        # è®¡ç®—é—®å·æ€»é•¿åº¦ï¼ˆåŸå§‹é—®é¢˜ + æ³¨å…¥é—®é¢˜ï¼‰
        total_length = original_question_count + total_injected

        # æ ¹æ®æ€»é•¿åº¦å†³å®šä¿ç•™æ¯”ä¾‹
        if total_length <= 7:
            # çŸ­é—®å·ï¼šä¿ç•™å…¨éƒ¨
            keep_ratio = 1.0
            logger.info(f"ğŸ“Š åŠ¨æ€è°ƒæ•´: é—®å·æ€»é•¿åº¦{total_length}â‰¤7, ä¿ç•™å…¨éƒ¨é—®é¢˜")
        elif total_length <= 10:
            # ä¸­ç­‰é—®å·ï¼šè½»åº¦è£å‰ª
            keep_ratio = 0.8
            logger.info(f"ğŸ“Š åŠ¨æ€è°ƒæ•´: é—®å·æ€»é•¿åº¦{total_length}åœ¨8-10, è½»åº¦è£å‰ªï¼ˆä¿ç•™80%ï¼‰")
        elif total_length <= 13:
            # è¾ƒé•¿é—®å·ï¼šä¸­åº¦è£å‰ª
            keep_ratio = 0.6
            logger.info(f"ğŸ“Š åŠ¨æ€è°ƒæ•´: é—®å·æ€»é•¿åº¦{total_length}åœ¨11-13, ä¸­åº¦è£å‰ªï¼ˆä¿ç•™60%ï¼‰")
        else:
            # è¶…é•¿é—®å·ï¼šé‡åº¦è£å‰ª
            keep_ratio = 0.4
            logger.info(f"ğŸ“Š åŠ¨æ€è°ƒæ•´: é—®å·æ€»é•¿åº¦{total_length}â‰¥14, é‡åº¦è£å‰ªï¼ˆä¿ç•™40%ï¼‰")

        # å¦‚æœä¸éœ€è¦è£å‰ªï¼Œç›´æ¥è¿”å›
        if keep_ratio >= 1.0:
            return philosophy_questions, conflict_questions

        # è®¡ç®—ç›®æ ‡ä¿ç•™æ•°é‡
        target_count = max(1, int(total_injected * keep_ratio))

        # æå–å†²çªä¸¥é‡æ€§ï¼ˆç”¨äºä¼˜å…ˆçº§åˆ¤æ–­ï¼‰
        conflict_severity = QuestionAdjuster._get_max_conflict_severity(feasibility_data)

        # ä¸ºæ¯ä¸ªé—®é¢˜åˆ†é…ä¼˜å…ˆçº§åˆ†æ•°
        scored_questions = []

        # è¯„åˆ†ï¼šå†²çªé—®é¢˜
        for cq in conflict_questions:
            severity = cq.get("severity", "unknown")
            if severity == "critical":
                score = 100
            elif severity == "high":
                score = 80
            elif severity == "medium":
                score = 60
            else:
                score = 40
            scored_questions.append({
                "question": cq,
                "score": score,
                "type": "conflict",
                "label": f"å†²çªé—®é¢˜({severity})"
            })

        # è¯„åˆ†ï¼šç†å¿µé—®é¢˜
        for pq in philosophy_questions:
            dimension = pq.get("dimension", "unknown")
            q_id = pq.get("id", "")

            # æ ¹æ®dimensionå’Œå†²çªä¸¥é‡æ€§åŠ¨æ€è°ƒæ•´åˆ†æ•°
            if dimension == "philosophy":
                # ç†å¿µé€‰æ‹©é—®é¢˜ï¼šå¦‚æœæœ‰criticalå†²çªï¼Œé™ä½ä¼˜å…ˆçº§ï¼›å¦åˆ™æœ€é«˜ä¼˜å…ˆçº§
                score = 70 if conflict_severity == "critical" else 90
                label = "ç†å¿µé€‰æ‹©é—®é¢˜"
            elif dimension == "approach":
                # æ–¹æ¡ˆå€¾å‘é—®é¢˜ï¼šå§‹ç»ˆé«˜ä¼˜å…ˆçº§
                score = 75
                label = "æ–¹æ¡ˆå€¾å‘é—®é¢˜"
            elif dimension == "goal":
                # ç›®æ ‡ç†å¿µé—®é¢˜ï¼šä¸­ç­‰ä¼˜å…ˆçº§
                score = 65
                label = "ç›®æ ‡ç†å¿µé—®é¢˜"
            elif dimension == "exploration":
                # å¼€æ”¾æ¢ç´¢é—®é¢˜ï¼šå¦‚æœé—®å·å¾ˆé•¿ï¼Œå¯è£å‰ªï¼›å¦åˆ™ä¿ç•™
                score = 50 if total_length >= 13 else 70
                label = "å¼€æ”¾æ¢ç´¢é—®é¢˜"
            else:
                score = 50
                label = "å…¶ä»–é—®é¢˜"

            scored_questions.append({
                "question": pq,
                "score": score,
                "type": "philosophy",
                "label": label
            })

        # æŒ‰åˆ†æ•°æ’åºï¼ˆä»é«˜åˆ°ä½ï¼‰
        scored_questions.sort(key=lambda x: x["score"], reverse=True)

        # ä¿ç•™å‰target_countä¸ªé—®é¢˜
        kept_questions = scored_questions[:target_count]

        # åˆ†ç¦»ç†å¿µé—®é¢˜å’Œå†²çªé—®é¢˜
        adjusted_philosophy = [q["question"] for q in kept_questions if q["type"] == "philosophy"]
        adjusted_conflict = [q["question"] for q in kept_questions if q["type"] == "conflict"]

        # è®°å½•è£å‰ªè¯¦æƒ…
        if len(kept_questions) < len(scored_questions):
            dropped = scored_questions[target_count:]
            dropped_labels = [q["label"] for q in dropped]
            logger.info(f"ğŸ“Š åŠ¨æ€è£å‰ª: ç§»é™¤ {len(dropped)} ä¸ªä½ä¼˜å…ˆçº§é—®é¢˜: {', '.join(dropped_labels)}")
            kept_labels = [q["label"] for q in kept_questions]
            logger.info(f"ğŸ“Š ä¿ç•™é—®é¢˜: {', '.join(kept_labels)}")

        return adjusted_philosophy, adjusted_conflict

    @staticmethod
    def _get_max_conflict_severity(feasibility_data: Dict[str, Any]) -> str:
        """
        è·å–æœ€é«˜å†²çªä¸¥é‡æ€§ç­‰çº§

        Args:
            feasibility_data: V1.5å¯è¡Œæ€§åˆ†ææ•°æ®

        Returns:
            æœ€é«˜ä¸¥é‡æ€§ç­‰çº§: "critical" | "high" | "medium" | "low" | "none"
        """
        if not feasibility_data:
            return "none"

        conflicts = feasibility_data.get("conflict_detection", {})
        if not conflicts:
            return "none"

        max_severity = "none"
        severity_order = {"critical": 4, "high": 3, "medium": 2, "low": 1, "none": 0}

        # æ£€æŸ¥æ‰€æœ‰å†²çªç±»å‹
        for conflict_type in ["budget_conflicts", "timeline_conflicts", "space_conflicts"]:
            conflict_list = conflicts.get(conflict_type, [])
            if conflict_list and conflict_list[0].get("detected"):
                severity = conflict_list[0].get("severity", "none")
                if severity_order.get(severity, 0) > severity_order.get(max_severity, 0):
                    max_severity = severity

        return max_severity
