"""
é—®å·ç”Ÿæˆä¸Šä¸‹æ–‡

å®šä¹‰é—®å·ç”Ÿæˆæ‰€éœ€çš„æ•°æ®ç»“æ„ï¼Œæ›¿ä»£ç›´æ¥è®¿é—® state å­—å…¸ï¼Œæå‡å¯æµ‹è¯•æ€§ã€‚
"""

import re
from dataclasses import dataclass, field
from typing import Dict, Any, Optional, List, Tuple
from loguru import logger


class KeywordExtractor:
    """
    æ™ºèƒ½å…³é”®è¯æå–å™¨

    ä»ç”¨æˆ·è¾“å…¥ä¸­æå–å…³é”®æ¦‚å¿µã€è¯†åˆ«é¢†åŸŸã€æ£€æµ‹ç”¨æˆ·æåŠçš„çº¦æŸæ¡ä»¶ã€‚
    ç”¨äºç”Ÿæˆé’ˆå¯¹æ€§çš„é—®å·é—®é¢˜ï¼Œé¿å…é€šç”¨æ¨¡æ¿é—®é¢˜ã€‚

    ğŸ”§ v7.4.1: æ·»åŠ ç¼“å­˜æœºåˆ¶ï¼Œé¿å…é‡å¤æå–
    """

    # ğŸ†• ç¼“å­˜å­—å…¸ï¼š{(user_input_hash, structured_data_hash): extraction_result}
    _cache: Dict[tuple, Dict[str, Any]] = {}
    _cache_max_size = 100  # æœ€å¤§ç¼“å­˜æ¡ç›®æ•°

    # é¢†åŸŸå…³é”®è¯åº“
    DOMAIN_KEYWORDS = {
        "tech_innovation": {
            "keywords": ["AI", "ç®—æ³•", "æ•°æ®", "æ™ºèƒ½", "è¿­ä»£", "æ•æ·", "æ¨¡å—åŒ–", "ä¼ æ„Ÿå™¨",
                        "çƒ­åŠ›å›¾", "ç‰©è”ç½‘", "IoT", "è‡ªåŠ¨åŒ–", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç ”å‘",
                        "ç§‘æŠ€", "äº’è”ç½‘", "è½¯ä»¶", "ç¡¬ä»¶", "èŠ¯ç‰‡", "äº‘è®¡ç®—", "å¤§æ•°æ®"],
            "label": "ç§‘æŠ€åˆ›æ–°",
            "question_focus": ["æŠ€æœ¯å®ç°è·¯å¾„", "æ•°æ®é©±åŠ¨å†³ç­–", "è¿­ä»£å‘¨æœŸ", "åä½œæ¨¡å¼"]
        },
        "hospitality": {
            "keywords": ["é…’åº—", "å®¢æˆ¿", "å¤§å ‚", "é¤å…", "SPA", "åº¦å‡", "æ°‘å®¿", "ä¼šæ‰€",
                        "å®´ä¼š", "åŒ…æˆ¿", "å‰å°", "å®¢æœ", "å…¥ä½", "é€€æˆ¿"],
            "label": "é…’åº—é¤é¥®",
            "question_focus": ["å®¢æˆ·ä½“éªŒ", "æœåŠ¡åŠ¨çº¿", "å“ç‰Œè°ƒæ€§", "è¿è¥æ•ˆç‡"]
        },
        "office": {
            "keywords": ["åŠå…¬", "å·¥ä½", "ä¼šè®®å®¤", "åä½œ", "å¼€æ”¾å¼", "ç‹¬ç«‹åŠå…¬", "å…±äº«åŠå…¬",
                        "è”åˆåŠå…¬", "å†™å­—æ¥¼", "å›­åŒº", "æ€»éƒ¨", "ç ”å‘ä¸­å¿ƒ", "åˆ›ä¸š"],
            "label": "åŠå…¬ç©ºé—´",
            "question_focus": ["å·¥ä½œæ¨¡å¼", "åä½œéœ€æ±‚", "ä¸“æ³¨éœ€æ±‚", "ä¼ä¸šæ–‡åŒ–"]
        },
        "retail": {
            "keywords": ["é›¶å”®", "å±•ç¤º", "åŠ¨çº¿", "åªæ•ˆ", "é—¨åº—", "æ——èˆ°åº—", "ä½“éªŒåº—",
                        "è´­ç‰©", "é™ˆåˆ—", "æ©±çª—", "æ”¶é“¶", "ä»“å‚¨"],
            "label": "é›¶å”®å•†ä¸š",
            "question_focus": ["å®¢æˆ·åŠ¨çº¿", "å±•ç¤ºæ•ˆæœ", "å“ç‰Œä½“éªŒ", "è¿è¥æ•ˆç‡"]
        },
        "residential": {
            "keywords": ["ä½å®…", "å…¬å¯“", "åˆ«å¢…", "å®¶", "å±…ä½", "å§å®¤", "å®¢å…", "å¨æˆ¿",
                        "å«ç”Ÿé—´", "é˜³å°", "ä¹¦æˆ¿", "å„¿ç«¥æˆ¿", "è€äººæˆ¿"],
            "label": "ä½å®…ç©ºé—´",
            "question_focus": ["ç”Ÿæ´»æ–¹å¼", "å®¶åº­ç»“æ„", "åŠŸèƒ½éœ€æ±‚", "æƒ…æ„Ÿéœ€æ±‚"]
        },
        "cultural_educational": {
            "keywords": ["å­¦æ ¡", "æ•™è‚²", "åŸ¹è®­", "å›¾ä¹¦é¦†", "åšç‰©é¦†", "å±•è§ˆ", "è‰ºæœ¯",
                        "æ–‡åŒ–", "ç”»å»Š", "å‰§é™¢", "éŸ³ä¹å…", "æ•™å®¤", "å®éªŒå®¤"],
            "label": "æ–‡åŒ–æ•™è‚²",
            "question_focus": ["å­¦ä¹ ä½“éªŒ", "å±•ç¤ºæ•ˆæœ", "äº’åŠ¨æ–¹å¼", "æ–‡åŒ–è¡¨è¾¾"]
        },
        "healthcare": {
            "keywords": ["åŒ»é™¢", "è¯Šæ‰€", "åŒ»ç–—", "å¥åº·", "åº·å¤", "å…»è€", "æŠ¤ç†",
                        "ç—…æˆ¿", "é—¨è¯Š", "æ‰‹æœ¯å®¤", "è¯æˆ¿"],
            "label": "åŒ»ç–—å¥åº·",
            "question_focus": ["æ‚£è€…ä½“éªŒ", "åŒ»ç–—æµç¨‹", "å®‰å…¨å«ç”Ÿ", "åº·å¤ç¯å¢ƒ"]
        }
    }

    # çº¦æŸæ¡ä»¶å…³é”®è¯
    CONSTRAINT_KEYWORDS = {
        "budget": ["é¢„ç®—", "æˆæœ¬", "è´¹ç”¨", "ä¸‡å…ƒ", "æŠ•èµ„", "é€ ä»·", "æŠ¥ä»·", "ä»·æ ¼", "ç»è´¹"],
        "timeline": ["å·¥æœŸ", "æ—¶é—´", "å‘¨æœŸ", "æœˆ", "å¤©", "å°½å¿«", "ç´§æ€¥", "deadline", "äº¤ä»˜"],
        "space": ["é¢ç§¯", "å¹³ç±³", "ã¡", "å¹³æ–¹", "ç©ºé—´ä¸è¶³", "æœ‰é™", "ç‹­å°", "ç´§å‡‘"],
        "regulation": ["è§„èŒƒ", "æ¶ˆé˜²", "ç‰©ä¸š", "å®¡æ‰¹", "åˆè§„", "æ ‡å‡†", "æ³•è§„"]
    }

    # æ ¸å¿ƒæ¦‚å¿µæå–æ¨¡å¼
    # ğŸ”¥ v7.4.2: ç®€åŒ–æ­£åˆ™æ¨¡å¼ï¼Œé¿å…ç¾éš¾æ€§å›æº¯
    # ğŸ”¥ v7.4.4: æ–°å¢å•å¼•å·æ”¯æŒï¼ŒåŒ¹é…æ›´å¤šç”¨æˆ·è¾“å…¥æ ¼å¼
    CONCEPT_PATTERNS = [
        r'"([^""]{2,15})"',  # ä¸­æ–‡å¼•å·åŒ…è£¹çš„æ¦‚å¿µï¼ˆå·¦å³å¼•å·é…å¯¹ï¼‰ï¼Œé™åˆ¶åˆ°15
        r'"([^"]{2,15})"',  # è‹±æ–‡å¼•å·åŒ…è£¹çš„æ¦‚å¿µï¼Œé™åˆ¶åˆ°15
        r"'([^'']{2,15})'",  # ğŸ†• ä¸­æ–‡å•å¼•å·ï¼ˆå¼¯å¼•å·ï¼‰ï¼Œé™åˆ¶åˆ°15
        r"'([^']{2,15})'",  # ğŸ†• è‹±æ–‡å•å¼•å·ï¼Œé™åˆ¶åˆ°15
        r'ã€Œ([^ã€]{2,15})ã€',  # æ—¥å¼å¼•å·ï¼Œé™åˆ¶åˆ°15
        r'ã€([^ã€‘]{2,15})ã€‘',  # æ–¹æ‹¬å·ï¼Œé™åˆ¶åˆ°15
        # ğŸ”¥ ç§»é™¤å¤æ‚çš„åŠ¨è¯+æ¦‚å¿µæ¨¡å¼ï¼Œè¿™äº›å®¹æ˜“å¯¼è‡´å›æº¯
    ]

    @classmethod
    def extract(cls, user_input: str, structured_data: Optional[Dict] = None) -> Dict[str, Any]:
        """
        ä»ç”¨æˆ·è¾“å…¥ä¸­æå–å…³é”®ä¿¡æ¯

        Args:
            user_input: ç”¨æˆ·åŸå§‹è¾“å…¥æ–‡æœ¬
            structured_data: éœ€æ±‚åˆ†æå¸ˆçš„ç»“æ„åŒ–è¾“å‡ºï¼ˆå¯é€‰ï¼‰

        Returns:
            åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
            - keywords: æå–çš„å…³é”®è¯åˆ—è¡¨ [(keyword, weight), ...]
            - domain: è¯†åˆ«çš„é¢†åŸŸ {"type": str, "label": str, "confidence": float}
            - core_concepts: æ ¸å¿ƒæ¦‚å¿µåˆ—è¡¨
            - user_mentioned_constraints: ç”¨æˆ·æåŠçš„çº¦æŸç±»å‹åˆ—è¡¨
            - question_focus: å»ºè®®çš„é—®é¢˜èšç„¦æ–¹å‘
        """
        if not user_input:
            return cls._empty_result()

        # ğŸ”¥ v7.4.2: æ›´ä¸¥æ ¼çš„è¾“å…¥éªŒè¯å’Œé•¿åº¦é™åˆ¶
        if len(user_input) > 5000:
            logger.warning(f"ğŸ” [KeywordExtractor] è¾“å…¥è¿‡é•¿ ({len(user_input)} å­—ç¬¦)ï¼Œæˆªæ–­åˆ° 5000")
            user_input = user_input[:5000]

        # ğŸ”¥ v7.4.2: é™åˆ¶ structured_data ä¸­çš„é•¿å­—æ®µï¼Œé¿å…æ­£åˆ™æŒ‚èµ·
        if structured_data:
            for key in ["design_challenge", "project_task", "project_overview", "character_narrative", "core_tension"]:
                if key in structured_data and isinstance(structured_data[key], str):
                    if len(structured_data[key]) > 300:  # ä»500é™åˆ°300
                        structured_data[key] = structured_data[key][:300]
                        logger.debug(f"ğŸ” [KeywordExtractor] æˆªæ–­ {key} å­—æ®µåˆ° 300 å­—ç¬¦")

        # ğŸ†• v7.4.1: æ£€æŸ¥ç¼“å­˜
        cache_key = cls._generate_cache_key(user_input, structured_data)
        if cache_key in cls._cache:
            logger.info(f"ğŸ” [KeywordExtractor] ä½¿ç”¨ç¼“å­˜ç»“æœ")
            return cls._cache[cache_key]

        logger.info(f"ğŸ” [KeywordExtractor] å¼€å§‹æå–ï¼Œè¾“å…¥é•¿åº¦: {len(user_input)}")

        # 1. æå–å…³é”®è¯ï¼ˆä½¿ç”¨ç®€å•çš„è¯é¢‘+ä½ç½®æƒé‡ï¼Œé¿å…ä¾èµ–jiebaï¼‰
        logger.info("ğŸ” [KeywordExtractor] Step 1: å¼€å§‹æå–å…³é”®è¯...")
        keywords = cls._extract_keywords_simple(user_input)
        logger.info(f"ğŸ” [KeywordExtractor] Step 1 å®Œæˆ: æå–äº† {len(keywords)} ä¸ªå…³é”®è¯")

        # 2. è¯†åˆ«é¢†åŸŸ
        logger.info("ğŸ” [KeywordExtractor] Step 2: å¼€å§‹è¯†åˆ«é¢†åŸŸ...")
        domain = cls._identify_domain(user_input, keywords)
        logger.info(f"ğŸ” [KeywordExtractor] Step 2 å®Œæˆ: é¢†åŸŸ={domain.get('label', 'unknown')}")

        # 3. æå–æ ¸å¿ƒæ¦‚å¿µ
        logger.info("ğŸ” [KeywordExtractor] Step 3: å¼€å§‹æå–æ ¸å¿ƒæ¦‚å¿µ...")
        core_concepts = cls._extract_core_concepts(user_input, structured_data)
        logger.info(f"ğŸ” [KeywordExtractor] Step 3 å®Œæˆ: æå–äº† {len(core_concepts)} ä¸ªæ¦‚å¿µ")

        # 4. æ£€æµ‹ç”¨æˆ·æåŠçš„çº¦æŸ
        logger.info("ğŸ” [KeywordExtractor] Step 4: å¼€å§‹æ£€æµ‹çº¦æŸ...")
        mentioned_constraints = cls._detect_constraints(user_input)
        logger.info(f"ğŸ” [KeywordExtractor] Step 4 å®Œæˆ: æ£€æµ‹åˆ° {len(mentioned_constraints)} ä¸ªçº¦æŸ")

        # 5. ç¡®å®šé—®é¢˜èšç„¦æ–¹å‘
        logger.info("ğŸ” [KeywordExtractor] Step 5: å¼€å§‹ç¡®å®šèšç„¦æ–¹å‘...")
        question_focus = cls._determine_question_focus(domain, core_concepts)
        logger.info(f"ğŸ” [KeywordExtractor] Step 5 å®Œæˆ: {len(question_focus)} ä¸ªèšç„¦æ–¹å‘")

        result = {
            "keywords": keywords,
            "domain": domain,
            "core_concepts": core_concepts,
            "user_mentioned_constraints": mentioned_constraints,
            "question_focus": question_focus
        }

        logger.info(f"ğŸ” [KeywordExtractor] é¢†åŸŸè¯†åˆ«: {domain['label']} (ç½®ä¿¡åº¦: {domain['confidence']:.2f})")
        logger.info(f"ğŸ” [KeywordExtractor] æ ¸å¿ƒæ¦‚å¿µ: {core_concepts[:5]}")
        logger.info(f"ğŸ” [KeywordExtractor] ç”¨æˆ·çº¦æŸ: {mentioned_constraints}")

        # ğŸ†• v7.4.1: å­˜å‚¨åˆ°ç¼“å­˜
        cls._store_in_cache(cache_key, result)

        return result

    @classmethod
    def _generate_cache_key(cls, user_input: str, structured_data: Optional[Dict]) -> tuple:
        """
        ç”Ÿæˆç¼“å­˜é”®

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            structured_data: ç»“æ„åŒ–æ•°æ®

        Returns:
            ç¼“å­˜é”®å…ƒç»„
        """
        import hashlib
        import json

        # å¯¹ user_input ç”Ÿæˆå“ˆå¸Œ
        input_hash = hashlib.md5(user_input.encode('utf-8')).hexdigest()

        # å¯¹ structured_data ç”Ÿæˆå“ˆå¸Œï¼ˆä»…ä½¿ç”¨å…³é”®å­—æ®µï¼‰
        if structured_data:
            # åªä½¿ç”¨å½±å“æå–ç»“æœçš„å…³é”®å­—æ®µ
            key_fields = {
                "design_challenge": structured_data.get("design_challenge", ""),
                "project_task": structured_data.get("project_task", "")
            }
            data_str = json.dumps(key_fields, sort_keys=True, ensure_ascii=False)
            data_hash = hashlib.md5(data_str.encode('utf-8')).hexdigest()
        else:
            data_hash = "none"

        return (input_hash, data_hash)

    @classmethod
    def _store_in_cache(cls, cache_key: tuple, result: Dict[str, Any]) -> None:
        """
        å­˜å‚¨ç»“æœåˆ°ç¼“å­˜

        Args:
            cache_key: ç¼“å­˜é”®
            result: æå–ç»“æœ
        """
        # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œç§»é™¤æœ€æ—§çš„æ¡ç›®ï¼ˆFIFOï¼‰
        if len(cls._cache) >= cls._cache_max_size:
            # ç§»é™¤ç¬¬ä¸€ä¸ªæ¡ç›®
            first_key = next(iter(cls._cache))
            del cls._cache[first_key]
            logger.debug(f"ğŸ” [KeywordExtractor] ç¼“å­˜å·²æ»¡ï¼Œç§»é™¤æœ€æ—§æ¡ç›®")

        cls._cache[cache_key] = result
        logger.debug(f"ğŸ” [KeywordExtractor] ç»“æœå·²ç¼“å­˜ (cache_size={len(cls._cache)})")

    @classmethod
    def _extract_keywords_simple(cls, text: str) -> List[Tuple[str, float]]:
        """ç®€å•å…³é”®è¯æå–ï¼ˆä¸ä¾èµ–jiebaï¼‰"""
        keywords = []

        # ğŸ”¥ ç´§æ€¥ä¿®å¤: é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé¿å…æ­£åˆ™è¡¨è¾¾å¼æŒ‚èµ·
        safe_text = text[:1000] if len(text) > 1000 else text  # ä»2000é™åˆ°1000
        logger.debug(f"ğŸ” [KeywordExtractor] _extract_keywords_simple: è¾“å…¥é•¿åº¦={len(text)}, å¤„ç†é•¿åº¦={len(safe_text)}")

        # ä»æ‰€æœ‰é¢†åŸŸå…³é”®è¯ä¸­åŒ¹é…
        for domain_info in cls.DOMAIN_KEYWORDS.values():
            for kw in domain_info["keywords"]:
                if kw.lower() in safe_text.lower():
                    # æƒé‡ï¼šå‡ºç°ä½ç½®è¶Šé å‰æƒé‡è¶Šé«˜
                    pos = safe_text.lower().find(kw.lower())
                    weight = 1.0 - (pos / len(safe_text)) * 0.5
                    keywords.append((kw, weight))

        # æå–å¼•å·å†…çš„è¯ä½œä¸ºé«˜æƒé‡å…³é”®è¯
        # ğŸ”¥ v7.4.2: ä½¿ç”¨æ›´å®‰å…¨çš„æ­£åˆ™æ¨¡å¼ï¼Œé¿å…ç¾éš¾æ€§å›æº¯
        quoted_patterns = [
            r'"([^""]{2,15})"',  # ä¸­æ–‡å¼•å·ï¼ˆå·¦å³é…å¯¹ï¼‰ï¼Œé™åˆ¶é•¿åº¦åˆ°15
            r'"([^"]{2,15})"',   # è‹±æ–‡å¼•å·ï¼Œé™åˆ¶é•¿åº¦åˆ°15
            r'ã€Œ([^ã€]{2,15})ã€',  # æ—¥å¼å¼•å·ï¼Œé™åˆ¶é•¿åº¦åˆ°15
            r'ã€([^ã€‘]{2,15})ã€‘'   # æ–¹æ‹¬å·ï¼Œé™åˆ¶é•¿åº¦åˆ°15
        ]
        for pattern in quoted_patterns:
            try:
                # ğŸ”¥ v7.4.2: é™åˆ¶åŒ¹é…æ¬¡æ•°ï¼Œé¿å…è¿‡å¤šåŒ¹é…å¯¼è‡´æ€§èƒ½é—®é¢˜
                matches = re.findall(pattern, safe_text[:500])  # åªåœ¨å‰500å­—ç¬¦ä¸­åŒ¹é…
                for match in matches[:10]:  # æœ€å¤šå–10ä¸ªåŒ¹é…
                    # æ¸…ç†åŒ¹é…ç»“æœï¼Œå»é™¤æ®‹ç•™å¼•å·
                    clean_match = match.strip().strip('"').strip('"').strip('"')
                    if clean_match and len(clean_match) >= 2 and clean_match not in [k[0] for k in keywords]:
                        keywords.append((clean_match, 1.0))
            except re.error as e:
                logger.warning(f"âš ï¸ Regex error in keyword extraction (pattern: {pattern}): {e}")
                continue
            except Exception as e:
                logger.error(f"âŒ Unexpected error in keyword extraction: {e}")
                continue

        # æŒ‰æƒé‡æ’åº
        keywords.sort(key=lambda x: x[1], reverse=True)
        return keywords[:15]

    @classmethod
    def _identify_domain(cls, text: str, keywords: List[Tuple[str, float]]) -> Dict[str, Any]:
        """è¯†åˆ«é¡¹ç›®é¢†åŸŸ"""
        domain_scores = {}

        # ğŸ”¥ v7.4.2: æ›´ä¸¥æ ¼çš„é™åˆ¶
        safe_text = text[:1000] if len(text) > 1000 else text  # ä»2000é™åˆ°1000

        for domain_type, domain_info in cls.DOMAIN_KEYWORDS.items():
            score = 0
            matched_keywords = []
            for kw in domain_info["keywords"]:
                if kw.lower() in safe_text.lower():
                    score += 1
                    matched_keywords.append(kw)
                    if len(matched_keywords) >= 10:  # ğŸ”¥ v7.4.2: é™åˆ¶åŒ¹é…æ•°é‡
                        break

            if score > 0:
                domain_scores[domain_type] = {
                    "score": score,
                    "matched": matched_keywords,
                    "label": domain_info["label"],
                    "question_focus": domain_info["question_focus"]
                }

        if not domain_scores:
            return {"type": "general", "label": "é€šç”¨", "confidence": 0.3, "question_focus": []}

        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„é¢†åŸŸ
        best_domain = max(domain_scores.items(), key=lambda x: x[1]["score"])
        confidence = min(best_domain[1]["score"] / 5, 1.0)  # 5ä¸ªå…³é”®è¯ä»¥ä¸Šä¸ºæ»¡åˆ†

        return {
            "type": best_domain[0],
            "label": best_domain[1]["label"],
            "confidence": confidence,
            "matched_keywords": best_domain[1]["matched"],
            "question_focus": best_domain[1]["question_focus"]
        }

    @classmethod
    def _extract_core_concepts(cls, text: str, structured_data: Optional[Dict]) -> List[str]:
        """æå–æ ¸å¿ƒæ¦‚å¿µ"""
        concepts = []

        # ä»æ­£åˆ™æ¨¡å¼æå–
        # ğŸ”¥ v7.4.2: æ›´ä¸¥æ ¼çš„é™åˆ¶ï¼Œé¿å…æ­£åˆ™è¡¨è¾¾å¼æŒ‚èµ·
        safe_text = text[:500] if len(text) > 500 else text  # ä»1000é™åˆ°500
        for pattern in cls.CONCEPT_PATTERNS:
            try:
                # ğŸ”¥ v7.4.2: é™åˆ¶åŒ¹é…æ¬¡æ•°
                matches = re.findall(pattern, safe_text)
                concepts.extend(matches[:5])  # æ¯ä¸ªæ¨¡å¼æœ€å¤šå–5ä¸ª
            except re.error as e:
                logger.warning(f"âš ï¸ Regex error in concept extraction (pattern: {pattern[:50]}...): {e}")
                continue
            except Exception as e:
                logger.error(f"âŒ Unexpected error in concept extraction: {e}")
                continue

        # ä»ç»“æ„åŒ–æ•°æ®è¡¥å……
        if structured_data:
            # ä» design_challenge æå–
            design_challenge = structured_data.get("design_challenge", "")
            if design_challenge and isinstance(design_challenge, str):
                # ğŸ”¥ v7.4.2: æ›´ä¸¥æ ¼çš„é™åˆ¶
                safe_challenge = design_challenge[:300] if len(design_challenge) > 300 else design_challenge
                try:
                    # ğŸ”¥ v7.4.2: ä½¿ç”¨æ›´å®‰å…¨çš„æ­£åˆ™ï¼Œé™åˆ¶é•¿åº¦
                    bracket_matches = re.findall(r'\[([^\]]{2,15})\]', safe_challenge)
                    concepts.extend(bracket_matches[:5])  # æœ€å¤šå–5ä¸ª
                except Exception as e:
                    logger.warning(f"âš ï¸ Error extracting from design_challenge: {e}")

            # ä» project_task æå–
            project_task = structured_data.get("project_task", "")
            if project_task and isinstance(project_task, str):
                # ğŸ”¥ v7.4.2: æ›´ä¸¥æ ¼çš„é™åˆ¶
                safe_task = project_task[:300] if len(project_task) > 300 else project_task
                try:
                    # ğŸ”¥ v7.4.2: ä½¿ç”¨æ›´å®‰å…¨çš„æ­£åˆ™ï¼Œé™åˆ¶é•¿åº¦
                    bracket_matches = re.findall(r'\[([^\]]{2,15})\]', safe_task)
                    concepts.extend(bracket_matches[:5])  # æœ€å¤šå–5ä¸ª
                except Exception as e:
                    logger.warning(f"âš ï¸ Error extracting from project_task: {e}")

        # å»é‡å¹¶ä¿æŒé¡ºåºï¼Œæ¸…ç†æ®‹ç•™å¼•å·
        seen = set()
        unique_concepts = []
        
        # ğŸ”¥ v7.4.4: é»‘åå• - è¿‡æ»¤æ‰ç³»ç»Ÿç”Ÿæˆçš„æ ‡é¢˜/æç¤ºæ–‡æœ¬
        blacklist = {
            "ç”¨æˆ·éœ€æ±‚æè¿°", "é™„ä»¶ææ–™", "é™„ä»¶", "è¯´æ˜", "æ‘˜è¦", "å†…å®¹", 
            "èƒŒæ™¯èµ„æ–™", "å‚è€ƒä¿¡æ¯", "æ˜ç¡®è¦æ±‚", "èƒŒæ™¯ä¿¡æ¯", "é¡¹ç›®èƒŒæ™¯",
            "éœ€æ±‚æè¿°", "è¯¦ç»†éœ€æ±‚", "å…·ä½“éœ€æ±‚", "åŸºæœ¬éœ€æ±‚", "æ ¸å¿ƒéœ€æ±‚"
        }
        
        for c in concepts[:20]:  # ğŸ”¥ v7.4.2: é™åˆ¶å¤„ç†æ•°é‡
            # æ¸…ç†æ®‹ç•™å¼•å·å’Œç©ºç™½
            c_clean = c.strip().strip('"').strip('"').strip('"').strip('ã€Œ').strip('ã€').strip()
            # è¿‡æ»¤æ— æ•ˆæ¦‚å¿µ
            if (c_clean
                and c_clean not in seen
                and c_clean not in blacklist  # ğŸ”¥ v7.4.4: è¿‡æ»¤é»‘åå•è¯æ±‡
                and len(c_clean) >= 2
                and len(c_clean) <= 20  # ğŸ”¥ v7.4.2: æ·»åŠ æœ€å¤§é•¿åº¦é™åˆ¶
                and not c_clean.endswith('"')  # æ’é™¤ä¸å®Œæ•´çš„å¼•å·
                and not c_clean.startswith('"')):
                seen.add(c_clean)
                unique_concepts.append(c_clean)

        return unique_concepts[:10]

    @classmethod
    def _detect_constraints(cls, text: str) -> List[str]:
        """æ£€æµ‹ç”¨æˆ·æåŠçš„çº¦æŸæ¡ä»¶"""
        mentioned = []

        for constraint_type, keywords in cls.CONSTRAINT_KEYWORDS.items():
            for kw in keywords:
                if kw in text:
                    if constraint_type not in mentioned:
                        mentioned.append(constraint_type)
                    break

        return mentioned

    @classmethod
    def _determine_question_focus(cls, domain: Dict, core_concepts: List[str]) -> List[str]:
        """ç¡®å®šé—®é¢˜èšç„¦æ–¹å‘"""
        focus = domain.get("question_focus", []).copy()

        # æ ¹æ®æ ¸å¿ƒæ¦‚å¿µè¡¥å……èšç„¦æ–¹å‘
        concept_focus_map = {
            "è¿­ä»£": "è¿­ä»£å‘¨æœŸä¸çµæ´»æ€§",
            "æ¨¡å—åŒ–": "æ¨¡å—åŒ–ç¨‹åº¦ä¸é‡ç»„æ–¹å¼",
            "æ•°æ®": "æ•°æ®é©±åŠ¨å†³ç­–",
            "æ™ºèƒ½": "æ™ºèƒ½åŒ–ç¨‹åº¦",
            "åä½œ": "åä½œæ¨¡å¼",
            "ä½“éªŒ": "ç”¨æˆ·ä½“éªŒä¼˜å…ˆçº§",
            "å“ç‰Œ": "å“ç‰Œè¡¨è¾¾æ–¹å¼"
        }

        for concept in core_concepts:
            for key, focus_item in concept_focus_map.items():
                if key in concept and focus_item not in focus:
                    focus.append(focus_item)

        return focus[:6]

    @classmethod
    def _empty_result(cls) -> Dict[str, Any]:
        """è¿”å›ç©ºç»“æœ"""
        return {
            "keywords": [],
            "domain": {"type": "general", "label": "é€šç”¨", "confidence": 0.0, "question_focus": []},
            "core_concepts": [],
            "user_mentioned_constraints": [],
            "question_focus": []
        }


@dataclass
class QuestionContext:
    """
    é—®å·ç”Ÿæˆä¸Šä¸‹æ–‡

    å°è£…é—®å·ç”Ÿæˆæ‰€éœ€çš„æ‰€æœ‰æ•°æ®ï¼Œé¿å…ç›´æ¥ä¾èµ– ProjectAnalysisStateï¼Œ
    ä½¿ç”Ÿæˆå™¨å¯ä»¥ç‹¬ç«‹æµ‹è¯•ã€‚

    Attributes:
        project_task: é¡¹ç›®ä»»åŠ¡æè¿°
        character_narrative: è§’è‰²å™äº‹
        design_challenge: è®¾è®¡æŒ‘æˆ˜
        core_tension: æ ¸å¿ƒçŸ›ç›¾
        resource_constraints: èµ„æºçº¦æŸ
        project_type: é¡¹ç›®ç±»å‹ (personal_residential/hybrid_residential_commercial/commercial_enterprise)
        expert_handoff: ä¸“å®¶äº¤æ¥æ•°æ®
        user_input: ç”¨æˆ·åŸå§‹è¾“å…¥
        structured_data: å®Œæ•´çš„ç»“æ„åŒ–æ•°æ®ï¼ˆç”¨äºå¤æ‚åœºæ™¯ï¼‰
        extracted_info: ğŸ†• æ™ºèƒ½æå–çš„å…³é”®ä¿¡æ¯ï¼ˆé¢†åŸŸã€å…³é”®è¯ã€æ ¸å¿ƒæ¦‚å¿µã€ç”¨æˆ·çº¦æŸï¼‰
    """

    project_task: str = ""
    character_narrative: str = ""
    design_challenge: str = ""
    core_tension: str = ""
    resource_constraints: str = ""
    project_type: str = ""
    expert_handoff: Dict[str, Any] = None
    user_input: str = ""
    structured_data: Dict[str, Any] = None
    extracted_info: Dict[str, Any] = None  # ğŸ†• æ™ºèƒ½æå–çš„å…³é”®ä¿¡æ¯

    def __post_init__(self):
        """åˆå§‹åŒ–é»˜è®¤å€¼"""
        if self.expert_handoff is None:
            self.expert_handoff = {}
        if self.structured_data is None:
            self.structured_data = {}
        if self.extracted_info is None:
            self.extracted_info = {}

    @classmethod
    def from_state(cls, state: Dict[str, Any]) -> "QuestionContext":
        """
        ä» ProjectAnalysisState æ„å»ºä¸Šä¸‹æ–‡

        Args:
            state: é¡¹ç›®åˆ†æçŠ¶æ€å­—å…¸

        Returns:
            QuestionContext å®ä¾‹
        """
        agent_results = state.get("agent_results", {})
        requirements_result = agent_results.get("requirements_analyst", {})
        structured_data = requirements_result.get("structured_data", {})
        user_input = state.get("user_input", "")

        # ğŸ†• æ™ºèƒ½æå–å…³é”®ä¿¡æ¯
        extracted_info = KeywordExtractor.extract(user_input, structured_data)

        return cls(
            project_task=structured_data.get("project_task", ""),
            character_narrative=structured_data.get("character_narrative", ""),
            design_challenge=structured_data.get("design_challenge", ""),
            core_tension=structured_data.get("core_tension", ""),
            resource_constraints=structured_data.get("resource_constraints", ""),
            project_type=structured_data.get("project_type", ""),
            expert_handoff=structured_data.get("expert_handoff", {}),
            user_input=user_input,
            structured_data=structured_data,
            extracted_info=extracted_info
        )

    # ğŸ†• ä¾¿æ·å±æ€§è®¿é—®
    @property
    def domain(self) -> Dict[str, Any]:
        """è·å–è¯†åˆ«çš„é¢†åŸŸä¿¡æ¯"""
        return self.extracted_info.get("domain", {"type": "general", "label": "é€šç”¨"})

    @property
    def keywords(self) -> List[Tuple[str, float]]:
        """è·å–æå–çš„å…³é”®è¯"""
        return self.extracted_info.get("keywords", [])

    @property
    def core_concepts(self) -> List[str]:
        """è·å–æ ¸å¿ƒæ¦‚å¿µ"""
        return self.extracted_info.get("core_concepts", [])

    @property
    def user_mentioned_constraints(self) -> List[str]:
        """è·å–ç”¨æˆ·æåŠçš„çº¦æŸç±»å‹"""
        return self.extracted_info.get("user_mentioned_constraints", [])

    @property
    def question_focus(self) -> List[str]:
        """è·å–å»ºè®®çš„é—®é¢˜èšç„¦æ–¹å‘"""
        return self.extracted_info.get("question_focus", [])
