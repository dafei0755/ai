"""
LLMé©±åŠ¨çš„é—®å·ç”Ÿæˆå™¨

ä½¿ç”¨LLMåŸºäºç”¨æˆ·è¾“å…¥å’Œéœ€æ±‚åˆ†æç»“æœç”Ÿæˆé«˜åº¦å®šåˆ¶åŒ–çš„é—®å·ï¼Œ
è§£å†³æ­£åˆ™/è¯åº“åŒ¹é…æ–¹å¼æ— æ³•ç†è§£ç”¨æˆ·æ„å›¾çš„é—®é¢˜ã€‚

v7.5 æ–°å¢ï¼š
- LLMé©±åŠ¨ç”Ÿæˆï¼šåˆ©ç”¨LLMçš„æ·±åº¦ç†è§£èƒ½åŠ›ç”Ÿæˆé’ˆå¯¹æ€§é—®é¢˜
- ç›¸å…³æ€§éªŒè¯ï¼šç”ŸæˆåéªŒè¯æ¯ä¸ªé—®é¢˜ä¸ç”¨æˆ·éœ€æ±‚çš„ç›¸å…³æ€§
- å›é€€æœºåˆ¶ï¼šLLMå¤±è´¥æ—¶ä½¿ç”¨FallbackQuestionGenerator

v7.18 æ–°å¢ï¼š
- æ·»åŠ  PerformanceMonitor æ€§èƒ½ç›‘æ§
- å…±äº«å‡½æ•°è¿ç§»åˆ° shared_agent_utils.py

ä½œè€…ï¼šDesign Beyond Team
æ—¥æœŸï¼š2025-12-11
"""

import json
import re
import time
from typing import Dict, Any, List, Optional, Tuple
from loguru import logger

# ğŸ†• v7.18: å¯¼å…¥æ€§èƒ½ç›‘æ§
from ...utils.shared_agent_utils import PerformanceMonitor


class LLMQuestionGenerator:
    """
    LLMé©±åŠ¨çš„é—®å·ç”Ÿæˆå™¨

    ä½¿ç”¨LLMåŸºäºç”¨æˆ·è¾“å…¥å’Œéœ€æ±‚åˆ†æç»“æœç”Ÿæˆå®šåˆ¶åŒ–é—®å·ï¼Œ
    è§£å†³ä¼ ç»ŸåŸºäºè§„åˆ™/è¯åº“çš„ç”Ÿæˆå™¨æ— æ³•ç†è§£ç”¨æˆ·æ·±å±‚éœ€æ±‚çš„é—®é¢˜ã€‚
    """

    # æç¤ºè¯ç‰ˆæœ¬å·
    PROMPT_VERSION = "1.0"

    @classmethod
    def generate(
        cls,
        user_input: str,
        structured_data: Dict[str, Any],
        llm_model: Optional[Any] = None,
        timeout: int = 30
    ) -> Tuple[List[Dict[str, Any]], str]:
        """
        ä½¿ç”¨LLMç”Ÿæˆå®šåˆ¶åŒ–é—®å·

        Args:
            user_input: ç”¨æˆ·åŸå§‹è¾“å…¥
            structured_data: éœ€æ±‚åˆ†æå¸ˆçš„ç»“æ„åŒ–è¾“å‡º
            llm_model: LLMæ¨¡å‹å®ä¾‹ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºé»˜è®¤å®ä¾‹ï¼‰
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            Tuple[List[Dict], str]:
                - é—®é¢˜åˆ—è¡¨
                - ç”Ÿæˆæ¥æºæ ‡è¯†ï¼ˆ"llm_generated" | "fallback"ï¼‰
        """
        logger.info("ğŸ¤– [LLMQuestionGenerator] å¼€å§‹LLMé©±åŠ¨é—®å·ç”Ÿæˆ")
        start_time = time.time()  # ğŸ†• v7.18: æ€§èƒ½ç›‘æ§å¼€å§‹

        try:
            # 1. æ„å»ºåˆ†ææ‘˜è¦
            analysis_summary = cls._build_analysis_summary(structured_data)
            logger.info(f"ğŸ“Š [LLMQuestionGenerator] åˆ†ææ‘˜è¦é•¿åº¦: {len(analysis_summary)}")

            # ğŸ†• v7.23: æå–ç”¨æˆ·å…³é”®è¯å¹¶æ³¨å…¥åˆ°åˆ†ææ‘˜è¦
            user_keywords = cls._extract_user_keywords(user_input)
            if user_keywords:
                keywords_str = "ã€".join(user_keywords[:12])
                analysis_summary += f"\n\n## âš ï¸ ç”¨æˆ·å…³é”®è¯ï¼ˆé—®é¢˜å¿…é¡»å¼•ç”¨ï¼‰\n{keywords_str}"
                logger.info(f"ğŸ”‘ [LLMQuestionGenerator] æ³¨å…¥ç”¨æˆ·å…³é”®è¯: {keywords_str[:50]}...")

            # 2. è·å–æˆ–åˆ›å»ºLLMå®ä¾‹
            if llm_model is None:
                from ...services.llm_factory import LLMFactory
                llm_model = LLMFactory.create_llm(temperature=0.7)
                logger.info("ğŸ”§ [LLMQuestionGenerator] ä½¿ç”¨é»˜è®¤LLMå®ä¾‹")

            # 3. åŠ è½½æç¤ºè¯
            system_prompt, human_prompt = cls._load_prompts(user_input, analysis_summary)

            # 4. è°ƒç”¨LLMç”Ÿæˆé—®å·
            logger.info("ğŸ“¤ [LLMQuestionGenerator] è°ƒç”¨LLMç”Ÿæˆé—®å·...")
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": human_prompt}
            ]

            response = llm_model.invoke(messages)
            raw_content = response.content if hasattr(response, "content") else str(response)
            logger.info(f"ğŸ“¥ [LLMQuestionGenerator] LLMå“åº”é•¿åº¦: {len(raw_content)}")

            # 5. è§£æLLMè¾“å‡º
            questionnaire_data = cls._parse_llm_response(raw_content)
            questions = questionnaire_data.get("questions", [])

            if not questions:
                logger.warning("âš ï¸ [LLMQuestionGenerator] LLMè¿”å›ç©ºé—®å·ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ")
                # ğŸ†• v7.18: è®°å½•å¤±è´¥æ€§èƒ½
                PerformanceMonitor.record("LLMQuestionGenerator", time.time() - start_time, "v7.18-fallback")
                return cls._fallback_generate(structured_data, user_input), "fallback"

            # 6. éªŒè¯å’Œä¿®å¤é—®é¢˜æ ¼å¼
            validated_questions = cls._validate_and_fix_questions(questions)
            logger.info(f"âœ… [LLMQuestionGenerator] æˆåŠŸç”Ÿæˆ {len(validated_questions)} ä¸ªé—®é¢˜")

            # ğŸ†• v7.6+v7.12: éªŒè¯é—®é¢˜ä¸ç”¨æˆ·è¾“å…¥çš„ç›¸å…³æ€§
            relevance_score, low_relevance_questions = cls._check_question_relevance(
                validated_questions, user_input
            )
            
            # ğŸ†• v7.18: è®°å½•æˆåŠŸæ€§èƒ½
            PerformanceMonitor.record("LLMQuestionGenerator", time.time() - start_time, "v7.18")
            
            if relevance_score < 0.3:
                # ğŸ”§ v7.12: ç›¸å…³æ€§è¿‡ä½æ—¶å°è¯•ç¬¬äºŒæ¬¡ç”Ÿæˆï¼Œå¼ºåŒ–å…³é”®è¯è¦æ±‚
                logger.warning(f"âš ï¸ [LLMQuestionGenerator] ç›¸å…³æ€§è¿‡ä½ ({relevance_score:.2f})ï¼Œå°è¯•ç¬¬äºŒæ¬¡ç”Ÿæˆ")
                user_keywords = cls._extract_user_keywords(user_input)
                if user_keywords:
                    try:
                        regenerated_questions, _ = cls._regenerate_with_keywords(
                            user_input, analysis_summary, user_keywords, llm_model
                        )
                        if regenerated_questions:
                            new_score, _ = cls._check_question_relevance(regenerated_questions, user_input)
                            if new_score > relevance_score:
                                logger.info(f"âœ… [LLMQuestionGenerator] é‡æ–°ç”Ÿæˆæå‡ç›¸å…³æ€§: {relevance_score:.2f} â†’ {new_score:.2f}")
                                validated_questions = regenerated_questions
                                relevance_score = new_score
                    except Exception as e:
                        logger.warning(f"âš ï¸ [LLMQuestionGenerator] é‡æ–°ç”Ÿæˆå¤±è´¥: {e}")
            elif relevance_score < 0.5:
                logger.warning(f"âš ï¸ [LLMQuestionGenerator] é—®é¢˜ç›¸å…³æ€§è¾ƒä½ ({relevance_score:.2f})ï¼Œ"
                              f"ä½ç›¸å…³é—®é¢˜: {low_relevance_questions}")

            # 7. è®°å½•ç”Ÿæˆç†ç”±ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            rationale = questionnaire_data.get("generation_rationale", "")
            if rationale:
                logger.info(f"ğŸ’¡ [LLMQuestionGenerator] ç”Ÿæˆç†ç”±: {rationale[:200]}...")

            return validated_questions, "llm_generated"

        except Exception as e:
            logger.error(f"âŒ [LLMQuestionGenerator] LLMç”Ÿæˆå¤±è´¥: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()

            # ä½¿ç”¨å›é€€æ–¹æ¡ˆ
            return cls._fallback_generate(structured_data, user_input), "fallback"

    @classmethod
    def _build_analysis_summary(cls, structured_data: Dict[str, Any]) -> str:
        """
        æ„å»ºéœ€æ±‚åˆ†ææ‘˜è¦ï¼Œç”¨äºæç¤ºè¯æ³¨å…¥

        ğŸ”§ v7.6ä¿®å¤ï¼š
        - æ‰©å±•å­—æ®µæå–èŒƒå›´ï¼ˆæ–°å¢ project_overview, core_objectives, narrative_characters ç­‰ï¼‰
        - å¢åŠ å­—æ®µåˆ«åå…¼å®¹ï¼ˆå¦‚ project_task/project_overview äº’è¡¥ï¼‰
        - æå– constraints_opportunities ç­‰é‡è¦å†³ç­–ä¿¡æ¯

        Args:
            structured_data: éœ€æ±‚åˆ†æå¸ˆçš„ç»“æ„åŒ–è¾“å‡º

        Returns:
            æ ¼å¼åŒ–çš„åˆ†ææ‘˜è¦å­—ç¬¦ä¸²
        """
        summary_parts = []

        # ğŸ†• v7.6: é¡¹ç›®æ¦‚è§ˆï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼Œç›´æ¥å½±å“é—®å·é’ˆå¯¹æ€§ï¼‰
        project_overview = structured_data.get("project_overview", "")
        if project_overview:
            summary_parts.append(f"## é¡¹ç›®æ¦‚è§ˆ\n{project_overview}")

        # é¡¹ç›®ä»»åŠ¡ï¼ˆå…¼å®¹æ—§å­—æ®µåï¼‰
        project_task = structured_data.get("project_task", "") or structured_data.get("project_tasks", "")
        if isinstance(project_task, list):
            project_task = "ï¼›".join(project_task[:5])  # æœ€å¤š5ä¸ªä»»åŠ¡
        if project_task and project_task != project_overview:  # é¿å…é‡å¤
            summary_parts.append(f"## é¡¹ç›®ä»»åŠ¡\n{project_task}")

        # ğŸ†• v7.6: æ ¸å¿ƒç›®æ ‡
        core_objectives = structured_data.get("core_objectives", [])
        if core_objectives:
            if isinstance(core_objectives, list):
                objectives_text = "\n".join([f"- {obj}" for obj in core_objectives[:5]])
            else:
                objectives_text = str(core_objectives)
            summary_parts.append(f"## æ ¸å¿ƒç›®æ ‡\n{objectives_text}")

        # é¡¹ç›®ç±»å‹
        project_type = structured_data.get("project_type", "")
        if project_type:
            type_label = {
                "personal_residential": "ä¸ªäººä½å®…",
                "hybrid_residential_commercial": "æ··åˆå‹ï¼ˆä½å®…+å•†ä¸šï¼‰",
                "commercial_enterprise": "å•†ä¸š/ä¼ä¸šé¡¹ç›®",
                "cultural_educational": "æ–‡åŒ–/æ•™è‚²é¡¹ç›®",
                "healthcare_wellness": "åŒ»ç–—/åº·å…»é¡¹ç›®",
                "office_coworking": "åŠå…¬/è”åˆåŠå…¬",
                "hospitality_tourism": "é…’åº—/æ–‡æ—…é¡¹ç›®",
                "sports_entertainment_arts": "ä½“è‚²/å¨±ä¹/è‰ºæœ¯"
            }.get(project_type, project_type)
            summary_parts.append(f"## é¡¹ç›®ç±»å‹\n{type_label}")

        # è®¾è®¡æŒ‘æˆ˜
        design_challenge = structured_data.get("design_challenge", "")
        if design_challenge:
            summary_parts.append(f"## æ ¸å¿ƒè®¾è®¡æŒ‘æˆ˜\n{design_challenge}")

        # æ ¸å¿ƒå¼ åŠ›
        core_tension = structured_data.get("core_tension", "")
        if core_tension:
            summary_parts.append(f"## æ ¸å¿ƒçŸ›ç›¾/å¼ åŠ›\n{core_tension}")

        # ğŸ†• v7.6: äººç‰©å™äº‹ï¼ˆä¼˜å…ˆ narrative_charactersï¼Œå…¼å®¹ character_narrativeï¼‰
        narrative_characters = structured_data.get("narrative_characters", "") or structured_data.get("character_narrative", "")
        if isinstance(narrative_characters, list):
            narrative_characters = "\n".join([f"- {char}" for char in narrative_characters[:3]])
        if narrative_characters:
            summary_parts.append(f"## äººç‰©å™äº‹/ç”¨æˆ·ç”»åƒ\n{narrative_characters}")

        # ğŸ†• v7.6: ç‰©ç†ç¯å¢ƒï¼ˆphysical_contextsï¼‰
        physical_contexts = structured_data.get("physical_contexts", "")
        if isinstance(physical_contexts, list):
            physical_contexts = "ï¼›".join(physical_contexts[:3])
        if physical_contexts:
            summary_parts.append(f"## ç‰©ç†ç¯å¢ƒ\n{physical_contexts}")

        # èµ„æºçº¦æŸ
        resource_constraints = structured_data.get("resource_constraints", "")
        if resource_constraints:
            summary_parts.append(f"## èµ„æºçº¦æŸ\n{resource_constraints}")

        # ğŸ†• v7.6: çº¦æŸä¸æœºé‡ï¼ˆconstraints_opportunitiesï¼‰
        constraints_opportunities = structured_data.get("constraints_opportunities", "")
        if isinstance(constraints_opportunities, dict):
            co_parts = []
            if constraints_opportunities.get("constraints"):
                co_parts.append(f"çº¦æŸ: {constraints_opportunities['constraints']}")
            if constraints_opportunities.get("opportunities"):
                co_parts.append(f"æœºé‡: {constraints_opportunities['opportunities']}")
            constraints_opportunities = "ï¼›".join(co_parts)
        if constraints_opportunities and constraints_opportunities != resource_constraints:
            summary_parts.append(f"## çº¦æŸä¸æœºé‡\n{constraints_opportunities}")

        # ä¸“å®¶äº¤æ¥ä¿¡æ¯
        expert_handoff = structured_data.get("expert_handoff", {})
        if expert_handoff:
            handoff_summary = []
            if expert_handoff.get("design_challenge_spectrum"):
                spectrum = expert_handoff.get("design_challenge_spectrum", {})
                æç«¯A = spectrum.get("æç«¯A", {}).get("æ ‡ç­¾", "")
                æç«¯B = spectrum.get("æç«¯B", {}).get("æ ‡ç­¾", "")
                if æç«¯A and æç«¯B:
                    handoff_summary.append(f"è®¾è®¡å…‰è°±: {æç«¯A} â†â†’ {æç«¯B}")
            if expert_handoff.get("required_roles"):
                roles = expert_handoff.get("required_roles", [])
                if roles:
                    handoff_summary.append(f"å»ºè®®ä¸“å®¶: {', '.join(roles[:5])}")
            # ğŸ†• v7.6: æå–å…³é”®é—®é¢˜ï¼ˆcritical_questionsï¼‰
            critical_questions = expert_handoff.get("critical_questions_for_experts", {})
            if critical_questions:
                cq_list = []
                for role, questions in list(critical_questions.items())[:3]:
                    if questions:
                        # ğŸ”§ ç¡®ä¿ q_text æ˜¯å­—ç¬¦ä¸²
                        if isinstance(questions, list):
                            q_text = questions[0] if questions else ""
                        elif isinstance(questions, dict):
                            # å¦‚æœæ˜¯å­—å…¸ï¼Œå°è¯•æå–ç¬¬ä¸€ä¸ªå€¼
                            q_text = next(iter(questions.values())) if questions else ""
                        else:
                            q_text = str(questions)
                        
                        # ç¡®ä¿ q_text æ˜¯å­—ç¬¦ä¸²åå†åˆ‡ç‰‡
                        if isinstance(q_text, str) and q_text:
                            cq_list.append(f"- {role}: {q_text[:50]}...")
                if cq_list:
                    handoff_summary.append(f"å…³é”®é—®é¢˜:\n" + "\n".join(cq_list))
            if handoff_summary:
                summary_parts.append(f"## ä¸“å®¶äº¤æ¥\n" + "\n".join(handoff_summary))

        # ğŸ”¥ v7.6: å¦‚æœæ‘˜è¦ä¸ºç©ºï¼Œè¿”å›æ›´æœ‰ç”¨çš„æç¤ºè€Œé"æš‚æ— "
        if not summary_parts:
            logger.warning("âš ï¸ [LLMQuestionGenerator] structured_data å­—æ®µå…¨éƒ¨ä¸ºç©ºï¼Œå»ºè®®æ£€æŸ¥éœ€æ±‚åˆ†æå¸ˆè¾“å‡º")
            return "ï¼ˆéœ€æ±‚åˆ†ææ•°æ®ä¸è¶³ï¼Œè¯·åŸºäºç”¨æˆ·åŸå§‹è¾“å…¥ç”Ÿæˆé—®å·ï¼‰"

        return "\n\n".join(summary_parts)

    @classmethod
    def _load_prompts(cls, user_input: str, analysis_summary: str) -> Tuple[str, str]:
        """
        åŠ è½½å¹¶å¡«å……æç¤ºè¯æ¨¡æ¿

        Args:
            user_input: ç”¨æˆ·åŸå§‹è¾“å…¥
            analysis_summary: åˆ†ææ‘˜è¦

        Returns:
            Tuple[str, str]: (system_prompt, human_prompt)
        """
        try:
            from ...core.prompt_manager import PromptManager
            prompt_manager = PromptManager()

            # åŠ è½½é—®å·ç”Ÿæˆå™¨æç¤ºè¯ (è¿”å›å®Œæ•´é…ç½®å­—å…¸)
            prompt_data = prompt_manager.get_prompt("questionnaire_generator", return_full_config=True)

            if prompt_data is None or not isinstance(prompt_data, dict):
                raise ValueError("questionnaire_generator prompt not found or invalid format")

            system_prompt = prompt_data.get("system_prompt", "")
            human_template = prompt_data.get("human_prompt_template", "")

            if not system_prompt:
                raise ValueError("system_prompt is empty")

            # å¡«å……human_promptæ¨¡æ¿
            human_prompt = human_template.format(
                user_input=user_input,
                analysis_summary=analysis_summary
            ) if human_template else f"""# ç”¨æˆ·åŸå§‹è¾“å…¥
{user_input}

# éœ€æ±‚åˆ†æç»“æœ
{analysis_summary}

è¯·ç”Ÿæˆä¸€ä»½å®šåˆ¶åŒ–é—®å·ï¼ˆ7-10ä¸ªé—®é¢˜ï¼‰ï¼Œç›´æ¥è¿”å›JSONæ ¼å¼ã€‚"""

            return system_prompt, human_prompt

        except Exception as e:
            logger.warning(f"âš ï¸ [LLMQuestionGenerator] åŠ è½½æç¤ºè¯å¤±è´¥: {e}")
            # ä½¿ç”¨å†…ç½®çš„åŸºç¡€æç¤ºè¯
            return cls._get_fallback_prompts(user_input, analysis_summary)

    @classmethod
    def _get_fallback_prompts(cls, user_input: str, analysis_summary: str) -> Tuple[str, str]:
        """
        è·å–å†…ç½®çš„å›é€€æç¤ºè¯

        Args:
            user_input: ç”¨æˆ·åŸå§‹è¾“å…¥
            analysis_summary: åˆ†ææ‘˜è¦

        Returns:
            Tuple[str, str]: (system_prompt, human_prompt)
        """
        system_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è®¾è®¡éœ€æ±‚æ´å¯Ÿä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·è¾“å…¥ç”Ÿæˆ7-10ä¸ªå®šåˆ¶åŒ–é—®å·é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. é—®é¢˜å¿…é¡»ç´§å¯†å›´ç»•ç”¨æˆ·çš„å…·ä½“éœ€æ±‚
2. ä½¿ç”¨å†²çªé€‰æ‹©é¢˜æ­ç¤ºç”¨æˆ·çš„çœŸå®ä¼˜å…ˆçº§
3. é¿å…é€šç”¨æ¨¡æ¿é—®é¢˜

è¾“å‡ºæ ¼å¼ï¼ˆçº¯JSONï¼‰ï¼š
{
  "introduction": "é—®å·å¼•è¨€",
  "questions": [
    {
      "id": "å”¯ä¸€ID",
      "question": "é—®é¢˜æ–‡æœ¬(å•é€‰)|(å¤šé€‰)|(å¼€æ”¾é¢˜)",
      "context": "é—®é¢˜èƒŒæ™¯",
      "type": "single_choice|multiple_choice|open_ended",
      "options": ["é€‰é¡¹A", "é€‰é¡¹B"],  // å•é€‰/å¤šé€‰å¿…å¡«
      "placeholder": "ç¤ºä¾‹..."  // å¼€æ”¾é¢˜å¿…å¡«
    }
  ],
  "generation_rationale": "ç”Ÿæˆç†ç”±"
}"""

        human_prompt = f"""# ç”¨æˆ·åŸå§‹è¾“å…¥
{user_input}

# éœ€æ±‚åˆ†æç»“æœ
{analysis_summary}

è¯·ç”Ÿæˆä¸€ä»½å®šåˆ¶åŒ–é—®å·ï¼ˆ7-10ä¸ªé—®é¢˜ï¼‰ï¼Œç›´æ¥è¿”å›JSONæ ¼å¼ã€‚"""

        return system_prompt, human_prompt

    @classmethod
    def _parse_llm_response(cls, raw_content: str) -> Dict[str, Any]:
        """
        è§£æLLMå“åº”ï¼Œæå–JSONæ ¼å¼çš„é—®å·æ•°æ®

        Args:
            raw_content: LLMåŸå§‹å“åº”å†…å®¹

        Returns:
            è§£æåçš„é—®å·æ•°æ®å­—å…¸
        """
        # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
        content = raw_content.strip()

        # å°è¯•ç§»é™¤ ```json ... ``` åŒ…è£…
        if content.startswith("```"):
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ¢è¡Œç¬¦åçš„å†…å®¹
            first_newline = content.find("\n")
            if first_newline != -1:
                content = content[first_newline + 1:]
            # ç§»é™¤ç»“å°¾çš„ ```
            if content.endswith("```"):
                content = content[:-3].strip()

        # å°è¯•ç›´æ¥è§£æJSON
        try:
            return json.loads(content)
        except json.JSONDecodeError:
            pass

        # å°è¯•æå–JSONå¯¹è±¡
        json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
        matches = re.findall(json_pattern, content, re.DOTALL)

        for match in matches:
            try:
                parsed = json.loads(match)
                if "questions" in parsed:
                    return parsed
            except json.JSONDecodeError:
                continue

        # æœ€åå°è¯•ï¼šæŸ¥æ‰¾åŒ…å«questionsçš„å¤§JSONå—
        start_idx = content.find('{"')
        if start_idx == -1:
            start_idx = content.find("{'")

        if start_idx != -1:
            # å°è¯•æ‰¾åˆ°åŒ¹é…çš„ç»“æŸå¤§æ‹¬å·
            brace_count = 0
            for i, char in enumerate(content[start_idx:]):
                if char == "{":
                    brace_count += 1
                elif char == "}":
                    brace_count -= 1
                    if brace_count == 0:
                        try:
                            return json.loads(content[start_idx:start_idx + i + 1])
                        except json.JSONDecodeError:
                            break

        logger.warning("âš ï¸ [LLMQuestionGenerator] æ— æ³•è§£æLLMå“åº”ä¸ºJSON")
        return {"questions": []}

    @classmethod
    def _validate_and_fix_questions(cls, questions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        éªŒè¯å’Œä¿®å¤é—®é¢˜æ ¼å¼

        Args:
            questions: åŸå§‹é—®é¢˜åˆ—è¡¨

        Returns:
            éªŒè¯åçš„é—®é¢˜åˆ—è¡¨
        """
        validated = []
        type_count = {"single_choice": 0, "multiple_choice": 0, "open_ended": 0}

        for i, q in enumerate(questions):
            # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
            if not isinstance(q, dict):
                continue

            question_text = q.get("question", "")
            if not question_text:
                continue

            # ç¡®ä¿æœ‰å”¯ä¸€ID
            if not q.get("id"):
                q["id"] = f"llm_q_{i + 1}"

            # éªŒè¯å’Œä¿®æ­£typeå­—æ®µ
            q_type = q.get("type", "").lower()
            if q_type not in ["single_choice", "multiple_choice", "open_ended"]:
                # ä»é—®é¢˜æ–‡æœ¬æ¨æ–­ç±»å‹
                if "(å•é€‰)" in question_text or "å•é€‰" in question_text:
                    q_type = "single_choice"
                elif "(å¤šé€‰)" in question_text or "å¤šé€‰" in question_text:
                    q_type = "multiple_choice"
                elif "(å¼€æ”¾é¢˜)" in question_text or "å¼€æ”¾" in question_text:
                    q_type = "open_ended"
                else:
                    # å¦‚æœæœ‰optionsåˆ™ä¸ºé€‰æ‹©é¢˜ï¼Œå¦åˆ™ä¸ºå¼€æ”¾é¢˜
                    if q.get("options"):
                        q_type = "single_choice"
                    else:
                        q_type = "open_ended"
            q["type"] = q_type
            type_count[q_type] = type_count.get(q_type, 0) + 1

            # éªŒè¯é€‰é¡¹ï¼ˆé€‰æ‹©é¢˜å¿…é¡»æœ‰optionsï¼‰
            if q_type in ["single_choice", "multiple_choice"]:
                if not q.get("options") or not isinstance(q["options"], list):
                    # å°è¯•ä»é—®é¢˜æ–‡æœ¬ä¸­æå–é€‰é¡¹
                    q["options"] = ["é€‰é¡¹A", "é€‰é¡¹B", "é€‰é¡¹C"]
                    logger.warning(f"âš ï¸ é—®é¢˜ {q['id']} ç¼ºå°‘é€‰é¡¹ï¼Œä½¿ç”¨å ä½é€‰é¡¹")

            # éªŒè¯å¼€æ”¾é¢˜çš„placeholder
            if q_type == "open_ended":
                if not q.get("placeholder"):
                    q["placeholder"] = "è¯·è¯¦ç»†æè¿°..."

            # ç¡®ä¿contextå­˜åœ¨
            if not q.get("context"):
                q["context"] = ""

            validated.append(q)

        # æŒ‰é¢˜å‹æ’åºï¼šå•é€‰ â†’ å¤šé€‰ â†’ å¼€æ”¾
        validated.sort(key=lambda x: {
            "single_choice": 0,
            "multiple_choice": 1,
            "open_ended": 2
        }.get(x.get("type", ""), 3))

        logger.info(f"ğŸ“Š [LLMQuestionGenerator] é—®é¢˜ç±»å‹åˆ†å¸ƒ: å•é€‰={type_count['single_choice']}, "
                    f"å¤šé€‰={type_count['multiple_choice']}, å¼€æ”¾={type_count['open_ended']}")

        return validated

    @classmethod
    def _fallback_generate(
        cls,
        structured_data: Dict[str, Any],
        user_input: str
    ) -> List[Dict[str, Any]]:
        """
        å›é€€åˆ°è§„åˆ™ç”Ÿæˆå™¨

        Args:
            structured_data: ç»“æ„åŒ–æ•°æ®
            user_input: ç”¨æˆ·è¾“å…¥

        Returns:
            é—®é¢˜åˆ—è¡¨
        """
        logger.info("ğŸ”„ [LLMQuestionGenerator] ä½¿ç”¨FallbackQuestionGeneratorå›é€€")

        from .generators import FallbackQuestionGenerator
        from .context import KeywordExtractor

        extracted_info = KeywordExtractor.extract(user_input, structured_data)
        return FallbackQuestionGenerator.generate(
            structured_data,
            user_input=user_input,
            extracted_info=extracted_info
        )

    @classmethod
    def _check_question_relevance(
        cls,
        questions: List[Dict[str, Any]],
        user_input: str
    ) -> Tuple[float, List[str]]:
        """
        ğŸ†• v7.6: æ£€æŸ¥é—®é¢˜ä¸ç”¨æˆ·è¾“å…¥çš„å…³é”®è¯é‡å åº¦

        é€šè¿‡ç®€å•çš„å…³é”®è¯åŒ¹é…ï¼ˆä¸è°ƒç”¨LLMï¼‰ï¼Œå¿«é€ŸéªŒè¯é—®é¢˜æ˜¯å¦å¼•ç”¨äº†ç”¨æˆ·åŸè¯ã€‚

        Args:
            questions: é—®é¢˜åˆ—è¡¨
            user_input: ç”¨æˆ·åŸå§‹è¾“å…¥

        Returns:
            Tuple[float, List[str]]:
                - å¹³å‡ç›¸å…³æ€§å¾—åˆ† (0-1)
                - ä½ç›¸å…³æ€§é—®é¢˜IDåˆ—è¡¨
        """
        if not questions or not user_input:
            return 1.0, []

        # æå–ç”¨æˆ·è¾“å…¥ä¸­çš„å…³é”®è¯ï¼ˆå»é™¤åœç”¨è¯ï¼‰
        import re
        stopwords = {
            "çš„", "æ˜¯", "åœ¨", "æœ‰", "æˆ‘", "ä½ ", "ä»–", "å¥¹", "å®ƒ", "ä»¬",
            "è¿™", "é‚£", "å’Œ", "ä¸", "æˆ–", "ä½†", "è€Œ", "äº†", "ç€", "è¿‡",
            "å—", "å‘¢", "å§", "å•Š", "å‘€", "å“¦", "å—¯", "å¯ä»¥", "èƒ½å¤Ÿ",
            "éœ€è¦", "å¸Œæœ›", "æƒ³è¦", "ä¸€ä¸ª", "ä¸€äº›", "ä¸€ç§", "è¿™ä¸ª", "é‚£ä¸ª",
            "å¦‚ä½•", "æ€ä¹ˆ", "ä»€ä¹ˆ", "å“ªäº›", "ä¸ºä»€ä¹ˆ", "è¯·", "å¸®", "åš",
            "è¿›è¡Œ", "å®ç°", "å®Œæˆ", "è€ƒè™‘", "åŒ…æ‹¬", "é€šè¿‡", "ä½¿ç”¨"
        }

        # ä»ç”¨æˆ·è¾“å…¥æå–å…³é”®è¯ï¼ˆ2-10ä¸ªå­—ç¬¦çš„è¯ï¼‰
        user_words = set()
        # æå–ä¸­æ–‡è¯è¯­
        chinese_words = re.findall(r'[\u4e00-\u9fa5]{2,10}', user_input)
        for word in chinese_words:
            if word not in stopwords:
                user_words.add(word)
        # æå–æ•°å­—+å•ä½
        numbers = re.findall(r'\d+[\u4e00-\u9fa5ã¡]+', user_input)
        user_words.update(numbers)
        # æå–è‹±æ–‡è¯
        english_words = re.findall(r'[a-zA-Z]{3,}', user_input)
        user_words.update(english_words)

        if not user_words:
            return 1.0, []  # æ— æ³•æå–å…³é”®è¯ï¼Œè·³è¿‡æ£€æŸ¥

        logger.info(f"ğŸ” [RelevanceCheck] ç”¨æˆ·å…³é”®è¯: {list(user_words)[:15]}...")

        # æ£€æŸ¥æ¯ä¸ªé—®é¢˜æ˜¯å¦åŒ…å«ç”¨æˆ·å…³é”®è¯
        scores = []
        low_relevance = []

        for q in questions:
            question_text = q.get("question", "") + " ".join(q.get("options", []))
            # è®¡ç®—å…³é”®è¯å‘½ä¸­æ•°
            hits = sum(1 for word in user_words if word in question_text)
            # å½’ä¸€åŒ–å¾—åˆ†
            score = min(1.0, hits / max(3, len(user_words) * 0.3))  # å‘½ä¸­3ä¸ªè¯å³æ»¡åˆ†
            scores.append(score)

            if score < 0.3:  # ä½äº30%ç›¸å…³æ€§
                low_relevance.append(q.get("id", "unknown"))

        avg_score = sum(scores) / len(scores) if scores else 0
        logger.info(f"ğŸ“Š [RelevanceCheck] å¹³å‡ç›¸å…³æ€§: {avg_score:.2f}, ä½ç›¸å…³é—®é¢˜: {len(low_relevance)}/{len(questions)}")

        return avg_score, low_relevance


class QuestionRelevanceValidator:
    """
    é—®é¢˜ç›¸å…³æ€§éªŒè¯å™¨

    ä½¿ç”¨LLMè¯„ä¼°æ¯ä¸ªé—®é¢˜ä¸ç”¨æˆ·åŸå§‹è¾“å…¥çš„ç›¸å…³æ€§ï¼Œ
    è¿‡æ»¤æ‰ç›¸å…³æ€§ä½çš„é—®é¢˜ã€‚
    """

    @classmethod
    def validate(
        cls,
        questions: List[Dict[str, Any]],
        user_input: str,
        llm_model: Optional[Any] = None,
        threshold: float = 6.0
    ) -> List[Dict[str, Any]]:
        """
        éªŒè¯é—®é¢˜ç›¸å…³æ€§ï¼Œè¿‡æ»¤ä½ç›¸å…³æ€§é—®é¢˜

        Args:
            questions: é—®é¢˜åˆ—è¡¨
            user_input: ç”¨æˆ·åŸå§‹è¾“å…¥
            llm_model: LLMæ¨¡å‹å®ä¾‹
            threshold: ç›¸å…³æ€§é˜ˆå€¼ï¼ˆ0-10ï¼‰ï¼Œä½äºæ­¤å€¼çš„é—®é¢˜å°†è¢«è¿‡æ»¤

        Returns:
            è¿‡æ»¤åçš„é—®é¢˜åˆ—è¡¨
        """
        if not questions:
            return []

        # å¦‚æœé—®é¢˜æ•°é‡è¾ƒå°‘ï¼Œè·³è¿‡éªŒè¯
        if len(questions) <= 5:
            logger.info("ğŸ“Š [RelevanceValidator] é—®é¢˜æ•°é‡â‰¤5ï¼Œè·³è¿‡ç›¸å…³æ€§éªŒè¯")
            return questions

        try:
            if llm_model is None:
                from ...services.llm_factory import LLMFactory
                llm_model = LLMFactory.create_llm(temperature=0.3)

            # æ„å»ºéªŒè¯æç¤ºè¯
            questions_text = "\n".join([
                f"{i+1}. [{q.get('id', f'q{i+1}')}] {q.get('question', '')}"
                for i, q in enumerate(questions)
            ])

            prompt = f"""è¯·è¯„ä¼°ä»¥ä¸‹é—®å·é—®é¢˜ä¸ç”¨æˆ·éœ€æ±‚çš„ç›¸å…³æ€§ã€‚

ç”¨æˆ·åŸå§‹è¾“å…¥ï¼š
{user_input}

é—®å·é—®é¢˜ï¼š
{questions_text}

è¯·ä¸ºæ¯ä¸ªé—®é¢˜æ‰“åˆ†ï¼ˆ0-10åˆ†ï¼‰ï¼Œè¿”å›JSONæ ¼å¼ï¼š
{{"scores": {{"é—®é¢˜ID": åˆ†æ•°, ...}}}}

è¯„åˆ†æ ‡å‡†ï¼š
- 10åˆ†ï¼šé—®é¢˜ç›´æ¥é’ˆå¯¹ç”¨æˆ·æåˆ°çš„å…·ä½“å†…å®¹
- 7-9åˆ†ï¼šé—®é¢˜ä¸ç”¨æˆ·éœ€æ±‚ç›¸å…³ï¼Œèƒ½å¤Ÿæä¾›æœ‰ä»·å€¼çš„ä¿¡æ¯
- 4-6åˆ†ï¼šé—®é¢˜æœ‰ä¸€å®šå…³è”ï¼Œä½†è¾ƒä¸ºé€šç”¨
- 0-3åˆ†ï¼šé—®é¢˜ä¸ç”¨æˆ·éœ€æ±‚å‡ ä¹æ— å…³

ç›´æ¥è¿”å›JSONï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚"""

            response = llm_model.invoke([{"role": "user", "content": prompt}])
            raw_content = response.content if hasattr(response, "content") else str(response)

            # è§£æè¯„åˆ†ç»“æœ
            scores = cls._parse_scores(raw_content)

            # è¿‡æ»¤ä½ç›¸å…³æ€§é—®é¢˜
            filtered = []
            for q in questions:
                q_id = q.get("id", "")
                score = scores.get(q_id, 7.0)  # é»˜è®¤7åˆ†ï¼ˆç›¸å…³ï¼‰

                if score >= threshold:
                    filtered.append(q)
                else:
                    logger.info(f"ğŸš« [RelevanceValidator] è¿‡æ»¤ä½ç›¸å…³æ€§é—®é¢˜: {q_id} (å¾—åˆ†: {score})")

            logger.info(f"ğŸ“Š [RelevanceValidator] ä¿ç•™ {len(filtered)}/{len(questions)} ä¸ªé—®é¢˜")
            return filtered

        except Exception as e:
            logger.warning(f"âš ï¸ [RelevanceValidator] éªŒè¯å¤±è´¥: {e}ï¼Œä¿ç•™æ‰€æœ‰é—®é¢˜")
            return questions

    @classmethod
    def _parse_scores(cls, raw_content: str) -> Dict[str, float]:
        """è§£æè¯„åˆ†ç»“æœ"""
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            content = raw_content.strip()
            if content.startswith("```"):
                content = content.split("\n", 1)[1] if "\n" in content else content
                if content.endswith("```"):
                    content = content[:-3]

            data = json.loads(content)
            return {str(k): float(v) for k, v in data.get("scores", {}).items()}

        except (json.JSONDecodeError, ValueError, TypeError):
            logger.warning("âš ï¸ [RelevanceValidator] æ— æ³•è§£æè¯„åˆ†ç»“æœ")
            return {}


# ============================================================
# ğŸ”§ v7.12: LLMQuestionGenerator è¾…åŠ©æ–¹æ³•ï¼ˆè¿½åŠ åˆ°ç±»å¤–éƒ¨ä½¿ç”¨ï¼‰
# ============================================================

# ä¸º LLMQuestionGenerator æ·»åŠ è¾…åŠ©æ–¹æ³•
def _extract_user_keywords_impl(user_input: str) -> List[str]:
    """
    ğŸ”§ v7.12: ä»ç”¨æˆ·è¾“å…¥ä¸­æå–å…³é”®è¯ï¼Œç”¨äºå¼ºåŒ–é—®å·ç”Ÿæˆ

    Args:
        user_input: ç”¨æˆ·åŸå§‹è¾“å…¥

    Returns:
        å…³é”®è¯åˆ—è¡¨ï¼ˆä¼˜å…ˆè¿”å›å…·ä½“çš„åè¯ã€æ•°å­—ã€ä¸“æœ‰åè¯ï¼‰
    """
    if not user_input:
        return []
    
    keywords = []
    
    # 1. æå–æ•°å­—+å•ä½ï¼ˆå¦‚ 200ã¡ã€38å²ã€3æˆ¿ï¼‰
    import re
    num_patterns = re.findall(r'\d+[\u4e00-\u9fa5ã¡a-zA-Z]+', user_input)
    keywords.extend(num_patterns)
    
    # 2. æå–å¼•å·å†…å®¹ï¼ˆç”¨æˆ·å¼ºè°ƒçš„å†…å®¹ï¼‰
    quoted = re.findall(r'[""ã€Œã€ã€ã€ã€ã€‘]([^""ã€Œã€ã€ã€ã€ã€‘]+)[""ã€Œã€ã€ã€ã€ã€‘]', user_input)
    keywords.extend(quoted)
    
    # 3. æå–ä¸“æœ‰åè¯ï¼ˆè¿ç»­ä¸­æ–‡ï¼Œé•¿åº¦2-8ï¼‰
    stopwords = {
        "çš„", "æ˜¯", "åœ¨", "æœ‰", "æˆ‘", "ä½ ", "ä»–", "å¥¹", "å®ƒ", "ä»¬",
        "è¿™", "é‚£", "å’Œ", "ä¸", "æˆ–", "ä½†", "è€Œ", "äº†", "ç€", "è¿‡",
        "éœ€è¦", "å¸Œæœ›", "æƒ³è¦", "ä¸€ä¸ª", "ä¸€äº›", "è¿™ä¸ª", "é‚£ä¸ª",
        "å¦‚ä½•", "æ€ä¹ˆ", "ä»€ä¹ˆ", "å“ªäº›", "ä¸ºä»€ä¹ˆ", "è¯·", "å¸®",
        "è¿›è¡Œ", "å®ç°", "å®Œæˆ", "è€ƒè™‘", "åŒ…æ‹¬", "é€šè¿‡", "ä½¿ç”¨",
        "è®¾è®¡", "é¡¹ç›®", "æ–¹æ¡ˆ", "å»ºè®®", "å¸Œæœ›", "èƒ½å¤Ÿ", "å¯ä»¥"
    }
    
    chinese_words = re.findall(r'[\u4e00-\u9fa5]{2,8}', user_input)
    for word in chinese_words:
        if word not in stopwords and word not in keywords:
            keywords.append(word)
    
    # 4. å»é‡å¹¶é™åˆ¶æ•°é‡
    unique_keywords = list(dict.fromkeys(keywords))  # ä¿æŒé¡ºåºå»é‡
    return unique_keywords[:15]  # æœ€å¤š15ä¸ªå…³é”®è¯


def _regenerate_with_keywords_impl(
    user_input: str,
    analysis_summary: str,
    keywords: List[str],
    llm_model
) -> Tuple[List[Dict[str, Any]], str]:
    """
    ğŸ”§ v7.12: ä½¿ç”¨å¼ºåŒ–å…³é”®è¯çº¦æŸé‡æ–°ç”Ÿæˆé—®å·

    Args:
        user_input: ç”¨æˆ·åŸå§‹è¾“å…¥
        analysis_summary: åˆ†ææ‘˜è¦
        keywords: ç”¨æˆ·å…³é”®è¯åˆ—è¡¨
        llm_model: LLMæ¨¡å‹å®ä¾‹

    Returns:
        Tuple[é—®é¢˜åˆ—è¡¨, æ¥æºæ ‡è¯†]
    """
    keywords_str = "ã€".join(keywords[:10])
    
    reinforced_prompt = f"""è¯·åŸºäºä»¥ä¸‹ç”¨æˆ·éœ€æ±‚ç”Ÿæˆé’ˆå¯¹æ€§é—®å·ã€‚

âš ï¸ é‡è¦çº¦æŸï¼šæ¯ä¸ªé—®é¢˜å¿…é¡»è‡³å°‘åŒ…å«ä»¥ä¸‹å…³é”®è¯ä¹‹ä¸€ï¼š
ã€{keywords_str}ã€‘

ç”¨æˆ·åŸå§‹éœ€æ±‚ï¼š
{user_input}

éœ€æ±‚åˆ†ææ‘˜è¦ï¼š
{analysis_summary}

ç”Ÿæˆè¦æ±‚ï¼š
1. æ¯ä¸ªé—®é¢˜å¿…é¡»å¼•ç”¨ç”¨æˆ·åŸè¯ä¸­çš„å…·ä½“å†…å®¹ï¼ˆæ•°å­—ã€åœ°ç‚¹ã€äººç‰©ã€ç‰¹å¾ï¼‰
2. ç¦æ­¢ç”Ÿæˆ"æ‚¨å¯¹...æœ‰ä»€ä¹ˆæƒ³æ³•ï¼Ÿ"ç­‰æ³›åŒ–é—®é¢˜
3. é—®é¢˜é€‰é¡¹å¿…é¡»å›´ç»•ç”¨æˆ·æåˆ°çš„å…·ä½“çº¦æŸå±•å¼€

è¿”å›JSONæ ¼å¼ï¼š
{{"questions": [
    {{"id": "q1", "question": "...", "options": ["é€‰é¡¹A", "é€‰é¡¹B", "é€‰é¡¹C", "é€‰é¡¹D"]}}
]}}"""

    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„ç”¨æˆ·éœ€æ±‚è°ƒç ”ä¸“å®¶ï¼Œæ“…é•¿ç”Ÿæˆé«˜åº¦é’ˆå¯¹æ€§çš„é—®å·é—®é¢˜ã€‚"},
        {"role": "user", "content": reinforced_prompt}
    ]
    
    response = llm_model.invoke(messages)
    raw_content = response.content if hasattr(response, "content") else str(response)
    
    # è§£æå“åº”
    questionnaire_data = LLMQuestionGenerator._parse_llm_response(raw_content)
    questions = questionnaire_data.get("questions", [])
    
    if questions:
        validated = LLMQuestionGenerator._validate_and_fix_questions(questions)
        return validated, "llm_regenerated"
    
    return [], "regeneration_failed"


# å°†æ–¹æ³•ç»‘å®šåˆ° LLMQuestionGenerator ç±»
LLMQuestionGenerator._extract_user_keywords = classmethod(lambda cls, user_input: _extract_user_keywords_impl(user_input))
LLMQuestionGenerator._regenerate_with_keywords = classmethod(
    lambda cls, user_input, analysis_summary, keywords, llm_model: 
    _regenerate_with_keywords_impl(user_input, analysis_summary, keywords, llm_model)
)
