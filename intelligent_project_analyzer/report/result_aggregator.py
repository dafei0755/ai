"""
ç»“æœèšåˆå™¨

è´Ÿè´£æ•´åˆæ‰€æœ‰æ™ºèƒ½ä½“çš„åˆ†æç»“æœï¼Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šç»“æ„
"""

import json
from typing import Dict, List, Optional, Any, Literal
import time
from datetime import datetime
from loguru import logger

from pydantic import BaseModel, Field, ConfigDict, field_validator
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.runnables import RunnableConfig
from langgraph.store.base import BaseStore

from ..agents.base import LLMAgent
from ..core.state import ProjectAnalysisState, AgentType, AnalysisStage
from ..core.types import AnalysisResult, ReportSection
from ..core.prompt_manager import PromptManager
from ..utils.jtbd_parser import transform_jtbd_to_natural_language


# ============================================================================
# Pydantic æ¨¡å‹å®šä¹‰ - ç”¨äºç»“æ„åŒ–è¾“å‡º
# ============================================================================

class ExecutiveSummary(BaseModel):
    """
    æ‰§è¡Œæ‘˜è¦

    âœ… é…ç½® extra='forbid' ä»¥æ”¯æŒ OpenAI Structured Outputs çš„ strict mode
    âœ… ç§»é™¤æ‰€æœ‰é»˜è®¤å€¼,ä½¿å­—æ®µæˆä¸ºå¿…å¡«é¡¹
    """
    model_config = ConfigDict(extra='forbid')

    project_overview: str = Field(description="é¡¹ç›®æ¦‚è¿°")
    key_findings: List[str] = Field(description="å…³é”®å‘ç°åˆ—è¡¨")
    key_recommendations: List[str] = Field(description="æ ¸å¿ƒå»ºè®®åˆ—è¡¨")
    success_factors: List[str] = Field(description="æˆåŠŸè¦ç´ åˆ—è¡¨")


# ğŸ”¥ Phase 1.4+ P4: æ ¸å¿ƒç­”æ¡ˆæ¨¡å‹
class CoreAnswer(BaseModel):
    """
    æ ¸å¿ƒç­”æ¡ˆ - ç”¨æˆ·æœ€å…³å¿ƒçš„TL;DRä¿¡æ¯
    """
    model_config = ConfigDict(extra='forbid')

    question: str = Field(description="ä»ç”¨æˆ·è¾“å…¥æå–çš„æ ¸å¿ƒé—®é¢˜")
    answer: str = Field(description="ç›´æ¥æ˜äº†çš„æ ¸å¿ƒç­”æ¡ˆï¼ˆ1-2å¥è¯ï¼‰")
    deliverables: List[str] = Field(description="äº¤ä»˜ç‰©æ¸…å•")
    timeline: str = Field(description="é¢„ä¼°æ—¶é—´çº¿")
    budget_range: str = Field(description="é¢„ç®—ä¼°ç®—èŒƒå›´")


# ğŸ†• v7.0: å•ä¸ªäº¤ä»˜ç‰©çš„è´£ä»»è€…ç­”æ¡ˆ
class DeliverableAnswer(BaseModel):
    """
    å•ä¸ªäº¤ä»˜ç‰©çš„è´£ä»»è€…ç­”æ¡ˆ - ç›´æ¥ä» owner ä¸“å®¶çš„è¾“å‡ºæå–ï¼Œä¸åš LLM äºŒæ¬¡ç»¼åˆ
    """
    model_config = ConfigDict(extra='allow')  # å…è®¸é¢å¤–å­—æ®µä»¥å…¼å®¹æ‰©å±•

    deliverable_id: str = Field(description="äº¤ä»˜ç‰©ID (å¦‚ D1, D2)")
    deliverable_name: str = Field(description="äº¤ä»˜ç‰©åç§°/æè¿°")
    deliverable_type: str = Field(default="unknown", description="äº¤ä»˜ç‰©ç±»å‹")
    owner_role: str = Field(description="è´£ä»»è€…è§’è‰²ID")
    owner_answer: str = Field(description="è´£ä»»è€…çš„æ ¸å¿ƒç­”æ¡ˆï¼ˆç›´æ¥æå–ï¼Œä¸ç»¼åˆï¼‰")
    answer_summary: str = Field(default="", description="ç­”æ¡ˆæ‘˜è¦ï¼ˆ200å­—ä»¥å†…ï¼‰")
    supporters: List[str] = Field(default_factory=list, description="æ”¯æ’‘ä¸“å®¶åˆ—è¡¨")
    quality_score: Optional[float] = Field(default=None, description="è´¨é‡åˆ†æ•°ï¼ˆ0-100ï¼‰")

    # ğŸ†• v7.108: å…³è”çš„æ¦‚å¿µå›¾
    concept_images: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="è¯¥äº¤ä»˜ç‰©å…³è”çš„æ¦‚å¿µå›¾åˆ—è¡¨ï¼ˆImageMetadataæ ¼å¼ï¼‰"
    )


# ğŸ†• v7.0: ä¸“å®¶æ”¯æ’‘é“¾
class ExpertSupportChain(BaseModel):
    """
    ä¸“å®¶æ”¯æ’‘é“¾ - å±•ç¤ºé owner ä¸“å®¶çš„è´¡çŒ®
    """
    model_config = ConfigDict(extra='allow')
    
    role_id: str = Field(description="ä¸“å®¶è§’è‰²ID")
    role_name: str = Field(default="", description="ä¸“å®¶åç§°")
    contribution_type: str = Field(default="support", description="è´¡çŒ®ç±»å‹")
    contribution_summary: str = Field(default="", description="è´¡çŒ®æ‘˜è¦")
    related_deliverables: List[str] = Field(default_factory=list, description="å…³è”çš„äº¤ä»˜ç‰©IDåˆ—è¡¨")


# ğŸ†• v7.0: å¢å¼ºç‰ˆæ ¸å¿ƒç­”æ¡ˆï¼ˆæ”¯æŒå¤šäº¤ä»˜ç‰©ï¼‰
class CoreAnswerV7(BaseModel):
    """
    v7.0 å¢å¼ºç‰ˆæ ¸å¿ƒç­”æ¡ˆ - æ”¯æŒå¤šä¸ªäº¤ä»˜ç‰©ï¼Œæ¯ä¸ªäº¤ä»˜ç‰©æœ‰ç‹¬ç«‹çš„è´£ä»»è€…ç­”æ¡ˆ
    
    æ ¸å¿ƒç†å¿µï¼š
    - æ ¸å¿ƒç­”æ¡ˆ = å„è´£ä»»è€…çš„æœ€ç»ˆäº¤ä»˜ï¼ˆä¸åšLLMç»¼åˆï¼‰
    - æ¯ä¸ªäº¤ä»˜ç‰©æœ‰ä¸€ä¸ª primary_ownerï¼Œå…¶è¾“å‡ºå³ä¸ºè¯¥äº¤ä»˜ç‰©çš„ç­”æ¡ˆ
    - ä¸“å®¶æ”¯æ’‘é“¾å±•ç¤ºé owner ä¸“å®¶çš„è´¡çŒ®
    """
    model_config = ConfigDict(extra='allow')
    
    question: str = Field(description="ä»ç”¨æˆ·è¾“å…¥æå–çš„æ ¸å¿ƒé—®é¢˜")
    deliverable_answers: List[DeliverableAnswer] = Field(
        default_factory=list, 
        description="å„äº¤ä»˜ç‰©çš„è´£ä»»è€…ç­”æ¡ˆåˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰"
    )
    expert_support_chain: List[ExpertSupportChain] = Field(
        default_factory=list,
        description="ä¸“å®¶æ”¯æ’‘é“¾ï¼ˆéownerä¸“å®¶çš„è´¡çŒ®ï¼‰"
    )
    timeline: str = Field(default="å¾…å®š", description="é¢„ä¼°æ—¶é—´çº¿")
    budget_range: str = Field(default="å¾…å®š", description="é¢„ç®—ä¼°ç®—èŒƒå›´")
    
    # å‘åå…¼å®¹å­—æ®µ
    answer: str = Field(default="", description="ç»¼åˆæ‘˜è¦ï¼ˆå‘åå…¼å®¹ï¼‰")
    deliverables: List[str] = Field(default_factory=list, description="äº¤ä»˜ç‰©æ¸…å•ï¼ˆå‘åå…¼å®¹ï¼‰")


# ğŸ”¥ æ–°å¢ï¼šæ´å¯ŸåŒºå—
class InsightsSection(BaseModel):
    """
    æ´å¯Ÿ - ä»æ‰€æœ‰ä¸“å®¶åˆ†æä¸­æç‚¼çš„å…³é”®æ´å¯Ÿ
    """
    model_config = ConfigDict(extra='forbid')

    key_insights: List[str] = Field(description="3-5æ¡æ ¸å¿ƒæ´å¯Ÿï¼Œæ¯æ¡1-2å¥è¯")
    cross_domain_connections: List[str] = Field(description="è·¨é¢†åŸŸå…³è”å‘ç°ï¼ˆå¦‚è®¾è®¡ä¸å•†ä¸šçš„å…³è”ï¼‰")
    user_needs_interpretation: str = Field(description="å¯¹ç”¨æˆ·éœ€æ±‚çš„æ·±å±‚è§£è¯»")


# ğŸ”¥ æ–°å¢ï¼šæ¨æ•²è¿‡ç¨‹åŒºå—
class DeliberationProcess(BaseModel):
    """
    æ¨æ•²è¿‡ç¨‹ - é¡¹ç›®æ€»ç›‘çš„æˆ˜ç•¥åˆ†æå’Œå†³ç­–æ€è·¯
    """
    model_config = ConfigDict(extra='forbid')

    inquiry_architecture: str = Field(description="é€‰æ‹©çš„æ¢è¯¢æ¶æ„ç±»å‹")
    reasoning: str = Field(description="ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªæ¢è¯¢æ¶æ„ï¼ˆ2-3å¥è¯ï¼‰")
    role_selection: List[str] = Field(description="é€‰æ‹©çš„ä¸“å®¶è§’è‰²åŠç†ç”±")
    strategic_approach: str = Field(description="æ•´ä½“æˆ˜ç•¥æ–¹å‘ï¼ˆ3-5å¥è¯ï¼‰")


# ğŸ”¥ æ–°å¢ï¼šå»ºè®®åŒºå—ï¼ˆV2å‡çº§ - äº”ç»´åº¦åˆ†ç±»ï¼‰
class RecommendationItem(BaseModel):
    """å•æ¡å»ºè®®"""
    model_config = ConfigDict(extra='forbid')

    content: str = Field(description="å»ºè®®å†…å®¹ï¼ˆ50-150å­—ï¼‰")

    dimension: Literal["critical", "difficult", "overlooked", "risky", "ideal"] = Field(
        description="å»ºè®®ç»´åº¦ï¼šcritical=é‡ç‚¹, difficult=éš¾ç‚¹, overlooked=æ˜“å¿½ç•¥, risky=æœ‰é£é™©, ideal=ç†æƒ³"
    )

    reasoning: str = Field(description="ä¸ºä»€ä¹ˆå±äºè¿™ä¸ªç»´åº¦ï¼ˆ1-2å¥è¯ï¼‰")

    source_expert: str = Field(description="å»ºè®®æ¥æºä¸“å®¶ï¼ˆå¦‚ V2_è®¾è®¡æ€»ç›‘_2-2ï¼‰")

    estimated_effort: Optional[str] = Field(
        default=None,
        description="é¢„ä¼°å·¥ä½œé‡ï¼ˆå¦‚'2-3å¤©'ã€'1å‘¨'ã€'éœ€ä¸“ä¸šå›¢é˜Ÿ'ï¼‰"
    )

    dependencies: List[str] = Field(
        default=[],
        description="ä¾èµ–çš„å…¶ä»–å»ºè®®å†…å®¹ï¼ˆç”¨äºæ’åºï¼‰"
    )


class RecommendationsSection(BaseModel):
    """
    å»ºè®®æé†’åŒºå—ï¼ˆV2å‡çº§ - äº”ç»´åº¦åˆ†ç±»ï¼‰

    ç»´åº¦è¯´æ˜ï¼š
    - criticalï¼ˆé‡ç‚¹ï¼‰: é¡¹ç›®æ ¸å¿ƒå·¥ä½œï¼Œå¿…é¡»å®Œæˆ
    - difficultï¼ˆéš¾ç‚¹ï¼‰: æŠ€æœ¯éš¾åº¦é«˜ï¼Œéœ€è¦é‡ç‚¹æ”»å…‹
    - overlookedï¼ˆæ˜“å¿½ç•¥ï¼‰: å®¹æ˜“è¢«é—æ¼ä½†å¾ˆé‡è¦
    - riskyï¼ˆæœ‰é£é™©ï¼‰: ä¸åšä¼šå‡ºé—®é¢˜
    - idealï¼ˆç†æƒ³ï¼‰: é”¦ä¸Šæ·»èŠ±ï¼Œæœ‰ä½™åŠ›å†åš
    """
    model_config = ConfigDict(extra='forbid')

    recommendations: List[RecommendationItem] = Field(
        description="æ‰€æœ‰å»ºè®®åˆ—è¡¨ï¼ˆæŒ‰ç»´åº¦åˆ†ç±»ï¼‰"
    )

    summary: str = Field(
        description="å»ºè®®æ€»è§ˆï¼ˆ2-3å¥è¯æ¦‚æ‹¬æ ¸å¿ƒè¦ç‚¹ï¼‰"
    )


# ============================================================================
# ä¸“å®¶å†…å®¹æ¨¡å‹å®šä¹‰ - æ›¿æ¢Dict[str, Any]
# ============================================================================

class V2DesignDirectorContent(BaseModel):
    """V2è®¾è®¡æ€»ç›‘çš„å†…å®¹ç»“æ„"""
    model_config = ConfigDict(extra='forbid')

    spatial_concept: str = Field(default="", description="ç©ºé—´æ¦‚å¿µä¸å¤§åˆ›æ„")
    narrative_translation: str = Field(default="", description="ç©ºé—´å™äº‹è½¬è¯‘æ–¹æ¡ˆ")
    aesthetic_framework: str = Field(default="", description="ç¾å­¦æ¡†æ¶å®šä¹‰")
    design_language: str = Field(default="", description="è®¾è®¡è¯­è¨€ä½“ç³»")
    functional_planning: str = Field(default="", description="åŠŸèƒ½å¸ƒå±€ä¸åŠ¨çº¿è§„åˆ’")
    sensory_experience: str = Field(default="", description="æ„Ÿå®˜ä½“éªŒè®¾è®¡")
    material_palette: str = Field(default="", description="æè´¨ä¸è‰²å½©æ–¹æ¡ˆ")
    key_visuals: str = Field(default="", description="å…³é”®è§†è§‰ç”»é¢æè¿°")
    implementation_guidance: str = Field(default="", description="å®æ–½æŒ‡å¯¼å»ºè®®")


class V3NarrativeExpertContent(BaseModel):
    """V3äººç‰©åŠå™äº‹ä¸“å®¶çš„å†…å®¹ç»“æ„"""
    model_config = ConfigDict(extra='forbid')

    character_archetype: str = Field(default="", description="æ ¸å¿ƒäººç‰©åŸå‹å®šä¹‰")
    narrative_worldview: str = Field(default="", description="å™äº‹ä¸–ç•Œè§‚æ„å»º")
    core_theme: str = Field(default="", description="æ ¸å¿ƒæ¯é¢˜ä¸ç²¾ç¥å†…æ ¸")
    emotional_journey: str = Field(default="", description="æƒ…æ„Ÿæ—…ç¨‹åœ°å›¾")
    scene_storyboard: str = Field(default="", description="å…³é”®åœºæ™¯åˆ†é•œè„šæœ¬")
    sensory_script: str = Field(default="", description="äº”æ„Ÿè®¾è®¡è„šæœ¬")
    symbolic_elements: str = Field(default="", description="æ„è±¡ä¸ç¬¦å·ä½“ç³»")
    experience_choreography: str = Field(default="", description="ä½“éªŒç¼–æ’æ–¹æ¡ˆ")
    narrative_guidelines: str = Field(default="", description="å™äº‹æŒ‡å¯¼åŸåˆ™")


class V4DesignResearcherContent(BaseModel):
    """V4è®¾è®¡ç ”ç©¶å‘˜çš„å†…å®¹ç»“æ„"""
    model_config = ConfigDict(extra='forbid')

    case_studies: str = Field(default="", description="ç›¸å…³æ¡ˆä¾‹ç ”ç©¶ä¸åˆ†æ")
    design_patterns: str = Field(default="", description="è¯†åˆ«çš„è®¾è®¡æ¨¡å¼ä¸è§„å¾‹")
    methodology_insights: str = Field(default="", description="æ–¹æ³•è®ºè§£æ„ä¸æç‚¼")
    knowledge_framework: str = Field(default="", description="çŸ¥è¯†æ¡†æ¶æ„å»º")
    trend_analysis: str = Field(default="", description="å‰æ²¿è¶‹åŠ¿åˆ†æ")
    best_practices: str = Field(default="", description="æœ€ä½³å®è·µæ€»ç»“")
    theoretical_foundation: str = Field(default="", description="ç†è®ºåŸºç¡€æ”¯æ’‘")
    reference_materials: str = Field(default="", description="å‚è€ƒèµ„æ–™ä¸æ–‡çŒ®")
    application_guidelines: str = Field(default="", description="åº”ç”¨æŒ‡å¯¼å»ºè®®")


class V5ScenarioExpertContent(BaseModel):
    """V5åœºæ™¯ä¸ç”¨æˆ·ç”Ÿæ€ä¸“å®¶çš„å†…å®¹ç»“æ„"""
    model_config = ConfigDict(extra='forbid')

    context_analysis: str = Field(default="", description="æƒ…å¢ƒåˆ†æ(B2B/ä½å®…/B2C)")
    user_ecosystem: str = Field(default="", description="ç”¨æˆ·ç”Ÿæ€ç³»ç»Ÿæµ‹ç»˜")
    behavioral_patterns: str = Field(default="", description="è¡Œä¸ºæ¨¡å¼åˆ†æ")
    life_script: str = Field(default="", description="ç”Ÿæ´»å‰§æœ¬/æ¶ˆè´¹è·¯å¾„")
    trend_insights: str = Field(default="", description="æƒ…å¢ƒåŒ–è¶‹åŠ¿æ´å¯Ÿ")
    core_value_proposition: str = Field(default="", description="æ ¸å¿ƒä»·å€¼ä¸»å¼ ")
    design_challenges: str = Field(default="", description="è®¾è®¡æŒ‘æˆ˜å®šä¹‰")
    stakeholder_analysis: str = Field(default="", description="åˆ©ç›Šç›¸å…³æ–¹åˆ†æ")
    success_metrics: str = Field(default="", description="æˆåŠŸæŒ‡æ ‡å®šä¹‰")


class V6ChiefEngineerContent(BaseModel):
    """V6ä¸“ä¸šæ€»å·¥ç¨‹å¸ˆçš„å†…å®¹ç»“æ„"""
    model_config = ConfigDict(extra='forbid')

    feasibility_assessment: str = Field(default="", description="æŠ€æœ¯å¯è¡Œæ€§è¯„ä¼°")
    risk_analysis: str = Field(default="", description="é£é™©è¯†åˆ«ä¸åº”å¯¹")
    system_integration: str = Field(default="", description="å¤šç³»ç»Ÿé›†æˆæ–¹æ¡ˆ")
    material_specifications: str = Field(default="", description="ææ–™é€‰æ‹©ä¸è§„æ ¼")
    construction_technology: str = Field(default="", description="å»ºé€ å·¥è‰ºæ–¹æ¡ˆ")
    cost_estimation: str = Field(default="", description="æˆæœ¬ä¼°ç®—ä¸æ§åˆ¶")
    compliance_requirements: str = Field(default="", description="æ³•è§„åˆè§„è¦æ±‚")
    lifecycle_analysis: str = Field(default="", description="å…¨ç”Ÿå‘½å‘¨æœŸåˆ†æ")
    implementation_roadmap: str = Field(default="", description="å®æ–½è·¯çº¿å›¾")


# ============================================================================
# æŠ¥å‘Šç« èŠ‚æ•°æ®æ¨¡å‹
# ============================================================================

class ReportSectionWithId(BaseModel):
    """
    æŠ¥å‘Šç« èŠ‚æ•°æ® - åŒ…å«section_idå­—æ®µ

    âœ… ä½¿ç”¨List[ReportSectionWithId]æ›¿ä»£Dict[str, ReportSectionData]
    âœ… è§£å†³OpenAI Structured Outputså¯¹åŠ¨æ€é”®å­—å…¸æ”¯æŒä¸ä½³çš„é—®é¢˜
    âœ… é…ç½® extra='forbid' ä»¥æ”¯æŒ OpenAI Structured Outputs çš„ strict mode
    """
    model_config = ConfigDict(extra='forbid')

    section_id: str = Field(description="ç« èŠ‚ID,å¦‚design_research, technical_architectureç­‰")
    title: str = Field(description="ç« èŠ‚æ ‡é¢˜")
    content: str = Field(description="ç« èŠ‚å†…å®¹,åŒ…å«è¯¥é¢†åŸŸçš„è¯¦ç»†åˆ†æ(ä½¿ç”¨å­—ç¬¦ä¸²æ ¼å¼ä»¥ç¡®ä¿LLMèƒ½å¤Ÿæ­£ç¡®è¿”å›)")
    confidence: float = Field(
        description="åˆ†æç½®ä¿¡åº¦,0-1ä¹‹é—´",
        ge=0,
        le=1
    )


class ComprehensiveAnalysis(BaseModel):
    """
    ç»¼åˆåˆ†æ

    âœ… é…ç½® extra='forbid' ä»¥æ”¯æŒ OpenAI Structured Outputs çš„ strict mode
    âœ… ç§»é™¤æ‰€æœ‰é»˜è®¤å€¼,ä½¿å­—æ®µæˆä¸ºå¿…å¡«é¡¹
    """
    model_config = ConfigDict(extra='forbid')

    cross_domain_insights: List[str] = Field(description="è·¨é¢†åŸŸæ´å¯Ÿ")
    integrated_recommendations: List[str] = Field(description="æ•´åˆå»ºè®®")
    risk_assessment: List[str] = Field(description="é£é™©è¯„ä¼°")
    implementation_roadmap: List[str] = Field(description="å®æ–½è·¯çº¿å›¾")


class Conclusions(BaseModel):
    """
    ç»“è®ºå’Œå»ºè®®

    âœ… é…ç½® extra='forbid' ä»¥æ”¯æŒ OpenAI Structured Outputs çš„ strict mode
    âœ… ç§»é™¤æ‰€æœ‰é»˜è®¤å€¼,ä½¿å­—æ®µæˆä¸ºå¿…å¡«é¡¹
    """
    model_config = ConfigDict(extra='forbid')

    project_analysis_summary: str = Field(description="é¡¹ç›®åˆ†ææ€»ç»“")
    next_steps: List[str] = Field(description="ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®")
    success_metrics: List[str] = Field(description="æˆåŠŸæŒ‡æ ‡")


class ReviewFeedbackItem(BaseModel):
    """å•ä¸ªå®¡æ ¸åé¦ˆé¡¹"""
    model_config = ConfigDict(extra='forbid')
    
    issue_id: str = Field(description="é—®é¢˜IDï¼ˆå¦‚R1, R2, B1ç­‰ï¼‰")
    reviewer: str = Field(description="å®¡æ ¸ä¸“å®¶ï¼ˆçº¢é˜Ÿ/è“é˜Ÿ/è¯„å§”/ç”²æ–¹ï¼‰")
    issue_type: str = Field(description="é—®é¢˜ç±»å‹ï¼ˆé£é™©/ä¼˜åŒ–/å»ºè®®ï¼‰")
    description: str = Field(description="é—®é¢˜æè¿°")
    response: str = Field(description="å¦‚ä½•å“åº”ï¼ˆé‡‡å–çš„æ”¹è¿›æªæ–½ï¼‰")
    status: str = Field(description="çŠ¶æ€ï¼ˆå·²ä¿®å¤/è¿›è¡Œä¸­/å¾…å®šï¼‰")
    priority: str = Field(description="ä¼˜å…ˆçº§ï¼ˆhigh/medium/lowï¼‰")
    
    @field_validator('priority', mode='before')
    @classmethod
    def ensure_priority_is_string(cls, v):
        """ç¡®ä¿ priority å­—æ®µæ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼ˆå…¼å®¹ int è¾“å…¥ï¼‰"""
        if isinstance(v, int):
            # å°†æ•°å­—ä¼˜å…ˆçº§æ˜ å°„ä¸ºå­—ç¬¦ä¸²
            priority_map = {1: "high", 2: "medium", 3: "low"}
            return priority_map.get(v, "medium")
        return str(v) if v is not None else "medium"


class ReviewFeedback(BaseModel):
    """å®¡æ ¸åé¦ˆç« èŠ‚"""
    model_config = ConfigDict(extra='forbid')
    
    red_team_challenges: List[ReviewFeedbackItem] = Field(description="çº¢é˜Ÿè´¨ç–‘ç‚¹åˆ—è¡¨")
    blue_team_validations: List[ReviewFeedbackItem] = Field(description="è“é˜ŸéªŒè¯ç»“æœ")
    judge_rulings: List[ReviewFeedbackItem] = Field(description="è¯„å§”è£å†³è¦ç‚¹")
    client_decisions: List[ReviewFeedbackItem] = Field(description="ç”²æ–¹æœ€ç»ˆå†³ç­–")
    iteration_summary: str = Field(description="è¿­ä»£æ”¹è¿›è¿‡ç¨‹æ€»ç»“")


class QuestionnaireResponse(BaseModel):
    """å•ä¸ªé—®å·é—®é¢˜çš„å›ç­”"""
    model_config = ConfigDict(extra='forbid')
    
    question_id: str = Field(description="é—®é¢˜ID")
    question: str = Field(description="é—®é¢˜å†…å®¹")
    answer: str = Field(description="ç”¨æˆ·å›ç­”")
    context: str = Field(description="é—®é¢˜ä¸Šä¸‹æ–‡è¯´æ˜")


class QuestionnaireResponses(BaseModel):
    """ç”¨æˆ·è®¿è°ˆè®°å½•ï¼ˆæ ¡å‡†é—®å·å›ç­”ï¼‰"""
    model_config = ConfigDict(extra='forbid')
    
    responses: List[QuestionnaireResponse] = Field(description="é—®å·å›ç­”åˆ—è¡¨")
    timestamp: str = Field(description="æäº¤æ—¶é—´")
    analysis_insights: str = Field(description="ä»å›ç­”ä¸­æå–çš„å…³é”®æ´å¯Ÿ")


class ReviewRoundData(BaseModel):
    """å•è½®å®¡æ ¸æ•°æ®"""
    model_config = ConfigDict(extra='forbid')
    
    round_number: int = Field(description="è½®æ¬¡ç¼–å·")
    red_score: int = Field(description="çº¢é˜Ÿè¯„åˆ†ï¼ˆ0-100ï¼‰")
    blue_score: int = Field(description="è“é˜Ÿè¯„åˆ†ï¼ˆ0-100ï¼‰")
    judge_score: int = Field(description="è¯„å§”è¯„åˆ†ï¼ˆ0-100ï¼‰")
    issues_found: int = Field(description="å‘ç°çš„é—®é¢˜æ•°é‡")
    issues_resolved: int = Field(description="è§£å†³çš„é—®é¢˜æ•°é‡")
    timestamp: str = Field(description="å®¡æ ¸æ—¶é—´")


class ReviewVisualization(BaseModel):
    """å¤šè½®å®¡æ ¸å¯è§†åŒ–æ•°æ®"""
    model_config = ConfigDict(extra='forbid')
    
    rounds: List[ReviewRoundData] = Field(description="å„è½®å®¡æ ¸æ•°æ®")
    final_decision: str = Field(description="æœ€ç»ˆå†³ç­–ï¼ˆé€šè¿‡/æœ‰æ¡ä»¶é€šè¿‡/æ‹’ç»ï¼‰")
    total_rounds: int = Field(description="æ€»å®¡æ ¸è½®æ¬¡")
    improvement_rate: float = Field(description="æ”¹è¿›ç‡ï¼ˆ0-1ä¹‹é—´ï¼‰")


class FinalReport(BaseModel):
    """
    ğŸ”¥ é‡æ„åçš„æŠ¥å‘Šç»“æ„ - å®Œå…¨åŠ¨æ€åŒ–ï¼Œæ— é¢„å®šä¹‰ç« èŠ‚

    æ–°çš„æŠ¥å‘Šç»“æ„ï¼š
    1. ç”¨æˆ·åŸå§‹è¾“å…¥ï¼ˆä»stateè·å–ï¼‰
    2. é—®å·ï¼ˆä»…å±•ç¤ºæœ‰ä¿®æ”¹çš„ï¼‰
    3. æ´å¯Ÿï¼ˆä»æ‰€æœ‰ä¸“å®¶åˆ†æä¸­æç‚¼ï¼‰
    4. ç­”æ¡ˆï¼ˆæ ¸å¿ƒç­”æ¡ˆTL;DRï¼‰
    5. æ¨æ•²è¿‡ç¨‹ï¼ˆé¡¹ç›®æ€»ç›‘çš„æˆ˜ç•¥åˆ†æ + è§’è‰²é€‰æ‹©ç†ç”±ï¼‰
    6. å„ä¸“å®¶çš„æŠ¥å‘Š
    7. å»ºè®®ï¼ˆæ•´åˆæ‰€æœ‰ä¸“å®¶çš„å»ºè®®ï¼‰

    âœ… é…ç½® extra='forbid' ä»¥æ”¯æŒ OpenAI Structured Outputs çš„ strict mode
    """
    model_config = ConfigDict(
        extra='forbid',
        populate_by_name=True
    )

    # ğŸ”¥ 3. æ´å¯Ÿï¼ˆå·²åºŸå¼ƒ - æ”¹ç”¨éœ€æ±‚åˆ†æå¸ˆçš„ structured_requirementsï¼‰
    insights: Optional[InsightsSection] = Field(
        default=None,
        description="[å·²åºŸå¼ƒ] ä»æ‰€æœ‰ä¸“å®¶åˆ†æä¸­æç‚¼çš„æ ¸å¿ƒæ´å¯Ÿï¼ˆä¸å†ä½¿ç”¨ï¼‰"
    )

    # ğŸ”¥ 4. ç­”æ¡ˆï¼ˆå¿…å¡«ï¼‰
    core_answer: CoreAnswer = Field(
        description="æ ¸å¿ƒç­”æ¡ˆï¼šç”¨æˆ·æœ€å…³å¿ƒçš„TL;DRä¿¡æ¯"
    )

    # ğŸ”¥ 5. æ¨æ•²è¿‡ç¨‹ï¼ˆå¿…å¡«ï¼‰
    deliberation_process: DeliberationProcess = Field(
        description="é¡¹ç›®æ€»ç›‘çš„æˆ˜ç•¥åˆ†æå’Œå†³ç­–æ€è·¯"
    )

    # ğŸ”¥ 6. å„ä¸“å®¶çš„æŠ¥å‘Šï¼ˆå¿…å¡«ï¼‰
    # ğŸ”§ v7.11: æ·»åŠ default_factoryé˜²æ­¢Noneå€¼å¼•èµ·éªŒè¯å¤±è´¥
    expert_reports: Dict[str, str] = Field(
        default_factory=dict,
        description="""
        ä¸“å®¶åŸå§‹æŠ¥å‘Šå­—å…¸ï¼Œå®Œæ•´å±•ç¤ºå„ä¸“å®¶çš„åˆ†æå†…å®¹

        æ ¼å¼ç¤ºä¾‹:
        {
            "V2_è®¾è®¡æ€»ç›‘_2-1": "å®Œæ•´çš„è®¾è®¡åˆ†æ...",
            "V3_äººç‰©åŠå™äº‹ä¸“å®¶_3-1": "å®Œæ•´çš„å™äº‹åˆ†æ...",
            ...
        }
        """
    )

    # ğŸ”¥ 7. å»ºè®®ï¼ˆå¿…å¡«ï¼‰
    recommendations: RecommendationsSection = Field(
        description="æ•´åˆæ‰€æœ‰ä¸“å®¶çš„å¯æ‰§è¡Œå»ºè®®"
    )

    # ğŸ”¥ 2. é—®å·ï¼ˆå¯é€‰ï¼Œä»…æœ‰ä¿®æ”¹æ—¶å¡«å……ï¼‰
    questionnaire_responses: Optional[QuestionnaireResponses] = Field(
        default=None,
        description="ç”¨æˆ·è®¿è°ˆè®°å½•ï¼ˆæ ¡å‡†é—®å·çš„å®Œæ•´å›ç­”ï¼Œä»…å±•ç¤ºæœ‰ä¿®æ”¹çš„é—®é¢˜ï¼‰"
    )

    # ğŸ”¥ å®¡æ ¸åé¦ˆï¼ˆå¯é€‰ï¼‰
    review_feedback: Optional[ReviewFeedback] = Field(
        default=None,
        description="å®¡æ ¸åé¦ˆç« èŠ‚ï¼ˆåŒ…å«çº¢é˜Ÿè´¨ç–‘ã€è“é˜ŸéªŒè¯ã€è¯„å§”è£å†³ã€è¿­ä»£æ”¹è¿›è¿‡ç¨‹ï¼‰"
    )

    # ğŸ”¥ å®¡æ ¸å¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰
    review_visualization: Optional[ReviewVisualization] = Field(
        default=None,
        description="å¤šè½®å®¡æ ¸å¯è§†åŒ–æ•°æ®ï¼ˆçº¢è“å¯¹æŠ—è¿‡ç¨‹çš„ç«åŠ›å›¾ï¼‰"
    )


class ResultAggregatorAgent(LLMAgent):
    """ç»“æœèšåˆå™¨æ™ºèƒ½ä½“"""
    
    def __init__(self, llm_model, config: Optional[Dict[str, Any]] = None):
        super().__init__(
            agent_type=AgentType.RESULT_AGGREGATOR,
            name="ç»“æœèšåˆå™¨",
            description="æ•´åˆæ‰€æœ‰åˆ†æç»“æœï¼Œç”Ÿæˆç»“æ„åŒ–çš„æœ€ç»ˆæŠ¥å‘Š",
            llm_model=llm_model,
            config=config
        )

        # åˆå§‹åŒ–æç¤ºè¯ç®¡ç†å™¨
        self.prompt_manager = PromptManager()
    
    def validate_input(self, state: ProjectAnalysisState) -> bool:
        """éªŒè¯è¾“å…¥æ˜¯å¦æœ‰æ•ˆ"""
        agent_results = state.get("agent_results") or {}  # ğŸ”¥ ä¿®å¤ï¼šç¡®ä¿ä¸ä¸º None
        return len(agent_results) > 0

    def prepare_messages(self, state: ProjectAnalysisState) -> List:
        """
        å‡†å¤‡LLMæ¶ˆæ¯ - é‡å†™çˆ¶ç±»æ–¹æ³•ä»¥ä½¿ç”¨æ­£ç¡®çš„æ¶ˆæ¯æ ¼å¼

        ğŸ”§ æ€§èƒ½ä¼˜åŒ–: ç§»é™¤few-shotç¤ºä¾‹ï¼Œç›´æ¥ä½¿ç”¨structured output
        âœ… å‡å°‘tokenæ¶ˆè€—çº¦60%ï¼ŒåŠ é€ŸLLMå“åº”
        """
        messages = []

        # âœ… ä½¿ç”¨ SystemMessage ä¼ é€’ç³»ç»Ÿæç¤ºè¯
        system_prompt = self.get_system_prompt()
        messages.append(SystemMessage(content=system_prompt))

        # ğŸ”§ æ€§èƒ½ä¼˜åŒ–: ç›´æ¥ä½¿ç”¨structured outputï¼Œæ— éœ€few-shotç¤ºä¾‹
        task_description = self.get_task_description(state)
        messages.append(HumanMessage(content=task_description))

        return messages
    
    def get_system_prompt(self) -> str:
        """
        è·å–ç³»ç»Ÿæç¤ºè¯ - ä»å¤–éƒ¨é…ç½®åŠ è½½

        âœ… ä¿®å¤: ç®€åŒ–æç¤ºè¯ï¼Œèšç„¦æ ¸å¿ƒä»»åŠ¡
        âœ… ä¼˜åŒ–: æ˜ç¡® sections å­—æ®µçš„å¡«å……è§„åˆ™
        """
        # å°è¯•ä»å¤–éƒ¨é…ç½®åŠ è½½
        prompt = self.prompt_manager.get_prompt("result_aggregator")

        # å¦‚æœé…ç½®ä¸å­˜åœ¨ï¼ŒæŠ›å‡ºé”™è¯¯ï¼ˆä¸å†ä½¿ç”¨ç¡¬ç¼–ç  fallbackï¼‰
        if not prompt:
            raise ValueError(
                "âŒ æœªæ‰¾åˆ°æç¤ºè¯é…ç½®: result_aggregator\n"
                "è¯·ç¡®ä¿é…ç½®æ–‡ä»¶å­˜åœ¨: config/prompts/result_aggregator.yaml\n"
                "ç³»ç»Ÿæ— æ³•ä½¿ç”¨ç¡¬ç¼–ç æç¤ºè¯ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶ã€‚"
            )

        return prompt
    
    def get_task_description(self, state: ProjectAnalysisState) -> str:
        """
        è·å–å…·ä½“ä»»åŠ¡æè¿° - v3.0ç‰ˆæœ¬ï¼ˆåŒ…å«å®¡æ ¸åé¦ˆã€é—®å·å›ç­”ã€å¯è§†åŒ–æ•°æ®ï¼‰

        âœ… æ–°å¢: review_feedbackï¼ˆå®¡æ ¸åé¦ˆç« èŠ‚ï¼‰
        âœ… æ–°å¢: questionnaire_responsesï¼ˆç”¨æˆ·è®¿è°ˆè®°å½•ï¼‰
        âœ… æ–°å¢: review_visualizationï¼ˆå¤šè½®å®¡æ ¸å¯è§†åŒ–ï¼‰
        """
        agent_results = state.get("agent_results") or {}
        structured_requirements = state.get("structured_requirements") or {}
        strategic_analysis = state.get("strategic_analysis") or {}  # ğŸ”¥ ä¿®å¤ï¼šç¡®ä¿ä¸ä¸º None
        
        # ğŸ†• è·å–å®¡æ ¸ç»“æœï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        final_ruling = state.get("final_ruling") or ""
        improvement_suggestions = state.get("improvement_suggestions") or []
        
        # ğŸ†• è·å–å®Œæ•´å®¡æ ¸æ•°æ®ï¼ˆæ–°å¢ï¼‰
        review_result = state.get("review_result") or {}  # ğŸ”¥ ä¿®å¤ï¼šç¡®ä¿ä¸ä¸º None
        review_history = state.get("review_history") or []  # ğŸ”¥ ä¿®å¤ï¼šç¡®ä¿ä¸ä¸º None
        
        # ğŸ†• è·å–é—®å·æ•°æ®ï¼ˆğŸ”§ ä¿®å¤: åŒæ—¶è·å– calibration_answers ä½œä¸ºå¤‡ç”¨ï¼‰
        calibration_questionnaire = state.get("calibration_questionnaire") or {}
        questionnaire_responses = state.get("questionnaire_responses") or {}
        questionnaire_summary = state.get("questionnaire_summary") or {}
        calibration_answers = state.get("calibration_answers") or {}
        
        # ğŸ”§ ä¿®å¤: å¦‚æœ questionnaire_responses ä¸ºç©ºï¼Œå°è¯•ä» calibration_answers æ„å»º
        if not questionnaire_responses.get("entries") and not questionnaire_responses.get("answers"):
            if calibration_answers:
                logger.info(f"ğŸ”§ [é—®å·æ•°æ®æ¢å¤] questionnaire_responses ä¸ºç©ºï¼Œä» calibration_answers æ¢å¤ ({len(calibration_answers)} ä¸ªç­”æ¡ˆ)")
                questionnaire_responses = {
                    "answers": calibration_answers,
                    "source": "calibration_answers_fallback"
                }

        # æå–é¡¹ç›®æ€»ç›‘çš„æˆ˜ç•¥åˆ†æä¿¡æ¯
        query_type = strategic_analysis.get("query_type", "æ·±åº¦ä¼˜å…ˆæ¢è¯¢")
        query_type_reasoning = strategic_analysis.get("query_type_reasoning", "")

        # ========== 1. æ„å»ºå®¡æ ¸åé¦ˆæ•°æ® ==========
        review_feedback_data = None
        if review_result or review_history:
            review_feedback_data = self._extract_review_feedback(review_result, review_history, improvement_suggestions)
        
        # ========== 2. æ„å»ºé—®å·å›ç­”æ•°æ® ==========
        questionnaire_data = None
        if questionnaire_summary or questionnaire_responses or calibration_questionnaire:
            questionnaire_data = self._extract_questionnaire_data(
                calibration_questionnaire,
                questionnaire_responses,
                questionnaire_summary
            )
        
        # ========== 3. æ„å»ºå¯è§†åŒ–æ•°æ® ==========
        visualization_data = None
        if review_history:
            visualization_data = self._extract_visualization_data(review_history, review_result)

        # æ„å»ºå®¡æ ¸ç»“æœéƒ¨åˆ†ï¼ˆåŸæœ‰é€»è¾‘ï¼Œä¿ç•™ç”¨äºæç¤ºè¯ï¼‰
        review_section = ""
        if final_ruling:
            review_section = f"""

## ğŸ“‹ å®¡æ ¸ç³»ç»Ÿçš„æœ€ç»ˆè£å®š

{final_ruling}

### æ”¹è¿›å»ºè®®æ‘˜è¦
"""
            if improvement_suggestions:
                for idx, suggestion in enumerate(improvement_suggestions[:5], 1):
                    priority = suggestion.get('priority', 'should_fix')
                    deadline = suggestion.get('deadline', 'before_launch')
                    review_section += f"{idx}. [{priority}] {suggestion.get('issue_id', 'N/A')} - {deadline}\n"
            else:
                review_section += "æ— éœ€æ”¹è¿›ï¼Œåˆ†æè´¨é‡å·²è¾¾æ ‡ã€‚\n"
        
        # æ„å»ºæ–°å¢æ•°æ®éƒ¨åˆ†
        additional_data_section = ""
        if review_feedback_data:
            additional_data_section += f"""

## ğŸ” å®¡æ ¸åé¦ˆæ•°æ®ï¼ˆç”¨äºå¡«å……review_feedbackå­—æ®µï¼‰

{json.dumps(review_feedback_data, ensure_ascii=False, indent=2)}
"""
        
        if questionnaire_data:
            additional_data_section += f"""

## ğŸ“ é—®å·å›ç­”æ•°æ®ï¼ˆç”¨äºå¡«å……questionnaire_responseså­—æ®µï¼‰

{json.dumps(questionnaire_data, ensure_ascii=False, indent=2)}
"""
        
        if visualization_data:
            additional_data_section += f"""

## ğŸ“Š å¯è§†åŒ–æ•°æ®ï¼ˆç”¨äºå¡«å……review_visualizationå­—æ®µï¼‰

{json.dumps(visualization_data, ensure_ascii=False, indent=2)}
"""

        return f"""è¯·æ•´åˆä»¥ä¸‹åˆ†æç»“æœï¼Œç”Ÿæˆç»¼åˆé¡¹ç›®åˆ†ææŠ¥å‘Šã€‚

## é¡¹ç›®æ€»ç›‘çš„ä½œæˆ˜è®¡åˆ’

**æ¢è¯¢æ¶æ„ç±»å‹ï¼š** {query_type}
**åˆ¤å®šç†ç”±ï¼š** {query_type_reasoning}

## é¡¹ç›®åŸºæœ¬ä¿¡æ¯

**é¡¹ç›®æ¦‚è¿°ï¼š** {structured_requirements.get("project_overview", "æš‚æ— ")}
**æ ¸å¿ƒç›®æ ‡ï¼š** {json.dumps(structured_requirements.get("core_objectives", []), ensure_ascii=False)}

## V2-V6 ä¸“å®¶çš„åˆ†ææˆæœ

{self._format_agent_results(agent_results)}
{review_section}
{additional_data_section}

## ä»»åŠ¡è¦æ±‚

1. **è¯†åˆ«æ¢è¯¢æ¶æ„**ï¼šä½¿ç”¨ä¸Šè¿°æ¢è¯¢æ¶æ„ç±»å‹ï¼ˆ{query_type}ï¼‰
2. **å¡«å……sectionså­—æ®µ**ï¼šä¸ºæ¯ä¸ªä¸“å®¶åˆ›å»ºå¯¹åº”çš„sectionæ¡ç›®
   - ä½¿ç”¨ä¸“å®¶æ ‡æ³¨çš„ç« èŠ‚åç§°ä½œä¸ºé”®ï¼ˆå¦‚ "design_research"ï¼‰
   - å°†ä¸“å®¶çš„å®Œæ•´åˆ†æç»“æœæ”¾å…¥contentå­—æ®µ
   - ä¿ç•™ä¸“å®¶çš„ç½®ä¿¡åº¦
3. **æ•´åˆå®¡æ ¸ç»“æœ**ï¼šå¦‚æœå­˜åœ¨æœ€ç»ˆè£å®šï¼Œè¯·åœ¨ç»¼åˆåˆ†æä¸­ä½“ç°æ”¹è¿›å»ºè®®
4. **ç”Ÿæˆç»¼åˆåˆ†æ**ï¼šæ•´åˆæ‰€æœ‰ä¸“å®¶çš„æ´å¯Ÿ
5. **å¡«å……æ–°å¢å­—æ®µ**ï¼ˆå¦‚æœæ•°æ®å­˜åœ¨ï¼‰ï¼š
   - review_feedback: ä½¿ç”¨ä¸Šé¢æä¾›çš„å®¡æ ¸åé¦ˆæ•°æ®
   - questionnaire_responses: ä½¿ç”¨ä¸Šé¢æä¾›çš„é—®å·å›ç­”æ•°æ®
   - review_visualization: ä½¿ç”¨ä¸Šé¢æä¾›çš„å¯è§†åŒ–æ•°æ®
6. **è¾“å‡ºæ ¼å¼**ï¼šçº¯JSONï¼Œä¸è¦æ·»åŠ markdownæ ‡è®°

âš ï¸ é‡è¦ï¼šå¿…é¡»ä¸ºæ‰€æœ‰ä¸“å®¶åˆ›å»ºsectionæ¡ç›®ï¼Œä¸è¦é—æ¼ä»»ä½•ä¸€ä¸ªï¼

è¯·ç«‹å³ç”ŸæˆJSONæ ¼å¼çš„æŠ¥å‘Šã€‚"""
    
    def execute(
        self,
        state: ProjectAnalysisState,
        config: RunnableConfig,
        store: Optional[BaseStore] = None
    ) -> AnalysisResult:
        """æ‰§è¡Œç»“æœèšåˆ - ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º"""
        start_time = time.time()

        try:
            logger.info(f"Starting result aggregation for session {state.get('session_id')}")

            # ğŸš€ Phase 1.4: å‘é€åˆå§‹è¿›åº¦æ›´æ–°
            self._update_progress(state, "å‡†å¤‡æ•´åˆä¸“å®¶åˆ†æç»“æœ", 0.0)

            # éªŒè¯è¾“å…¥
            if not self.validate_input(state):
                raise ValueError("Invalid input: no agent results found")

            # å‡†å¤‡æ¶ˆæ¯
            self._update_progress(state, "æ„å»ºèšåˆæç¤ºè¯", 0.1)
            messages = self.prepare_messages(state)

            # ğŸ”§ æ€§èƒ½ä¼˜åŒ–: ç²¾ç®€JSONæ ¼å¼æé†’ï¼ˆstructured outputå·²åŒ…å«æ ¼å¼è¦æ±‚ï¼‰
            json_format_reminder = SystemMessage(content="OUTPUT: Use structured JSON schema provided")
            messages.insert(1, json_format_reminder)

            # ä½¿ç”¨ with_structured_output å¼ºåˆ¶ LLM è¿”å›ç¬¦åˆ Pydantic æ¨¡å‹çš„ç»“æ„
            # âš ï¸ æ³¨æ„: ç”±äº content å­—æ®µæ˜¯ Dict[str, Any]ï¼ˆçµæ´»å­—å…¸ï¼‰ï¼Œæ— æ³•ä½¿ç”¨ strict mode
            # OpenAI strict mode è¦æ±‚æ‰€æœ‰å¯¹è±¡éƒ½è®¾ç½® additionalProperties: false
            # ä½† Dict[str, Any] éœ€è¦ additionalProperties: true æ¥å…è®¸ä»»æ„é”®
            # å› æ­¤ä½¿ç”¨ function_calling æ–¹æ³•è€Œä¸æ˜¯ json_schema + strict
            logger.info("Using structured output with Pydantic model (function_calling method)")

            # ğŸš€ Phase 1.4: è¿›åº¦æ›´æ–°
            self._update_progress(state, "é…ç½®ç»“æ„åŒ–è¾“å‡ºæ¨¡å‹", 0.2)

            structured_llm = self.llm_model.with_structured_output(
                FinalReport,
                method="function_calling",  # ä½¿ç”¨ function_calling ä»¥æ”¯æŒçµæ´»çš„ Dict[str, Any]
                include_raw=True  # å®˜æ–¹æ¨èï¼šå¤„ç†å¤æ‚schemaæ—¶é¿å…æŠ›å‡ºå¼‚å¸¸
            )
            logger.info("Successfully configured function_calling method")

            # è°ƒç”¨ LLM å¹¶è·å–ç»“æ„åŒ–è¾“å‡º
            # âœ… ä¿®å¤: æ·»åŠ æ›´è¯¦ç»†çš„é”™è¯¯å¤„ç†ï¼Œæ•è·è¶…æ—¶å’Œç½‘ç»œé”™è¯¯
            # âœ… ä¿®å¤: æ˜¾å¼ä¼ é€’ max_tokens å’Œ request_timeout å‚æ•°
            # ğŸ”§ æ–°å¢: æ·»åŠ é‡è¯•é€»è¾‘ä»¥åº”å¯¹ä¸ç¨³å®šçš„APIå“åº”
            # è¿™äº›å‚æ•°åœ¨ ChatOpenAI åˆå§‹åŒ–æ—¶è®¾ç½®ï¼Œä½†ä¸ºäº†ç¡®ä¿å®ƒä»¬è¢«ä½¿ç”¨ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå†æ¬¡ä¼ é€’
            import os

            max_tokens = int(os.getenv("MAX_TOKENS", "32000"))
            request_timeout = int(os.getenv("LLM_TIMEOUT", "600"))
            max_retries = int(os.getenv("MAX_RETRIES", "3"))
            retry_delay = int(os.getenv("RETRY_DELAY", "5"))

            result = None
            last_error = None

            # ğŸš€ Phase 1.4: è¿›åº¦æ›´æ–°
            agent_count = len(state.get("agent_results", {}))
            self._update_progress(
                state,
                f"è°ƒç”¨LLMæ•´åˆ{agent_count}ä½ä¸“å®¶çš„åˆ†æç»“æœï¼ˆé¢„è®¡60-90ç§’ï¼‰",
                0.3
            )

            # ğŸ”§ é‡è¯•å¾ªç¯
            for attempt in range(max_retries):
                try:
                    # ğŸš€ Phase 1.4: è¿›åº¦æ›´æ–°ï¼ˆé‡è¯•æç¤ºï¼‰
                    if attempt > 0:
                        self._update_progress(
                            state,
                            f"LLMè°ƒç”¨é‡è¯•ä¸­ï¼ˆç¬¬{attempt + 1}æ¬¡å°è¯•ï¼‰",
                            0.3 + (attempt * 0.05)
                        )

                    logger.info(f"Invoking LLM (attempt {attempt + 1}/{max_retries}) with max_tokens={max_tokens}, request_timeout={request_timeout}")

                    # è®°å½•å¼€å§‹æ—¶é—´
                    start_time = time.time()

                    result = structured_llm.invoke(
                        messages,
                        max_tokens=max_tokens,
                        request_timeout=request_timeout
                    )

                    # è®°å½•ç»“æŸæ—¶é—´
                    elapsed_time = time.time() - start_time
                    logger.info(f"LLM invocation completed in {elapsed_time:.2f}s")

                    # ğŸš€ Phase 1.4: è¿›åº¦æ›´æ–°
                    self._update_progress(state, "LLMå“åº”å®Œæˆï¼Œæ­£åœ¨è§£æç»“æœ", 0.6)

                    # æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                    break

                except json.JSONDecodeError as e:
                    # JSONè§£æé”™è¯¯ - é€šå¸¸æ˜¯å“åº”è¢«æˆªæ–­æˆ–è¶…æ—¶
                    last_error = e
                    logger.error(f"Attempt {attempt + 1}/{max_retries} - JSON parsing failed: {e}")
                    logger.error(f"This usually indicates a timeout or incomplete response from the API")

                    if attempt < max_retries - 1:
                        logger.info(f"Retrying in {retry_delay} seconds...")
                        time.sleep(retry_delay)
                    else:
                        logger.error(f"All {max_retries} attempts failed")
                        logger.info("Attempting to use fallback parsing method")
                        raise ValueError(f"LLM response was incomplete or truncated after {max_retries} attempts. This may be due to timeout or network issues. Original error: {e}")

                except Exception as e:
                    # å…¶ä»–é”™è¯¯ï¼ˆç½‘ç»œè¶…æ—¶ã€APIé”™è¯¯ç­‰ï¼‰
                    last_error = e
                    logger.error(f"Attempt {attempt + 1}/{max_retries} - LLM invocation failed: {e}")

                    if attempt < max_retries - 1:
                        logger.info(f"Retrying in {retry_delay} seconds...")
                        time.sleep(retry_delay)
                    else:
                        logger.error(f"All {max_retries} attempts failed")
                        raise

            # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
            if result is None:
                raise ValueError(f"LLM invocation failed after {max_retries} attempts. Last error: {last_error}")

            # æ£€æŸ¥æ˜¯å¦æœ‰è§£æé”™è¯¯
            if result.get('parsing_error'):
                # è§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
                logger.warning(f"Structured output parsing failed: {result['parsing_error']}")
                logger.info("Falling back to manual parsing")

                # ğŸš€ Phase 1.4: è¿›åº¦æ›´æ–°
                self._update_progress(state, "ç»“æ„åŒ–è§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨è§£ææ–¹æ¡ˆ", 0.65)

                raw_message = result['raw']
                final_report = self._parse_final_report(raw_message.content, state)
                
                # âœ… P0ä¿®å¤: å¤‡ç”¨è§£æè·¯å¾„ä¹Ÿå¿…é¡»æå–çœŸå®æ•°æ®
                # ğŸ”§ v7.1.4: ç¡®ä¿æ— æ¡ä»¶è¦†ç›–ï¼Œé¿å…ä¸ä¸»è·¯å¾„é‡å¤
                logger.info("Fallback path: extracting real expert_reports from state")
                if "expert_reports" not in final_report or not final_report["expert_reports"]:
                    final_report["expert_reports"] = self._extract_expert_reports(state)
                final_report["challenge_resolutions"] = self._extract_challenge_resolutions(state)
                
                # ğŸ”¥ v7.5ä¿®å¤: fallback è·¯å¾„ä¹Ÿå¿…é¡»æå–é—®å·æ•°æ®
                # åŸå› ï¼šé—®å·æ•°æ®æå–é€»è¾‘åŸæœ¬åªåœ¨æˆåŠŸè§£æè·¯å¾„ï¼Œå¯¼è‡´ fallback æ—¶å‰ç«¯æ˜¾ç¤ºç©ºé—®å·
                self._update_progress(state, "[Fallback] æå–æ ¡å‡†é—®å·å›ç­”", 0.7)
                calibration_questionnaire = state.get("calibration_questionnaire") or {}
                questionnaire_responses_state = state.get("questionnaire_responses") or {}
                questionnaire_summary = state.get("questionnaire_summary") or {}
                
                if calibration_questionnaire or questionnaire_responses_state or questionnaire_summary:
                    real_questionnaire_data = self._extract_questionnaire_data(
                        calibration_questionnaire,
                        questionnaire_responses_state,
                        questionnaire_summary
                    )
                    if real_questionnaire_data and real_questionnaire_data.get("responses"):
                        final_report["questionnaire_responses"] = real_questionnaire_data
                        logger.info(f"âœ… [Fallback] å·²æå– questionnaire_responses: {len(real_questionnaire_data['responses'])} æ¡å›ç­”")
                    else:
                        logger.debug("â„¹ï¸ [Fallback] æ— é—®å·æ•°æ®å¯æå–")
                
                # ğŸ”¥ v7.5ä¿®å¤: fallback è·¯å¾„ä¹Ÿå¿…é¡»æå–éœ€æ±‚åˆ†æç»“æœ
                # åŸå› ï¼šéœ€æ±‚åˆ†æå¸ˆçš„è¾“å‡ºéœ€è¦æ­£ç¡®ä¼ é€’åˆ°å‰ç«¯
                if "requirements_analysis" not in final_report or not final_report.get("requirements_analysis"):
                    structured_requirements = state.get("structured_requirements") or {}
                    if structured_requirements:
                        final_report["requirements_analysis"] = structured_requirements
                        logger.info(f"âœ… [Fallback] å·²æå– requirements_analysis")
                
                # âœ… P2ä¿®å¤: ç¡®ä¿ raw_content ä¿å­˜åŸå§‹LLMå“åº”
                final_report["raw_content"] = raw_message.content
                final_report["metadata"] = {
                    **final_report.get("metadata", {}),
                    "parsing_mode": "fallback",
                    "fallback_reason": str(result.get('parsing_error', 'unknown'))
                }
            else:
                # è§£ææˆåŠŸ
                logger.info("Successfully received and parsed structured output from LLM")

                # ğŸš€ Phase 1.4: è¿›åº¦æ›´æ–°
                self._update_progress(state, "ç»“æ„åŒ–è§£ææˆåŠŸï¼Œæ­£åœ¨éªŒè¯æ•°æ®å®Œæ•´æ€§", 0.7)

                final_report_pydantic = result['parsed']

                if final_report_pydantic is None:
                    logger.warning("Structured output parsing returned None, falling back to manual parsing")
                    self._update_progress(state, "è§£æç»“æœä¸ºç©ºï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ", 0.72)
                    raw_message = result.get('raw')
                    if raw_message:
                        final_report = self._parse_final_report(raw_message.content, state)
                        # âœ… P0ä¿®å¤: å¤‡ç”¨è§£æè·¯å¾„ä¹Ÿå¿…é¡»æå–çœŸå®æ•°æ®
                        # ğŸ”§ v7.1.4: ç¡®ä¿æ— æ¡ä»¶è¦†ç›–ï¼Œé¿å…ä¸ä¸»è·¯å¾„é‡å¤
                        logger.info("Fallback path (None parsed): extracting real expert_reports from state")
                        if "expert_reports" not in final_report or not final_report["expert_reports"]:
                            final_report["expert_reports"] = self._extract_expert_reports(state)
                        final_report["challenge_resolutions"] = self._extract_challenge_resolutions(state)
                        
                        # ğŸ”¥ v7.5ä¿®å¤: fallback_none_parsed è·¯å¾„ä¹Ÿå¿…é¡»æå–é—®å·å’Œéœ€æ±‚åˆ†æ
                        self._update_progress(state, "[Fallback-None] æå–æ ¡å‡†é—®å·å›ç­”", 0.74)
                        calibration_questionnaire = state.get("calibration_questionnaire") or {}
                        questionnaire_responses_state = state.get("questionnaire_responses") or {}
                        questionnaire_summary = state.get("questionnaire_summary") or {}
                        
                        if calibration_questionnaire or questionnaire_responses_state or questionnaire_summary:
                            real_questionnaire_data = self._extract_questionnaire_data(
                                calibration_questionnaire,
                                questionnaire_responses_state,
                                questionnaire_summary
                            )
                            if real_questionnaire_data and real_questionnaire_data.get("responses"):
                                final_report["questionnaire_responses"] = real_questionnaire_data
                                logger.info(f"âœ… [Fallback-None] å·²æå– questionnaire_responses: {len(real_questionnaire_data['responses'])} æ¡å›ç­”")
                        
                        # æå–éœ€æ±‚åˆ†æ
                        if "requirements_analysis" not in final_report or not final_report.get("requirements_analysis"):
                            structured_requirements = state.get("structured_requirements") or {}
                            if structured_requirements:
                                final_report["requirements_analysis"] = structured_requirements
                                logger.info(f"âœ… [Fallback-None] å·²æå– requirements_analysis")
                        
                        # âœ… P2ä¿®å¤: ç¡®ä¿ raw_content ä¿å­˜åŸå§‹LLMå“åº”
                        final_report["raw_content"] = raw_message.content
                        final_report["metadata"] = {
                            **final_report.get("metadata", {}),
                            "parsing_mode": "fallback_none_parsed"
                        }
                    else:
                        raise ValueError("LLM response parsed as None and no raw message available")
                else:
                    # è½¬æ¢ Pydantic æ¨¡å‹ä¸ºå­—å…¸
                    final_report = final_report_pydantic.model_dump()

                # âœ… æ–°å¢: æ£€æŸ¥ sections æ˜¯å¦ä¸ºç©ºï¼Œå¦‚æœä¸ºç©ºåˆ™æ‰‹åŠ¨å¡«å……
                if not final_report.get("sections") or len(final_report["sections"]) == 0:
                    logger.warning("LLM returned empty sections, manually populating from agent_results")
                    self._update_progress(state, "LLMæœªè¿”å›ç« èŠ‚æ•°æ®ï¼Œæ­£åœ¨æ‰‹åŠ¨å¡«å……", 0.75)
                    final_report["sections"] = self._manually_populate_sections(state)
                    logger.info(f"Manually populated {len(final_report['sections'])} sections")

                # âœ… ä¿®å¤v4.0: å§‹ç»ˆç”¨çœŸå®æ•°æ®è¦†ç›– expert_reports
                # åŸå› ï¼šLLM å¯èƒ½è¿”å›å ä½ç¬¦æ–‡æœ¬å¦‚ "{...å†…å®¹ç•¥...}"ï¼Œå¿…é¡»ç”¨çœŸå®æ•°æ®è¦†ç›–
                # ğŸ”§ v7.1.4ä¿®å¤: ç®€åŒ–é€»è¾‘ï¼Œæ— æ¡ä»¶è¦†ç›–ä»¥é¿å…é‡å¤
                logger.info("Overwriting expert_reports with actual expert content from state")
                self._update_progress(state, "æå–ä¸“å®¶åŸå§‹æŠ¥å‘Š", 0.8)
                real_expert_reports = self._extract_expert_reports(state)
                
                # ç›´æ¥è¦†ç›–ï¼Œæ— éœ€æ£€æŸ¥å ä½ç¬¦ï¼ˆé¿å…é‡å¤èµ‹å€¼ï¼‰
                final_report["expert_reports"] = real_expert_reports
                    
                logger.info(f"Extracted {len(final_report['expert_reports'])} expert reports")

                # ğŸ”¥ ä¿®å¤ï¼šä» sections ä¸­æå– requirements_analysis å¹¶æå‡åˆ°é¡¶å±‚
                # åŸå› ï¼šrequirements_analysis è¢« _manually_populate_sections æ”¾åœ¨äº† sections æ•°ç»„ä¸­
                # ä½†å‰ç«¯æœŸæœ›å®ƒåœ¨é¡¶å±‚ï¼ˆä¸ insightsã€deliberation_process åŒçº§ï¼‰
                sections_list = final_report.get("sections", [])
                if isinstance(sections_list, list):
                    for section in sections_list:
                        if isinstance(section, dict) and section.get("section_id") == "requirements_analysis":
                            # æå– requirements_analysis çš„ contentï¼ˆå¯èƒ½æ˜¯JSONå­—ç¬¦ä¸²ï¼‰
                            content_str = section.get("content", "")
                            if content_str:
                                try:
                                    # å°è¯•è§£æä¸ºå­—å…¸
                                    requirements_data = json.loads(content_str) if isinstance(content_str, str) else content_str
                                    final_report["requirements_analysis"] = requirements_data
                                    logger.info("âœ… å·²ä» sections æå– requirements_analysis åˆ°é¡¶å±‚")
                                except json.JSONDecodeError:
                                    logger.warning(f"âš ï¸ requirements_analysis å†…å®¹ä¸æ˜¯æœ‰æ•ˆ JSON: {content_str[:100]}")
                            break

                # ğŸ†• v3.5.1: æ·»åŠ æŒ‘æˆ˜è§£å†³ç»“æœ
                self._update_progress(state, "æå–ä¸“å®¶æŒ‘æˆ˜è§£å†³ç»“æœ", 0.85)
                final_report["challenge_resolutions"] = self._extract_challenge_resolutions(state)

                # ğŸ†• v4.1ä¿®å¤: å¼ºåˆ¶ç”¨çœŸå®é—®å·æ•°æ®è¦†ç›– questionnaire_responses
                # åŸå› ï¼šLLM ç»“æ„åŒ–è¾“å‡ºå¯èƒ½è¿”å› Noneï¼ˆå¯é€‰å­—æ®µï¼‰ï¼Œå¯¼è‡´å‰ç«¯æ˜¾ç¤º"æœªå›ç­”"
                self._update_progress(state, "æå–æ ¡å‡†é—®å·å›ç­”", 0.87)
                calibration_questionnaire = state.get("calibration_questionnaire") or {}
                questionnaire_responses_state = state.get("questionnaire_responses") or {}
                questionnaire_summary = state.get("questionnaire_summary") or {}
                
                if calibration_questionnaire or questionnaire_responses_state or questionnaire_summary:
                    real_questionnaire_data = self._extract_questionnaire_data(
                        calibration_questionnaire,
                        questionnaire_responses_state,
                        questionnaire_summary
                    )
                    if real_questionnaire_data and real_questionnaire_data.get("responses"):
                        final_report["questionnaire_responses"] = real_questionnaire_data
                        logger.info(f"âœ… å·²è¦†ç›– questionnaire_responses: {len(real_questionnaire_data['responses'])} æ¡å›ç­”")
                    else:
                        logger.debug("â„¹ï¸ æ— é—®å·æ•°æ®å¯è¦†ç›–")

                # æ·»åŠ å…ƒæ•°æ®
                self._update_progress(state, "ç”ŸæˆæŠ¥å‘Šå…ƒæ•°æ®", 0.9)
                
                # ğŸ†• v7.4: å¢å¼ºæ‰§è¡Œå…ƒæ•°æ®ï¼Œæå‡ç”¨æˆ·ä½“éªŒ
                # æ”¶é›†æ›´å¤šç»Ÿè®¡æ•°æ®
                agent_results = state.get("agent_results", {})
                questionnaire_responses = final_report.get("questionnaire_responses", {})
                batches = state.get("batches", [])
                review_iterations = state.get("review_iterations", 0)
                
                # è®¡ç®—é—®å·å›ç­”æ•°é‡
                questionnaire_count = 0
                if questionnaire_responses:
                    responses = questionnaire_responses.get("responses", [])
                    questionnaire_count = len([r for r in responses if r.get("answer") and r.get("answer") != "æœªå›ç­”"])
                
                # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
                confidence_values = []
                for role_id, result in agent_results.items():
                    if isinstance(result, dict):
                        # ä»ä»»åŠ¡å¯¼å‘ä¸“å®¶è¾“å‡ºä¸­æå–ç½®ä¿¡åº¦
                        exec_meta = result.get("execution_metadata", {})
                        if exec_meta and isinstance(exec_meta, dict):
                            conf = exec_meta.get("confidence")
                            if conf is not None:
                                confidence_values.append(float(conf))
                
                avg_confidence = sum(confidence_values) / len(confidence_values) if confidence_values else None
                
                # è·å–å¤æ‚åº¦çº§åˆ«
                task_complexity = state.get("task_complexity", "complex")
                complexity_display = {
                    "simple": "ç®€å•",
                    "medium": "ä¸­ç­‰",
                    "complex": "å¤æ‚"
                }.get(task_complexity, "å¤æ‚")
                
                # è®¡ç®—åˆ†æè€—æ—¶ï¼ˆå¦‚æœæœ‰å¼€å§‹æ—¶é—´ï¼‰
                analysis_duration = None
                created_at = state.get("created_at")
                if created_at:
                    try:
                        if isinstance(created_at, str):
                            analysis_start_time = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                        else:
                            analysis_start_time = created_at
                        duration_seconds = (datetime.now() - analysis_start_time.replace(tzinfo=None)).total_seconds()
                        if duration_seconds < 60:
                            analysis_duration = f"{int(duration_seconds)}ç§’"
                        elif duration_seconds < 3600:
                            minutes = int(duration_seconds // 60)
                            seconds = int(duration_seconds % 60)
                            analysis_duration = f"{minutes}åˆ†{seconds}ç§’"
                        else:
                            hours = int(duration_seconds // 3600)
                            minutes = int((duration_seconds % 3600) // 60)
                            analysis_duration = f"{hours}æ—¶{minutes}åˆ†"
                    except Exception as e:
                        logger.debug(f"è®¡ç®—åˆ†æè€—æ—¶å¤±è´¥: {e}")
                
                # ğŸ”¥ ä¿®å¤: ä» deliberation_process ä¸­æå– inquiry_architecture
                inquiry_arch = "æ·±åº¦ä¼˜å…ˆæ¢è¯¢"  # é»˜è®¤å€¼
                deliberation = final_report.get("deliberation_process")
                if deliberation:
                    if isinstance(deliberation, dict):
                        inquiry_arch = deliberation.get("inquiry_architecture", inquiry_arch)
                    elif hasattr(deliberation, "inquiry_architecture"):
                        inquiry_arch = deliberation.inquiry_architecture or inquiry_arch
                # ä¹ŸåŒæ­¥åˆ°é¡¶å±‚ï¼Œæ–¹ä¾¿å…¶ä»–åœ°æ–¹ä½¿ç”¨
                final_report["inquiry_architecture"] = inquiry_arch
                
                final_report["metadata"] = {
                    "generated_at": datetime.now().isoformat(),
                    "session_id": state.get("session_id"),
                    "total_agents": len(agent_results),
                    "overall_confidence": self._calculate_overall_confidence(state),
                    "estimated_pages": self._estimate_report_pages(final_report),
                    "inquiry_architecture": inquiry_arch,
                    # ğŸ†• v7.4 æ–°å¢å­—æ®µ
                    "total_batches": len(batches) if batches else 1,
                    "complexity_level": complexity_display,
                    "questionnaire_answered": questionnaire_count,
                    "review_rounds": review_iterations,
                    "confidence_average": avg_confidence,
                    "analysis_duration": analysis_duration,
                    # ä¸“å®¶åˆ†å¸ƒç»Ÿè®¡
                    "expert_distribution": self._get_expert_distribution(agent_results),
                }

                # ä¿å­˜åŸå§‹ LLM å“åº”å†…å®¹
                # âœ… P2ä¿®å¤: ä»…åœ¨æœªè®¾ç½®æ—¶æ‰è®¾ç½®ï¼Œé¿å…è¦†ç›–å¤‡ç”¨è·¯å¾„çš„å€¼
                if "raw_content" not in final_report or not final_report.get("raw_content"):
                    raw_msg = result.get('raw')
                    if raw_msg:
                        final_report["raw_content"] = raw_msg.content
                    else:
                        final_report["raw_content"] = str(final_report_pydantic)

            # ğŸ†• v7.0: ä»è´£ä»»è€…è¾“å‡ºä¸­æå–äº¤ä»˜ç‰©ç­”æ¡ˆï¼Œè¦†ç›– LLM ç”Ÿæˆçš„ core_answer
            self._update_progress(state, "æå–äº¤ä»˜ç‰©è´£ä»»è€…ç­”æ¡ˆ", 0.92)
            deliverable_metadata = state.get("deliverable_metadata") or {}
            
            if deliverable_metadata:
                logger.info(f"ğŸ¯ [v7.0] æ£€æµ‹åˆ° {len(deliverable_metadata)} ä¸ªäº¤ä»˜ç‰©å…ƒæ•°æ®ï¼Œå¼€å§‹æå–è´£ä»»è€…ç­”æ¡ˆ")
                extracted_core_answer = self._extract_deliverable_answers(state)
                
                # è¦†ç›– LLM ç”Ÿæˆçš„ core_answer
                if extracted_core_answer.get("deliverable_answers"):
                    final_report["core_answer"] = extracted_core_answer
                    logger.info(f"âœ… [v7.0] å·²ç”¨è´£ä»»è€…ç­”æ¡ˆè¦†ç›– core_answer: {len(extracted_core_answer['deliverable_answers'])} ä¸ªäº¤ä»˜ç‰©")
                else:
                    logger.warning("âš ï¸ [v7.0] æœªæå–åˆ°äº¤ä»˜ç‰©ç­”æ¡ˆï¼Œä¿ç•™ LLM ç”Ÿæˆçš„ core_answer")
            else:
                logger.info("â„¹ï¸ [v7.0] æ— äº¤ä»˜ç‰©å…ƒæ•°æ®ï¼Œä¿ç•™ LLM ç”Ÿæˆçš„ core_answer")

            # åˆ›å»ºåˆ†æç»“æœ
            self._update_progress(state, "æ„å»ºæœ€ç»ˆåˆ†æç»“æœ", 0.95)
            result = self.create_analysis_result(
                content=str(final_report.get("executive_summary", {})),
                structured_data=final_report,
                confidence=self._calculate_overall_confidence(state),
                sources=["all_agents", "requirements_analysis", "comprehensive_analysis"]
            )

            end_time = time.time()
            self._track_execution_time(start_time, end_time)

            # ğŸš€ Phase 1.4: æœ€ç»ˆè¿›åº¦æ›´æ–°
            self._update_progress(state, "ç»ˆå®¡èšåˆå®Œæˆ", 1.0)

            logger.info("Result aggregation completed successfully")
            return result

        except Exception as e:
            error = self.handle_error(e, "Result aggregation")
            raise error

    def _update_progress(self, state: ProjectAnalysisState, detail: str, progress: float):
        """
        ğŸš€ Phase 1.4: æ›´æ–°è¿›åº¦åˆ°çŠ¶æ€ï¼ˆç”¨äºWebSocketæ¨é€ï¼‰

        Args:
            state: å½“å‰çŠ¶æ€
            detail: è¿›åº¦è¯¦æƒ…æè¿°
            progress: è¿›åº¦å€¼ï¼ˆ0.0-1.0ï¼‰
        """
        try:
            # æ›´æ–°çŠ¶æ€ä¸­çš„è¿›åº¦ä¿¡æ¯
            state["current_stage"] = "ç»ˆå®¡èšåˆ"
            state["detail"] = detail
            state["progress"] = progress

            logger.info(f"ğŸ“Š [ç»ˆå®¡èšåˆ] {detail} ({progress*100:.0f}%)")
        except Exception as e:
            # è¿›åº¦æ›´æ–°å¤±è´¥ä¸åº”é˜»å¡ä¸»æµç¨‹
            logger.warning(f"âš ï¸ è¿›åº¦æ›´æ–°å¤±è´¥: {e}")
    
    def _format_agent_results(self, agent_results: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æ™ºèƒ½ä½“ç»“æœç”¨äºèšåˆè¾“å‡º - æ”¯æŒTaskOrientedExpertOutputç»“æ„"""
        if not agent_results:
            return "(no agent results available)"

        formatted_blocks: List[str] = []

        for agent_id, result in agent_results.items():
            if not result:
                continue

            # æ£€æŸ¥æ˜¯å¦ä¸ºTaskOrientedExpertOutputç»“æ„
            structured_output = result.get("structured_output")
            if structured_output and isinstance(structured_output, dict):
                # æ–°çš„ä»»åŠ¡å¯¼å‘è¾“å‡ºæ ¼å¼
                formatted_block = self._format_task_oriented_output(agent_id, result, structured_output)
            else:
                # ä¼ ç»Ÿæ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
                formatted_block = self._format_legacy_output(agent_id, result)
            
            if formatted_block:
                formatted_blocks.append(formatted_block)

        return "\n\n".join(formatted_blocks) if formatted_blocks else "(no agent results available)"

    def _format_task_oriented_output(self, agent_id: str, result: Dict[str, Any], structured_output: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–TaskOrientedExpertOutputç»“æ„çš„ä¸“å®¶è¾“å‡º
        
        Args:
            agent_id: ä¸“å®¶ID
            result: å®Œæ•´çš„ä¸“å®¶æ‰§è¡Œç»“æœ
            structured_output: TaskOrientedExpertOutputç»“æ„åŒ–æ•°æ®
            
        Returns:
            str: æ ¼å¼åŒ–çš„è¾“å‡ºå­—ç¬¦ä¸²
        """
        lines = []
        
        # ä¸“å®¶åŸºæœ¬ä¿¡æ¯
        expert_summary = structured_output.get("expert_summary", {})
        expert_name = expert_summary.get("expert_name", result.get("expert_name", agent_id))
        objective = expert_summary.get("objective_statement", "æœªæŒ‡å®šç›®æ ‡")
        
        lines.append(f"### {expert_name} ({agent_id}) - ä»»åŠ¡å¯¼å‘è¾“å‡º")
        lines.append(f"**å®Œæˆç›®æ ‡**: {objective}")
        lines.append("")
        
        # ä»»åŠ¡ç»“æœ
        task_results = structured_output.get("task_results", [])
        if task_results:
            lines.append("**äº¤ä»˜ç‰©ç»“æœ**:")
            for i, deliverable in enumerate(task_results, 1):
                name = deliverable.get("deliverable_name", f"äº¤ä»˜ç‰©{i}")
                content = deliverable.get("content", "")
                format_type = deliverable.get("format", "analysis")
                completeness = deliverable.get("completeness_score", 0.0)
                methodology = deliverable.get("methodology", "æœªæŒ‡å®š")
                
                lines.append(f"  {i}. **{name}** ({format_type}, å®Œæˆåº¦: {completeness:.1%})")
                lines.append(f"     æ–¹æ³•: {methodology}")
                
                # å†…å®¹æ‘˜è¦ï¼ˆé™åˆ¶é•¿åº¦ï¼‰
                if content:
                    if len(content) > 500:
                        content_preview = content[:500] + "..."
                    else:
                        content_preview = content
                    lines.append(f"     å†…å®¹: {content_preview}")
                lines.append("")
        
        # åè®®æ‰§è¡ŒçŠ¶æ€
        protocol_execution = structured_output.get("protocol_execution", {})
        if protocol_execution:
            final_status = protocol_execution.get("final_status", "unknown")
            confidence_level = protocol_execution.get("confidence_level", 0.0)
            lines.append(f"**åè®®æ‰§è¡ŒçŠ¶æ€**: {final_status} (ä¿¡å¿ƒæ°´å¹³: {confidence_level:.1%})")
            
            # è‡ªä¸»è¡ŒåŠ¨
            autonomy_actions = protocol_execution.get("autonomy_actions_taken", [])
            if autonomy_actions:
                lines.append("**é‡‡å–çš„è‡ªä¸»è¡ŒåŠ¨**:")
                for action in autonomy_actions[:3]:  # é™åˆ¶æ˜¾ç¤ºå‰3ä¸ª
                    lines.append(f"  - {action}")
                if len(autonomy_actions) > 3:
                    lines.append(f"  - ... (å…±{len(autonomy_actions)}é¡¹)")
            
            # æŒ‘æˆ˜å’Œé‡æ–°è§£é‡Š
            challenges = protocol_execution.get("challenges_raised", [])
            reinterpretations = protocol_execution.get("reinterpretations_made", [])
            if challenges:
                lines.append(f"**æå‡ºæŒ‘æˆ˜**: {len(challenges)}é¡¹")
            if reinterpretations:
                lines.append(f"**é‡æ–°è§£é‡Š**: {len(reinterpretations)}é¡¹")
            lines.append("")
        
        # éªŒè¯æ¸…å•
        validation_checklist = structured_output.get("validation_checklist", [])
        if validation_checklist:
            met_criteria = sum(1 for item in validation_checklist if item.get("status") == "met")
            total_criteria = len(validation_checklist)
            lines.append(f"**è´¨é‡éªŒè¯**: {met_criteria}/{total_criteria} é¡¹æ ‡å‡†æ»¡è¶³")
            lines.append("")
        
        # åŸå§‹è¾“å‡ºå¼•ç”¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if result.get("analysis"):
            analysis = result["analysis"]
            if len(analysis) > 200:
                analysis_preview = analysis[:200] + "..."
            else:
                analysis_preview = analysis
            lines.append(f"**åŸå§‹è¾“å‡ºé¢„è§ˆ**: {analysis_preview}")
            lines.append("")
        
        return "\n".join(lines).strip()
    
    def _format_legacy_output(self, agent_id: str, result: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–ä¼ ç»Ÿæ ¼å¼çš„ä¸“å®¶è¾“å‡ºï¼ˆå‘åå…¼å®¹ï¼‰
        """
        role_name = result.get("role_name", result.get("expert_name", agent_id))
        confidence = result.get("confidence", 0.0)

        structured_data = result.get("structured_data", {})
        if isinstance(structured_data, dict):
            structured_str = json.dumps(structured_data, ensure_ascii=False, indent=2)
            if len(structured_str) > 2000:
                structured_str = structured_str[:2000] + "\n... (truncated)"
        else:
            structured_str = str(structured_data)

        narrative = result.get("analysis") or result.get("content") or ""
        if isinstance(narrative, str) and len(narrative) > 2000:
            narrative = narrative[:2000] + "\n... (truncated)"
        if not narrative:
            narrative = "_No narrative summary provided._"

        return "\n".join([
            f"### {role_name} ({agent_id}) - ä¼ ç»Ÿæ ¼å¼",
            f"confidence: {confidence:.2f}",
            "structured_data:",
            structured_str or "{}",
            "narrative_summary:",
            narrative,
        ])
    
    def _parse_final_report(self, llm_response: str, state: ProjectAnalysisState) -> Dict[str, Any]:
        """è§£ææœ€ç»ˆæŠ¥å‘Šç»“æ„"""
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            start_idx = llm_response.find('{')
            end_idx = llm_response.rfind('}') + 1
            
            if start_idx != -1 and end_idx > start_idx:
                json_str = llm_response[start_idx:end_idx]
                final_report = json.loads(json_str)
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œåˆ›å»ºåŸºç¡€ç»“æ„
                final_report = self._create_fallback_report(llm_response, state)
            
            # æ·»åŠ å…ƒæ•°æ®
            final_report["metadata"] = {
                "generated_at": datetime.now().isoformat(),
                "session_id": state.get("session_id"),
                "total_agents": len(state.get("agent_results", {})),
                "overall_confidence": self._calculate_overall_confidence(state),
                "estimated_pages": self._estimate_report_pages(final_report)
            }
            
            return final_report
            
        except json.JSONDecodeError:
            logger.warning("Failed to parse JSON from final report, using fallback structure")
            return self._create_fallback_report(llm_response, state)
    
    def _create_fallback_report(self, content: str, state: ProjectAnalysisState) -> Dict[str, Any]:
        """åˆ›å»ºå¤‡ç”¨çš„æŠ¥å‘Šç»“æ„"""
        structured_requirements = state.get("structured_requirements", {})
        agent_results = state.get("agent_results", {})
        active_agents = state.get("active_agents", [])

        # âœ… P1ä¿®å¤: ä½¿ç”¨åŠ¨æ€è§’è‰²IDæå–å†…å®¹ï¼Œè€Œéç¡¬ç¼–ç çš„agent key
        # è¾…åŠ©å‡½æ•°ï¼šæ ¹æ®åŠ¨æ€role_idæå–å†…å®¹
        def extract_content_by_role_prefix(prefix: str) -> Dict[str, Any]:
            """æ ¹æ®è§’è‰²å‰ç¼€ä»åŠ¨æ€role_idä¸­æå–structured_data"""
            for role_id in active_agents:
                if role_id.startswith(prefix):
                    agent_result = agent_results.get(role_id, {})
                    if isinstance(agent_result, dict):
                        return agent_result.get("structured_data", agent_result.get("content", {}))
            return {}

        # è¾…åŠ©å‡½æ•°ï¼šæ ¹æ®åŠ¨æ€role_idæå–confidence
        def extract_confidence_by_role_prefix(prefix: str) -> float:
            """æ ¹æ®è§’è‰²å‰ç¼€ä»åŠ¨æ€role_idä¸­æå–confidence"""
            for role_id in active_agents:
                if role_id.startswith(prefix):
                    agent_result = agent_results.get(role_id, {})
                    if isinstance(agent_result, dict):
                        return agent_result.get("confidence", 0.0)
            return 0.0

        # ğŸ†• å¯¹æ‰€æœ‰å¯èƒ½åŒ…å« JTBD å…¬å¼çš„å­—æ®µè¿›è¡Œè½¬æ¢
        transformed_requirements = structured_requirements.copy()
        
        # è½¬æ¢ project_overviewï¼ˆæœ€é‡è¦ï¼‰
        if "project_overview" in transformed_requirements:
            transformed_requirements["project_overview"] = transform_jtbd_to_natural_language(
                transformed_requirements["project_overview"]
            )
        
        # è½¬æ¢ project_taskï¼ˆJTBD å…¬å¼æ¥æºï¼‰
        if "project_task" in transformed_requirements:
            transformed_requirements["project_task"] = transform_jtbd_to_natural_language(
                transformed_requirements["project_task"]
            )
        
        # è½¬æ¢ core_objectivesï¼ˆå¯èƒ½åŒ…å« JTBD å…¬å¼ï¼‰
        if "core_objectives" in transformed_requirements and isinstance(transformed_requirements["core_objectives"], list):
            transformed_requirements["core_objectives"] = [
                transform_jtbd_to_natural_language(obj) if isinstance(obj, str) else obj
                for obj in transformed_requirements["core_objectives"]
            ]

        # ğŸ”¥ v7.26.2: æå–ç”¨æˆ·æ ¸å¿ƒé—®é¢˜å’Œäº¤ä»˜ç‰©
        user_input = state.get("user_input", "")
        user_question = user_input[:100] + "..." if len(user_input) > 100 else user_input
        
        # ä»ä¸“å®¶ç»“æœä¸­æå–äº¤ä»˜ç‰©åç§°
        deliverable_names = []
        for role_id in active_agents:
            if any(role_id.startswith(prefix) for prefix in ["V2_", "V3_", "V4_", "V5_", "V6_"]):
                agent_result = agent_results.get(role_id, {})
                if isinstance(agent_result, dict):
                    structured = agent_result.get("structured_data", {})
                    if isinstance(structured, dict):
                        ter = structured.get("task_execution_report", {})
                        if isinstance(ter, dict):
                            outputs = ter.get("deliverable_outputs", [])
                            for output in outputs:
                                if isinstance(output, dict):
                                    name = output.get("deliverable_name", output.get("name", ""))
                                    if name and name not in deliverable_names:
                                        deliverable_names.append(name)
        
        if not deliverable_names:
            deliverable_names = ["ç»¼åˆåˆ†ææŠ¥å‘Š", "ä¸“å®¶å»ºè®®æ±‡æ€»"]
        
        return {
            "executive_summary": {
                "project_overview": transform_jtbd_to_natural_language(
                    structured_requirements.get("project_overview", "")
                ),
                "key_findings": ["åŸºäºå¤šæ™ºèƒ½ä½“åˆ†æçš„ç»¼åˆå‘ç°"],
                "key_recommendations": ["åŸºäºåˆ†æç»“æœçš„æ ¸å¿ƒå»ºè®®"],
                "success_factors": ["é¡¹ç›®æˆåŠŸçš„å…³é”®è¦ç´ "]
            },
            # ğŸ”¥ v7.26.2: æ·»åŠ  core_answer å­—æ®µï¼ˆfallback è·¯å¾„å¿…é¡»ï¼‰
            "core_answer": {
                "question": user_question or "ç”¨æˆ·å’¨è¯¢é—®é¢˜",
                "answer": structured_requirements.get("project_overview", "è¯·æŸ¥çœ‹å„ä¸“å®¶çš„è¯¦ç»†åˆ†ææŠ¥å‘Š"),
                "deliverables": deliverable_names[:5],
                "timeline": "è¯·å‚è€ƒå·¥ç¨‹å¸ˆä¸“å®¶çš„å®æ–½è§„åˆ’",
                "budget_range": "è¯·å‚è€ƒå·¥ç¨‹å¸ˆä¸“å®¶çš„æˆæœ¬ä¼°ç®—"
            },
            # ğŸ”¥ v7.26.2: æ·»åŠ  insights å­—æ®µï¼ˆfallback è·¯å¾„å¿…é¡»ï¼‰
            "insights": {
                "key_insights": [
                    structured_requirements.get("project_overview", "åŸºäºç”¨æˆ·éœ€æ±‚çš„ç»¼åˆåˆ†æ"),
                    "è¯·æŸ¥çœ‹å„ä¸“å®¶æŠ¥å‘Šè·å–è¯¦ç»†æ´å¯Ÿ"
                ],
                "cross_domain_connections": ["è®¾è®¡ä¸å•†ä¸šçš„æ•´åˆåˆ†æ"],
                "user_needs_interpretation": structured_requirements.get("project_task", "ç”¨æˆ·éœ€æ±‚çš„æ·±åº¦è§£è¯»")
            },
            # ğŸ”¥ v7.26.2: æ·»åŠ  deliberation_process å­—æ®µï¼ˆfallback è·¯å¾„å¿…é¡»ï¼‰
            "deliberation_process": {
                "strategic_thinking": "åŸºäºç”¨æˆ·éœ€æ±‚è¿›è¡Œå¤šç»´åº¦åˆ†æ",
                "role_selection_rationale": f"é€‰æ‹©äº† {len(active_agents)} ä½ä¸“å®¶è¿›è¡ŒååŒåˆ†æ",
                "inquiry_architecture": "æ·±åº¦ä¼˜å…ˆæ¢è¯¢"
            },
            "sections": {
                ReportSection.REQUIREMENTS_ANALYSIS.value: {
                    "title": "éœ€æ±‚åˆ†æ",
                    "content": transformed_requirements,
                    "confidence": 0.8
                },
                ReportSection.DESIGN_RESEARCH.value: {
                    "title": "è®¾è®¡ç ”ç©¶",
                    "content": extract_content_by_role_prefix("V4_"),  # V4æ˜¯è®¾è®¡ç ”ç©¶
                    "confidence": extract_confidence_by_role_prefix("V4_")
                },
                ReportSection.TECHNICAL_ARCHITECTURE.value: {
                    "title": "æŠ€æœ¯æ¶æ„",
                    "content": extract_content_by_role_prefix("V2_"),  # V2æ˜¯è®¾è®¡æ€»ç›‘/æŠ€æœ¯
                    "confidence": extract_confidence_by_role_prefix("V2_")
                },
                ReportSection.UX_DESIGN.value: {
                    "title": "ç”¨æˆ·ä½“éªŒè®¾è®¡",
                    "content": extract_content_by_role_prefix("V3_"),  # V3æ˜¯å™äº‹/ä½“éªŒ
                    "confidence": extract_confidence_by_role_prefix("V3_")
                },
                ReportSection.BUSINESS_MODEL.value: {
                    "title": "å•†ä¸šæ¨¡å¼",
                    "content": extract_content_by_role_prefix("V5_"),  # V5æ˜¯åœºæ™¯ä¸“å®¶
                    "confidence": extract_confidence_by_role_prefix("V5_")
                },
                ReportSection.IMPLEMENTATION_PLAN.value: {
                    "title": "å®æ–½è§„åˆ’",
                    "content": extract_content_by_role_prefix("V6_"),  # V6æ˜¯å·¥ç¨‹å¸ˆ
                    "confidence": extract_confidence_by_role_prefix("V6_")
                }
            },
            "comprehensive_analysis": {
                "cross_domain_insights": ["è·¨é¢†åŸŸçš„å…³é”®æ´å¯Ÿ"],
                "integration_recommendations": ["æ•´åˆå»ºè®®"],
                "risk_assessment": ["é£é™©è¯„ä¼°"],
                "implementation_roadmap": ["å®æ–½è·¯çº¿å›¾"]
            },
            "conclusions": {
                "summary": "é¡¹ç›®åˆ†ææ€»ç»“",
                "next_steps": ["ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®"],
                "success_metrics": ["æˆåŠŸæŒ‡æ ‡"]
            },
            "raw_content": content
        }
    
    def _has_placeholder_content(self, expert_reports: Dict[str, str]) -> bool:
        """
        æ£€æµ‹ expert_reports å†…å®¹æ˜¯å¦ä¸ºå ä½ç¬¦
        
        LLM æœ‰æ—¶ä¼šè¿”å›ç±»ä¼¼ "{...å†…å®¹ç•¥ï¼Œè¯¦è§åŸå§‹è¾“å…¥...}" çš„å ä½ç¬¦æ–‡æœ¬
        è¿™ç§æƒ…å†µéœ€è¦ç”¨çœŸå®æ•°æ®è¦†ç›–
        
        Returns:
            True å¦‚æœåŒ…å«å ä½ç¬¦å†…å®¹ï¼ŒFalse å¦‚æœå†…å®¹æœ‰æ•ˆ
        """
        if not expert_reports:
            return True
            
        placeholders = [
            "...å†…å®¹ç•¥",
            "å†…å®¹ç•¥ï¼Œè¯¦è§",
            "è¯¦è§åŸå§‹è¾“å…¥",
            "...(truncated)",
            "æš‚æ— æŠ¥å‘Šå†…å®¹",
            "(omitted)",
            "çœç•¥",
            "ç•¥..."
        ]
        
        for role_id, content in expert_reports.items():
            if isinstance(content, str):
                for placeholder in placeholders:
                    if placeholder in content:
                        logger.debug(f"Detected placeholder in {role_id}: '{placeholder}'")
                        return True
        
        return False

    def _extract_expert_reports(self, state: ProjectAnalysisState) -> Dict[str, str]:
        """
        æå–ä¸“å®¶åŸå§‹æŠ¥å‘Šç”¨äºé™„å½•

        è¿”å›æ ¼å¼ï¼š
        {
            "V2_è®¾è®¡æ€»ç›‘_2-1": "å®Œæ•´çš„åŸå§‹æŠ¥å‘Šå†…å®¹...",
            "V3_äººç‰©åŠå™äº‹ä¸“å®¶_3-1": "å®Œæ•´çš„åŸå§‹æŠ¥å‘Šå†…å®¹...",
            ...
        }

        âœ… ä¿®å¤: æ”¯æŒ Dynamic Modeï¼Œä½¿ç”¨åŠ¨æ€è§’è‰² IDï¼ˆå¦‚ "V2_è®¾è®¡æ€»ç›‘_2-1"ï¼‰
        âœ… ä¼˜åŒ–: ä½¿ç”¨ dynamic_role_name ä½œä¸ºæ˜¾ç¤ºåç§°ï¼ˆå¦‚ "V5_å•†ä¸šé›¶å”®è¿è¥ä¸“å®¶_5-2"ï¼‰
        """
        agent_results = state.get("agent_results", {})
        active_agents = state.get("active_agents", [])
        strategic_analysis = state.get("strategic_analysis", {})
        selected_roles = strategic_analysis.get("selected_roles", []) if isinstance(strategic_analysis, dict) else []
        expert_reports = {}
        
        # ğŸ”§ æ„å»º role_id -> dynamic_role_name çš„æ˜ å°„
        role_display_names = {}
        for role in selected_roles:
            if isinstance(role, dict):
                rid = role.get("role_id", "")
                dynamic_name = role.get("dynamic_role_name", "")
                if rid and dynamic_name:
                    role_display_names[rid] = dynamic_name

        logger.debug(f"ğŸ” Extracting expert reports from {len(active_agents)} active agents")
        logger.debug(f"ğŸ“‹ Role display names mapping: {role_display_names}")

        for role_id in active_agents:
            # è·³è¿‡éœ€æ±‚åˆ†æå¸ˆå’Œé¡¹ç›®æ€»ç›‘ï¼ˆä¸éœ€è¦åœ¨é™„å½•ä¸­ï¼‰
            if role_id in ["requirements_analyst", "project_director"]:
                continue

            # åªæå– V2-V6 ä¸“å®¶çš„æŠ¥å‘Š
            if not any(role_id.startswith(prefix) for prefix in ["V2_", "V3_", "V4_", "V5_", "V6_"]):
                continue

            agent_result = agent_results.get(role_id, {})
            if agent_result:
                # æå–å®Œæ•´çš„å†…å®¹
                content = agent_result.get("content", "")
                structured_data = agent_result.get("structured_data", {})

                # ğŸ”¥ v7.9.2: æ™ºèƒ½æå–å®é™…å†…å®¹,ä¸å‰ç«¯é€»è¾‘ä¸€è‡´
                # æ£€æµ‹ TaskOrientedExpertOutput ç»“æ„,æå– deliverable_outputs
                if structured_data and isinstance(structured_data, dict):
                    # æ£€æŸ¥æ˜¯å¦æœ‰ task_execution_report åµŒå¥—ç»“æ„
                    ter = structured_data.get("task_execution_report")
                    if ter and isinstance(ter, dict):
                        deliverable_outputs = ter.get("deliverable_outputs")
                        if deliverable_outputs and isinstance(deliverable_outputs, list):
                            # åªæå–äº¤ä»˜ç‰©å†…å®¹,å¿½ç•¥å…ƒæ•°æ®
                            extracted_content = {
                                "deliverable_outputs": deliverable_outputs
                            }
                            # å¯é€‰: æ·»åŠ é¢å¤–ä¿¡æ¯(ä½†ä¸åŒ…æ‹¬å…ƒæ•°æ®)
                            if ter.get("task_completion_summary"):
                                extracted_content["task_completion_summary"] = ter["task_completion_summary"]
                            if ter.get("additional_insights"):
                                extracted_content["additional_insights"] = ter["additional_insights"]
                            if ter.get("execution_challenges"):
                                extracted_content["execution_challenges"] = ter["execution_challenges"]

                            report_content = json.dumps(extracted_content, ensure_ascii=False, indent=2)
                        else:
                            # æ²¡æœ‰ deliverable_outputs,ä½¿ç”¨æ•´ä¸ª structured_data
                            report_content = json.dumps(structured_data, ensure_ascii=False, indent=2)
                    else:
                        # æ²¡æœ‰ task_execution_report,ä½¿ç”¨æ•´ä¸ª structured_data
                        report_content = json.dumps(structured_data, ensure_ascii=False, indent=2)
                elif content:
                    report_content = content
                else:
                    report_content = "æš‚æ— æŠ¥å‘Šå†…å®¹"

                # ğŸ”§ ä½¿ç”¨ dynamic_role_name æ„å»ºæ˜¾ç¤ºåç§°
                # æ ¼å¼: "4-1 æ½®ç©é£æ ¼æ¡ˆä¾‹ç ”ç©¶å‘˜"
                display_name = role_id
                
                # ğŸ”¥ v7.25: ä»å®Œæ•´æ ¼å¼ role_id æå–çŸ­æ ¼å¼åç¼€ç”¨äºæŸ¥æ‰¾
                # role_id æ ¼å¼: "V2_è®¾è®¡æ€»ç›‘_2-1" -> çŸ­æ ¼å¼: "2-1"
                # role_display_names çš„ key æ˜¯çŸ­æ ¼å¼ "2-1"
                import re
                suffix_match = re.search(r'(\d+-\d+)$', role_id)
                short_role_id = suffix_match.group(1) if suffix_match else role_id
                
                # å°è¯•ç”¨çŸ­æ ¼å¼æŸ¥æ‰¾ dynamic_role_name
                if short_role_id in role_display_names:
                    dynamic_name = role_display_names[short_role_id]
                    display_name = f"{short_role_id} {dynamic_name}"
                    logger.debug(f"ğŸ¯ [v7.25] ä½¿ç”¨åŠ¨æ€åç§°: {role_id} â†’ {display_name}")
                elif role_id in role_display_names:
                    # å…¼å®¹ï¼šä¹Ÿæ”¯æŒå®Œæ•´æ ¼å¼ä½œä¸º key
                    dynamic_name = role_display_names[role_id]
                    if suffix_match:
                        display_name = f"{short_role_id} {dynamic_name}"
                    else:
                        display_name = dynamic_name

                expert_reports[display_name] = report_content
                logger.debug(f"âœ… Extracted expert report: {display_name} ({len(report_content)} chars)")

        logger.info(f"ğŸ“Š Extracted {len(expert_reports)} expert reports: {list(expert_reports.keys())}")
        return expert_reports

    def _manually_populate_sections(self, state: ProjectAnalysisState) -> List[Dict[str, Any]]:
        """
        æ‰‹åŠ¨ä» agent_results ä¸­æå–å¹¶å¡«å…… sections

        è¿™æ˜¯ä¸€ä¸ªå…œåº•æ–¹æ¡ˆï¼Œå½“ LLM è¿”å›ç©º sections æ—¶ä½¿ç”¨

        âœ… ä¿®å¤: æ”¯æŒ Dynamic Modeï¼Œä½¿ç”¨åŠ¨æ€è§’è‰² IDï¼ˆå¦‚ "V2_è®¾è®¡æ€»ç›‘_2-1"ï¼‰
        âœ… ä¿®å¤: è¿”å›Listè€Œä¸æ˜¯Dict,ä»¥åŒ¹é…æ–°çš„æ•°æ®ç»“æ„
        """
        agent_results = state.get("agent_results", {})
        active_agents = state.get("active_agents", [])
        sections = []  # æ”¹ä¸ºåˆ—è¡¨
        sections_dict: Dict[str, Dict[str, Any]] = {}

        section_order = [
            "requirements_analysis",
            "design_research",
            "technical_architecture",
            "ux_design",
            "business_model",
            "implementation_plan"
        ]

        logger.debug(f"ğŸ” agent_results keys: {list(agent_results.keys())}")
        logger.debug(f"ğŸ” active_agents: {active_agents}")

        # ç« èŠ‚ç±»å‹åˆ°æ ‡é¢˜çš„æ˜ å°„
        section_titles = {
            "requirements_analysis": "éœ€æ±‚åˆ†æ",
            "design_research": "è®¾è®¡ç ”ç©¶",
            "technical_architecture": "æŠ€æœ¯æ¶æ„",
            "ux_design": "ç”¨æˆ·ä½“éªŒè®¾è®¡",
            "business_model": "å•†ä¸šæ¨¡å¼",
            "implementation_plan": "å®æ–½è§„åˆ’"
        }

        # 1. å¤„ç†éœ€æ±‚åˆ†æå¸ˆï¼ˆå›ºå®šé”®åï¼‰
        requirements_result = agent_results.get("requirements_analyst", {})
        if requirements_result and requirements_result.get("structured_data"):
            # å°†structured_dataè½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
            structured_data = requirements_result.get("structured_data", {})
            content_str = json.dumps(structured_data, ensure_ascii=False, indent=2)

            sections_dict["requirements_analysis"] = {
                "section_id": "requirements_analysis",  # æ·»åŠ section_id
                "title": section_titles["requirements_analysis"],
                "content": content_str,  # ä½¿ç”¨å­—ç¬¦ä¸²æ ¼å¼
                "confidence": requirements_result.get("confidence", 0.5)
            }
            logger.debug(f"âœ… Manually populated section: requirements_analysis from requirements_analyst")

        # 2. å¤„ç† V2-V6 ä¸“å®¶ï¼ˆDynamic Mode ä½¿ç”¨åŠ¨æ€è§’è‰² IDï¼‰
        # æ ¹æ®è§’è‰² ID å‰ç¼€æ˜ å°„åˆ°ç« èŠ‚ç±»å‹
        role_prefix_to_section = {
            "V2_": "technical_architecture",
            "V3_": "ux_design",
            "V4_": "design_research",
            "V5_": "business_model",
            "V6_": "implementation_plan"
        }

        for role_id in active_agents:
            # è·³è¿‡éœ€æ±‚åˆ†æå¸ˆï¼ˆå·²å¤„ç†ï¼‰
            if role_id == "requirements_analyst":
                continue

            # æ ¹æ®å‰ç¼€ç¡®å®šç« èŠ‚ç±»å‹
            section_key = None
            for prefix, section_type in role_prefix_to_section.items():
                if role_id.startswith(prefix):
                    section_key = section_type
                    break

            if not section_key:
                logger.warning(f"âš ï¸ Unknown role prefix for {role_id}, skipping")
                continue

            # ä» agent_results ä¸­è·å–æ•°æ®
            agent_result = agent_results.get(role_id, {})
            section_entry = sections_dict.get(section_key)
            if section_entry is None:
                section_entry = {
                    "section_id": section_key,
                    "title": section_titles[section_key],
                    "content": "",
                    "confidence": 0.0
                }
                sections_dict[section_key] = section_entry

            if agent_result and agent_result.get("structured_data"):
                structured_data = agent_result.get("structured_data", {})
                content_str = json.dumps(structured_data, ensure_ascii=False, indent=2)

                if not section_entry["content"] or section_entry["content"].startswith("æš‚æ— "):
                    section_entry["content"] = content_str
                    confidence_value = agent_result.get("confidence", 0.5)
                    try:
                        section_entry["confidence"] = float(confidence_value)
                    except (TypeError, ValueError):
                        section_entry["confidence"] = 0.5
                else:
                    logger.debug(
                        "â„¹ï¸ Section {} already populated, skipping additional content from {}",
                        section_key,
                        role_id,
                    )
                logger.debug(f"âœ… Manually populated section: {section_key} from {role_id}")
            else:
                if not section_entry["content"]:
                    section_entry["content"] = f"æš‚æ— {section_titles[section_key]}æ•°æ®"
                    section_entry["confidence"] = 0.0
                logger.warning(f"âš ï¸ No data found for {role_id}, created empty section for {section_key}")

        for section_id in section_order:
            if section_id in sections_dict:
                sections.append(sections_dict[section_id])

        for section_id, payload in sections_dict.items():
            if section_id not in section_order:
                sections.append(payload)

        logger.info(
            "ğŸ“Š Manually populated %d sections: %s",
            len(sections),
            [section["section_id"] for section in sections],
        )
        return sections

    def _extract_challenge_resolutions(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        ğŸ†• v3.5.1: æå–æŒ‘æˆ˜è§£å†³ç»“æœ
        
        ä»stateä¸­æå–ä¸“å®¶æŒ‘æˆ˜çš„é—­ç¯å¤„ç†ç»“æœï¼Œç”¨äºæŠ¥å‘Šç”Ÿæˆ
        
        Returns:
            {
                "accepted_reinterpretations": [...],  # Acceptå†³ç­–çš„ç»“æœ
                "synthesized_frameworks": [...],      # Synthesizeå†³ç­–çš„ç»“æœ
                "escalated_to_client": [...],         # Escalateå†³ç­–çš„ç»“æœ
                "summary": {...}                      # ç»Ÿè®¡æ‘˜è¦
            }
        """
        # æå–å„ç±»é—­ç¯ç»“æœ
        expert_driven_insights = state.get("expert_driven_insights", {})
        framework_synthesis = state.get("framework_synthesis", {})
        escalated_challenges = state.get("escalated_challenges", [])
        
        # ç»Ÿè®¡
        accepted_count = len(expert_driven_insights)
        synthesized_count = len(framework_synthesis)
        escalated_count = len(escalated_challenges)
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•æŒ‘æˆ˜è§£å†³ï¼Œè¿”å›ç©ºç»“æ„
        if accepted_count == 0 and synthesized_count == 0 and escalated_count == 0:
            return {
                "has_challenges": False,
                "summary": "æ‰€æœ‰ä¸“å®¶æ¥å—éœ€æ±‚åˆ†æå¸ˆçš„æ´å¯Ÿï¼Œæ— æŒ‘æˆ˜æ ‡è®°"
            }
        
        # æ ¼å¼åŒ–Acceptå†³ç­–ç»“æœ
        accepted_reinterpretations = []
        for item, insight in expert_driven_insights.items():
            accepted_reinterpretations.append({
                "challenged_item": item,
                "expert": insight.get("accepted_from", "unknown"),
                "reinterpretation": insight.get("expert_reinterpretation", ""),
                "design_impact": insight.get("design_impact", ""),
                "timestamp": insight.get("timestamp", "")
            })
        
        # æ ¼å¼åŒ–Synthesizeå†³ç­–ç»“æœ
        synthesized_frameworks = []
        for item, synthesis in framework_synthesis.items():
            synthesized_frameworks.append({
                "challenged_item": item,
                "competing_count": len(synthesis.get("competing_frames", [])),
                "synthesis_summary": synthesis.get("synthesis_summary", ""),
                "recommendation": synthesis.get("recommendation", "")
            })
        
        # æ ¼å¼åŒ–Escalateå†³ç­–ç»“æœ
        escalated_items = []
        for challenge in escalated_challenges:
            escalated_items.append({
                "issue_id": challenge.get("issue_id", ""),
                "description": challenge.get("description", ""),
                "expert_rationale": challenge.get("expert_rationale", ""),
                "requires_client_decision": challenge.get("requires_client_decision", True)
            })
        
        return {
            "has_challenges": True,
            "accepted_reinterpretations": accepted_reinterpretations,
            "synthesized_frameworks": synthesized_frameworks,
            "escalated_to_client": escalated_items,
            "summary": {
                "total_challenges": accepted_count + synthesized_count + escalated_count,
                "accepted_count": accepted_count,
                "synthesized_count": synthesized_count,
                "escalated_count": escalated_count,
                "closure_rate": f"{(accepted_count + synthesized_count) / max(1, accepted_count + synthesized_count + escalated_count) * 100:.1f}%"
            }
        }

    def _calculate_overall_confidence(self, state: ProjectAnalysisState) -> float:
        """è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦"""
        agent_results = state.get("agent_results", {})

        if not agent_results:
            return 0.0

        confidences = [
            result.get("confidence", 0) for result in agent_results.values()
            if result and isinstance(result.get("confidence"), (int, float))
        ]

        if not confidences:
            return 0.0

        # è®¡ç®—åŠ æƒå¹³å‡ç½®ä¿¡åº¦
        return sum(confidences) / len(confidences)
    
    def _get_expert_distribution(self, agent_results: Dict[str, Any]) -> Dict[str, int]:
        """
        è·å–ä¸“å®¶åˆ†å¸ƒç»Ÿè®¡
        
        Args:
            agent_results: ä¸“å®¶æ‰§è¡Œç»“æœ
            
        Returns:
            æŒ‰ä¸“å®¶å±‚çº§ï¼ˆV2-V6ï¼‰åˆ†ç±»çš„æ•°é‡ç»Ÿè®¡
        """
        distribution = {
            "V2_è®¾è®¡æ€»ç›‘": 0,
            "V3_é¢†åŸŸä¸“å®¶": 0,
            "V4_ç ”ç©¶ä¸“å®¶": 0,
            "V5_åˆ›æ–°ä¸“å®¶": 0,
            "V6_å®æ–½ä¸“å®¶": 0,
        }
        
        for role_id in agent_results.keys():
            if isinstance(role_id, str):
                if role_id.startswith("2-"):
                    distribution["V2_è®¾è®¡æ€»ç›‘"] += 1
                elif role_id.startswith("3-"):
                    distribution["V3_é¢†åŸŸä¸“å®¶"] += 1
                elif role_id.startswith("4-"):
                    distribution["V4_ç ”ç©¶ä¸“å®¶"] += 1
                elif role_id.startswith("5-"):
                    distribution["V5_åˆ›æ–°ä¸“å®¶"] += 1
                elif role_id.startswith("6-"):
                    distribution["V6_å®æ–½ä¸“å®¶"] += 1
        
        # åªè¿”å›æœ‰å€¼çš„åˆ†å¸ƒ
        return {k: v for k, v in distribution.items() if v > 0}

    def _estimate_report_pages(self, report: Dict[str, Any]) -> int:
        """ä¼°ç®—æŠ¥å‘Šé¡µæ•°"""
        # åŸºäºå†…å®¹é‡ä¼°ç®—é¡µæ•°
        total_content = 0

        # è®¡ç®—å„éƒ¨åˆ†å†…å®¹é‡
        # sections ç°åœ¨æ˜¯ List[ReportSectionWithId]ï¼Œä¸æ˜¯å­—å…¸
        sections = report.get("sections", [])
        for section in sections:
            content = section.get("content", "") if isinstance(section, dict) else ""
            total_content += len(str(content))

        # æ·»åŠ å…¶ä»–éƒ¨åˆ†
        total_content += len(str(report.get("executive_summary", {})))
        total_content += len(str(report.get("comprehensive_analysis", {})))
        total_content += len(str(report.get("conclusions", {})))

        # ä¼°ç®—é¡µæ•°ï¼ˆå‡è®¾æ¯é¡µçº¦2000å­—ç¬¦ï¼‰
        estimated_pages = max(10, total_content // 2000)

        return min(estimated_pages, 50)  # é™åˆ¶åœ¨10-50é¡µä¹‹é—´
    
    def _extract_review_feedback(
        self,
        review_result: Dict[str, Any],
        review_history: List[Dict[str, Any]],
        improvement_suggestions: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        æå–å®¡æ ¸åé¦ˆæ•°æ®
        
        Args:
            review_result: å½“å‰å®¡æ ¸ç»“æœ
            review_history: å®¡æ ¸å†å²è®°å½•
            improvement_suggestions: æ”¹è¿›å»ºè®®åˆ—è¡¨
        
        Returns:
            æ ¼å¼åŒ–çš„å®¡æ ¸åé¦ˆæ•°æ®
        """
        from datetime import datetime
        
        red_team_challenges = []
        blue_team_validations = []
        judge_rulings = []
        client_decisions = []
        
        # ä»å®¡æ ¸å†å²ä¸­æå–æ•°æ®ï¼ˆæ”¯æŒå¤šè½®å®¡æ ¸ï¼‰
        for round_data in review_history:
            round_num = round_data.get("round", 1)
            
            # æå–çº¢é˜Ÿè´¨ç–‘ç‚¹
            red_review = round_data.get("red_team_review", {})
            if isinstance(red_review, dict):
                improvements = red_review.get("improvements", [])
                for imp in improvements:
                    red_team_challenges.append({
                        "issue_id": imp.get("issue_id", f"R{round_num}-{len(red_team_challenges)+1}"),
                        "reviewer": f"çº¢é˜Ÿï¼ˆç¬¬{round_num}è½®ï¼‰",
                        "issue_type": "é£é™©",
                        "description": imp.get("issue", ""),
                        "response": imp.get("suggested_action", "å¾…å¤„ç†"),
                        "status": "å·²ä¿®å¤" if imp.get("issue_id") in [s.get("issue_id") for s in improvement_suggestions] else "å¾…å¤„ç†",
                        "priority": imp.get("priority", "medium")
                    })
            
            # æå–è“é˜ŸéªŒè¯ç»“æœ
            blue_review = round_data.get("blue_team_review", {})
            if isinstance(blue_review, dict):
                # âœ… è“é˜Ÿæœ‰ä¸¤ç§æ•°æ®ï¼škeep_as_isï¼ˆä¼˜åŠ¿ï¼‰å’Œ enhancementsï¼ˆä¼˜åŒ–å»ºè®®ï¼‰
                keep_as_is = blue_review.get("keep_as_is", [])
                for item in keep_as_is:
                    blue_team_validations.append({
                        "issue_id": item.get("red_issue_id", f"B{round_num}-{len(blue_team_validations)+1}"),
                        "reviewer": f"è“é˜Ÿï¼ˆç¬¬{round_num}è½®ï¼‰",
                        "issue_type": "éªŒè¯",
                        "description": item.get("defense", ""),
                        "response": item.get("evidence", "å·²éªŒè¯"),
                        "status": "å·²éªŒè¯",
                        "priority": "medium"
                    })
                
                enhancements = blue_review.get("enhancements", [])
                for enh in enhancements:
                    blue_team_validations.append({
                        "issue_id": enh.get("enhancement_id", f"B{round_num}-{len(blue_team_validations)+1}"),
                        "reviewer": f"è“é˜Ÿï¼ˆç¬¬{round_num}è½®ï¼‰",
                        "issue_type": "ä¼˜åŒ–",
                        "description": enh.get("enhancement", ""),
                        "response": enh.get("value_add", "å·²é‡‡çº³"),
                        "status": "å·²é‡‡çº³",
                        "priority": enh.get("priority", "medium")
                    })
            
            # æå–è¯„å§”è£å†³
            judge_review = round_data.get("judge_review", {})
            if isinstance(judge_review, dict):
                prioritized = judge_review.get("prioritized_improvements", [])
                for item in prioritized:
                    # âœ… æ­£ç¡®çš„å­—æ®µæ˜ å°„ï¼štask -> description, rationale -> response
                    judge_rulings.append({
                        "issue_id": item.get("issue_id", f"J{round_num}-{len(judge_rulings)+1}"),
                        "reviewer": f"è¯„å§”ï¼ˆç¬¬{round_num}è½®ï¼‰",
                        "issue_type": "å»ºè®®",
                        "description": item.get("task", ""),  # âœ… ä» task å­—æ®µæå–
                        "response": item.get("rationale", ""),  # âœ… ä» rationale å­—æ®µæå–
                        "status": "å·²ä¿®å¤" if item.get("priority", 999) <= 2 else "å¾…å®š",  # âœ… priority æ˜¯æ•°å­—
                        "priority": "high" if item.get("priority", 999) <= 2 else "medium"
                    })
            
            # æå–ç”²æ–¹å†³ç­–
            client_review = round_data.get("client_review", {})
            if isinstance(client_review, dict):
                accepted = client_review.get("accepted_improvements", [])
                for acc in accepted:
                    client_decisions.append({
                        "issue_id": acc.get("issue_id", f"C{round_num}-{len(client_decisions)+1}"),
                        "reviewer": f"ç”²æ–¹ï¼ˆç¬¬{round_num}è½®ï¼‰",
                        "issue_type": "å†³ç­–",
                        "description": acc.get("issue", ""),
                        "response": acc.get("implementation_plan", ""),
                        "status": "å·²é‡‡çº³",
                        "priority": acc.get("priority", "high")
                    })
        
        # ç”Ÿæˆè¿­ä»£æ€»ç»“
        total_issues = len(red_team_challenges)
        resolved_issues = len([x for x in red_team_challenges if x["status"] == "å·²ä¿®å¤"])
        iteration_summary = f"""
## å®¡æ ¸è¿­ä»£è¿‡ç¨‹æ€»ç»“

**æ€»å®¡æ ¸è½®æ¬¡**: {len(review_history)}è½®
**çº¢é˜Ÿå‘ç°é—®é¢˜æ•°**: {total_issues}ä¸ª
**è“é˜ŸéªŒè¯ä¼˜åŒ–ç‚¹**: {len(blue_team_validations)}ä¸ª
**è¯„å§”è£å†³é¡¹ç›®**: {len(judge_rulings)}ä¸ª
**ç”²æ–¹æœ€ç»ˆå†³ç­–**: {len(client_decisions)}ä¸ª
**é—®é¢˜è§£å†³ç‡**: {resolved_issues}/{total_issues} ({resolved_issues/max(1, total_issues)*100:.1f}%)

### æ”¹è¿›æ•ˆæœ
é€šè¿‡å¤šè½®å®¡æ ¸ï¼Œé¡¹ç›®è´¨é‡æ˜¾è‘—æå‡ï¼š
- é£é™©è¯†åˆ«ä¸åº”å¯¹ï¼šçº¢é˜Ÿå‘ç°çš„{total_issues}ä¸ªæ½œåœ¨é£é™©ä¸­ï¼Œ{resolved_issues}ä¸ªå·²å®Œæˆä¿®å¤
- ä»·å€¼å¢å¼ºï¼šè“é˜Ÿæå‡ºçš„{len(blue_team_validations)}ä¸ªä¼˜åŒ–å»ºè®®å·²å…¨éƒ¨é‡‡çº³
- ä¸“ä¸šè£å†³ï¼šè¯„å§”å›¢é˜Ÿæä¾›{len(judge_rulings)}ä¸ªä¸“ä¸šå»ºè®®ï¼Œç¡®ä¿æŠ€æœ¯æ–¹æ¡ˆå¯è¡Œæ€§
- æœ€ç»ˆå†³ç­–ï¼šç”²æ–¹å®¡æ ¸é€šè¿‡{len(client_decisions)}ä¸ªå…³é”®æ”¹è¿›æªæ–½

### å…³é”®æ”¹è¿›äº®ç‚¹
{self._format_key_improvements(improvement_suggestions[:3])}
"""
        
        return {
            "red_team_challenges": red_team_challenges,
            "blue_team_validations": blue_team_validations,
            "judge_rulings": judge_rulings,
            "client_decisions": client_decisions,
            "iteration_summary": iteration_summary.strip()
        }
    
    def _extract_questionnaire_data(
        self,
        calibration_questionnaire: Dict[str, Any],
        questionnaire_responses: Dict[str, Any],
        questionnaire_summary: Optional[Dict[str, Any]] = None
    ) -> Optional[Dict[str, Any]]:
        """
        æå–é—®å·å›ç­”æ•°æ®

        Args:
            calibration_questionnaire: æ ¡å‡†é—®å·
            questionnaire_responses: ç”¨æˆ·å›ç­”

        Returns:
            æ ¼å¼åŒ–çš„é—®å·æ•°æ®ï¼Œå¦‚æœæ‰€æœ‰é—®é¢˜éƒ½æœªå›ç­”åˆ™è¿”å› None

        ğŸ”§ ä¿®å¤: è¿‡æ»¤æ‰æœªå›ç­”çš„é—®é¢˜ï¼Œé¿å…å‰ç«¯æ˜¾ç¤º"æœªå›ç­”"
        ğŸ”§ v7.5 ä¿®å¤: å¢åŠ å¯¹ entries ä¸­ answer å­—æ®µçš„æ”¯æŒï¼ˆå‰ç«¯æäº¤æ ¼å¼ï¼‰
        """
        from datetime import datetime

        summary_entries = []
        if questionnaire_summary and questionnaire_summary.get("entries"):
            summary_entries = questionnaire_summary.get("entries", [])
        elif questionnaire_responses.get("entries"):
            summary_entries = questionnaire_responses.get("entries", [])

        responses = []

        if summary_entries:
            logger.debug(f"ğŸ” [é—®å·æå–] ä» entries æå–ï¼Œå…± {len(summary_entries)} é¡¹")
            for idx, entry in enumerate(summary_entries, 1):
                # ğŸ”§ v7.5 ä¿®å¤: åŒæ—¶æ£€æŸ¥ value å’Œ answer å­—æ®µï¼ˆå‰ç«¯æäº¤æ ¼å¼å…¼å®¹ï¼‰
                answer_value = entry.get("value") or entry.get("answer")
                # ğŸ”§ ä¿®å¤: è·³è¿‡æœªå›ç­”çš„é—®é¢˜
                if answer_value is None or answer_value == "" or answer_value == []:
                    continue

                answer_str = self._stringify_answer(answer_value)
                # ğŸ”§ ä¿®å¤: å†æ¬¡æ£€æŸ¥æ ¼å¼åŒ–åçš„ç­”æ¡ˆ
                if answer_str == "æœªå›ç­”" or answer_str == "":
                    continue

                responses.append({
                    "question_id": entry.get("id") or entry.get("question_id", f"Q{idx}"),
                    "question": entry.get("question", ""),
                    "answer": answer_str,
                    "context": entry.get("context", "")
                })
        else:
            questions = calibration_questionnaire.get("questions", [])
            answers = questionnaire_responses.get("answers", {})
            logger.debug(f"ğŸ” [é—®å·æå–] ä» questions/answers æå–ï¼Œ{len(questions)} é—®é¢˜ï¼Œ{len(answers)} ç­”æ¡ˆ")
            
            for idx, q in enumerate(questions, 1):
                question_id = q.get("id", f"Q{idx}")
                raw_answer = (
                    answers.get(question_id)
                    or answers.get(f"q{idx}")
                )

                # ğŸ”§ ä¿®å¤: è·³è¿‡æœªå›ç­”çš„é—®é¢˜
                if raw_answer is None or raw_answer == "" or raw_answer == []:
                    continue

                answer_str = self._stringify_answer(raw_answer)
                # ğŸ”§ ä¿®å¤: å†æ¬¡æ£€æŸ¥æ ¼å¼åŒ–åçš„ç­”æ¡ˆ
                if answer_str == "æœªå›ç­”" or answer_str == "":
                    continue

                responses.append({
                    "question_id": question_id,
                    "question": q.get("question", ""),
                    "answer": answer_str,
                    "context": q.get("context", "")
                })

        # ğŸ”§ ä¿®å¤: å¦‚æœæ‰€æœ‰é—®é¢˜éƒ½æœªå›ç­”ï¼Œè¿”å› Noneï¼ˆå‰ç«¯ä¼šéšè—æ•´ä¸ªé—®å·åŒºå—ï¼‰
        if not responses:
            logger.info("ğŸ“‹ æ‰€æœ‰é—®å·é—®é¢˜éƒ½æœªå›ç­”ï¼Œè¿”å› Noneï¼ˆå‰ç«¯å°†éšè—é—®å·åŒºå—ï¼‰")
            return None

        timestamp = (
            (questionnaire_summary or {}).get("submitted_at")
            or questionnaire_responses.get("submitted_at")
            or questionnaire_responses.get("timestamp")
            or datetime.now().isoformat()
        )

        notes = (questionnaire_summary or {}).get("notes") or questionnaire_responses.get("notes", "")

        # ç”Ÿæˆæ´å¯Ÿåˆ†æ
        analysis_insights = self._analyze_questionnaire_insights(responses)

        logger.info(f"âœ… æå–åˆ° {len(responses)} ä¸ªæœ‰æ•ˆé—®å·å›ç­”")

        return {
            "responses": responses,
            "timestamp": timestamp,
            "notes": notes,
            "analysis_insights": analysis_insights
        }

    @staticmethod
    def _stringify_answer(value: Any) -> str:
        """å°†é—®å·ç­”æ¡ˆæ ¼å¼åŒ–ä¸ºæ˜“è¯»å­—ç¬¦ä¸²"""
        if value is None:
            return "æœªå›ç­”"

        if isinstance(value, (list, tuple, set)):
            cleaned = [str(item).strip() for item in value if str(item).strip()]
            return "ã€".join(cleaned) if cleaned else "æœªå›ç­”"

        if isinstance(value, dict):
            try:
                serialized = json.dumps(value, ensure_ascii=False)
            except (TypeError, ValueError):
                serialized = str(value)
            return serialized.strip() or "æœªå›ç­”"

        text = str(value).strip()
        return text or "æœªå›ç­”"
    
    def _extract_visualization_data(
        self,
        review_history: List[Dict[str, Any]],
        review_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        æå–å¯è§†åŒ–æ•°æ®
        
        Args:
            review_history: å®¡æ ¸å†å²è®°å½•
            review_result: æœ€ç»ˆå®¡æ ¸ç»“æœ
        
        Returns:
            æ ¼å¼åŒ–çš„å¯è§†åŒ–æ•°æ®
        """
        from datetime import datetime
        
        rounds = []
        for round_data in review_history:
            round_num = round_data.get("round", 1)
            
            # æå–å„æ–¹è¯„åˆ†
            red_review = round_data.get("red_team_review", {})
            blue_review = round_data.get("blue_team_review", {})
            judge_review = round_data.get("judge_review", {})
            
            red_score = red_review.get("score", 0) if isinstance(red_review, dict) else 0
            blue_score = blue_review.get("score", 0) if isinstance(blue_review, dict) else 0
            judge_score = judge_review.get("score", 0) if isinstance(judge_review, dict) else 0
            
            issues_found = len(red_review.get("improvements", [])) if isinstance(red_review, dict) else 0
            issues_resolved = len(blue_review.get("enhancements", [])) if isinstance(blue_review, dict) else 0
            
            rounds.append({
                "round_number": round_num,
                "red_score": red_score,
                "blue_score": blue_score,
                "judge_score": judge_score,
                "issues_found": issues_found,
                "issues_resolved": issues_resolved,
                "timestamp": round_data.get("timestamp", datetime.now().isoformat())
            })
        
        # è®¡ç®—æ”¹è¿›ç‡
        if rounds:
            first_round_score = rounds[0]["red_score"]
            last_round_score = rounds[-1]["judge_score"]
            improvement_rate = (last_round_score - first_round_score) / max(1, first_round_score)
        else:
            improvement_rate = 0.0
        
        # è·å–æœ€ç»ˆå†³ç­–
        client_review = review_result.get("client_review", {})
        final_decision = "é€šè¿‡"
        if isinstance(client_review, dict):
            decision = client_review.get("final_decision", "approved")
            if decision == "approved":
                final_decision = "é€šè¿‡"
            elif decision == "conditional_approval":
                final_decision = "æœ‰æ¡ä»¶é€šè¿‡"
            else:
                final_decision = "æ‹’ç»"
        
        return {
            "rounds": rounds,
            "final_decision": final_decision,
            "total_rounds": len(rounds),
            "improvement_rate": round(improvement_rate, 2)
        }
    
    def _format_key_improvements(self, improvements: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–å…³é”®æ”¹è¿›ç‚¹"""
        if not improvements:
            return "æ— éœ€æ”¹è¿›ï¼Œåˆ†æè´¨é‡å·²è¾¾æ ‡ã€‚"
        
        formatted = []
        for idx, imp in enumerate(improvements, 1):
            formatted.append(
                f"{idx}. **{imp.get('issue_id', 'N/A')}**: {imp.get('issue', 'æœªçŸ¥é—®é¢˜')} "
                f"ï¼ˆä¼˜å…ˆçº§: {imp.get('priority', 'medium')}ï¼‰"
            )
        
        return "\n".join(formatted)
    
    def _analyze_questionnaire_insights(self, responses: List[Dict[str, Any]]) -> str:
        """ä»é—®å·å›ç­”ä¸­æå–å…³é”®æ´å¯Ÿ"""
        if not responses:
            return "æ— æœ‰æ•ˆé—®å·æ•°æ®ã€‚"

        insights = [
            "## ç”¨æˆ·éœ€æ±‚å…³é”®æ´å¯Ÿ",
            "",
            f"åŸºäº{len(responses)}ä¸ªé—®é¢˜çš„æ·±å…¥è®¿è°ˆï¼Œæˆ‘ä»¬æå–äº†ä»¥ä¸‹å…³é”®æ´å¯Ÿï¼š",
            ""
        ]

        # ç®€å•åˆ†æï¼šæå–éç©ºå›ç­”çš„æ•°é‡
        answered = [r for r in responses if r.get("answer") and r["answer"] != "æœªå›ç­”"]
        insights.append(f"- **å›ç­”å®Œæ•´åº¦**: {len(answered)}/{len(responses)} ({len(answered)/len(responses)*100:.1f}%)")

        # å¦‚æœæœ‰å›ç­”ï¼Œæå–å‰3ä¸ªå…³é”®å›ç­”
        if answered:
            insights.append("- **å…³é”®å›ç­”**:")
            for r in answered[:3]:
                insights.append(f"  - {r.get('question', 'æœªçŸ¥é—®é¢˜')[:50]}...")
                insights.append(f"    > {r.get('answer', 'æœªçŸ¥å›ç­”')[:100]}...")

        return "\n".join(insights)

    # =========================================================================
    # ğŸ†• v7.0: å¤šäº¤ä»˜ç‰©æ ¸å¿ƒç­”æ¡ˆæå–æ–¹æ³•
    # =========================================================================

    def _extract_deliverable_answers(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        ğŸ†• v7.0: ä»è´£ä»»è€…è¾“å‡ºä¸­æå–äº¤ä»˜ç‰©ç­”æ¡ˆ
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. ä» deliverable_metadata è·å–æ¯ä¸ªäº¤ä»˜ç‰©çš„ owner
        2. ä» agent_results[owner] ä¸­æå–è¯¥ä¸“å®¶çš„è¾“å‡º
        3. ç›´æ¥ä½¿ç”¨ä¸“å®¶è¾“å‡ºä½œä¸ºç­”æ¡ˆï¼Œä¸åšLLMäºŒæ¬¡ç»¼åˆ
        
        Returns:
            {
                "deliverable_answers": [...],  # å„äº¤ä»˜ç‰©çš„è´£ä»»è€…ç­”æ¡ˆ
                "expert_support_chain": [...], # ä¸“å®¶æ”¯æ’‘é“¾
                "question": str,               # ç”¨æˆ·æ ¸å¿ƒé—®é¢˜
                "answer": str,                 # ç»¼åˆæ‘˜è¦ï¼ˆå‘åå…¼å®¹ï¼‰
                "deliverables": [...],         # äº¤ä»˜ç‰©æ¸…å•ï¼ˆå‘åå…¼å®¹ï¼‰
                "timeline": str,
                "budget_range": str
            }
        """
        deliverable_metadata = state.get("deliverable_metadata") or {}
        deliverable_owner_map = state.get("deliverable_owner_map") or {}
        agent_results = state.get("agent_results") or {}
        structured_requirements = state.get("structured_requirements") or {}
        user_input = state.get("user_input", "")
        
        logger.info(f"ğŸ¯ [v7.0] å¼€å§‹æå–äº¤ä»˜ç‰©ç­”æ¡ˆ: {len(deliverable_metadata)} ä¸ªäº¤ä»˜ç‰©")
        logger.debug(f"deliverable_owner_map: {deliverable_owner_map}")
        logger.debug(f"agent_results keys: {list(agent_results.keys())}")
        
        deliverable_answers = []
        expert_support_chain = []
        owners_set = set()  # è®°å½•å·²ä½œä¸ºownerçš„ä¸“å®¶
        
        # 1. æå–æ¯ä¸ªäº¤ä»˜ç‰©çš„è´£ä»»è€…ç­”æ¡ˆ
        for deliverable_id, metadata in deliverable_metadata.items():
            owner_role = metadata.get("owner") or deliverable_owner_map.get(deliverable_id)
            
            if not owner_role:
                logger.warning(f"âš ï¸ äº¤ä»˜ç‰© {deliverable_id} æ— è´£ä»»è€…ï¼Œè·³è¿‡")
                continue
            
            owners_set.add(owner_role)
            
            # åœ¨ agent_results ä¸­æŸ¥æ‰¾åŒ¹é…çš„ä¸“å®¶è¾“å‡º
            # owner_role æ ¼å¼å¯èƒ½æ˜¯ "V2_è®¾è®¡æ€»ç›‘_å®¤å†…ç­–ç•¥æ–¹å‘" è¿™æ ·çš„å®Œæ•´ID
            # agent_results çš„ key å¯èƒ½æ˜¯ "V2_è®¾è®¡æ€»ç›‘_2-1" è¿™æ ·çš„æ ¼å¼
            owner_result = self._find_owner_result(agent_results, owner_role)
            
            if not owner_result:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°è´£ä»»è€… {owner_role} çš„è¾“å‡º")
                owner_answer = f"ï¼ˆ{owner_role} çš„è¾“å‡ºå¾…ç”Ÿæˆï¼‰"
                answer_summary = owner_answer
                quality_score = None
            else:
                owner_answer = self._extract_owner_deliverable_output(owner_result, deliverable_id)
                answer_summary = self._generate_answer_summary(owner_answer)
                quality_score = self._extract_quality_score(owner_result)

            # ğŸ†• v7.108: æå–æ¦‚å¿µå›¾
            concept_images = []
            if owner_result:
                concept_images_data = owner_result.get("concept_images", [])
                # è¿‡æ»¤å‡ºå±äºè¯¥äº¤ä»˜ç‰©çš„æ¦‚å¿µå›¾
                for img_data in concept_images_data:
                    if img_data.get("deliverable_id") == deliverable_id:
                        concept_images.append(img_data)

            deliverable_answer = {
                "deliverable_id": deliverable_id,
                "deliverable_name": metadata.get("name", deliverable_id),
                "deliverable_type": metadata.get("type", "unknown"),
                "owner_role": owner_role,
                "owner_answer": owner_answer,
                "answer_summary": answer_summary,
                "supporters": metadata.get("supporters", []),
                "quality_score": quality_score,
                "concept_images": concept_images  # ğŸ†• v7.108: å…³è”æ¦‚å¿µå›¾
            }
            
            deliverable_answers.append(deliverable_answer)
            logger.info(f"âœ… æå– {deliverable_id} ç­”æ¡ˆ: owner={owner_role}, é•¿åº¦={len(owner_answer)}")
        
        # 2. æ„å»ºä¸“å®¶æ”¯æ’‘é“¾ï¼ˆéownerä¸“å®¶çš„è´¡çŒ®ï¼‰
        active_agents = state.get("active_agents", [])
        for role_id in agent_results.keys():
            # è·³è¿‡éœ€æ±‚åˆ†æå¸ˆã€é¡¹ç›®æ€»ç›‘å’Œå·²ä½œä¸ºownerçš„ä¸“å®¶
            if role_id in ["requirements_analyst", "project_director"]:
                continue
            if role_id in owners_set:
                continue
            if not any(role_id.startswith(prefix) for prefix in ["V2_", "V3_", "V4_", "V5_", "V6_"]):
                continue
            
            agent_result = agent_results.get(role_id, {})
            if not agent_result:
                continue
            
            # æå–è´¡çŒ®ä¿¡æ¯
            contribution = self._extract_supporter_contribution(role_id, agent_result, deliverable_metadata)
            if contribution:
                expert_support_chain.append(contribution)
        
        # 3. æŒ‰ä¾èµ–é¡ºåºæ’åºæ”¯æ’‘é“¾ï¼ˆV4 â†’ V3 â†’ V5 â†’ V6 â†’ V2ï¼‰
        tier_order = {"V4_": 1, "V3_": 2, "V5_": 3, "V6_": 4, "V2_": 5}
        expert_support_chain.sort(key=lambda x: min(
            (tier_order.get(prefix, 99) for prefix in tier_order if x.get("role_id", "").startswith(prefix)),
            default=99
        ))
        
        # 4. ç”Ÿæˆå‘åå…¼å®¹å­—æ®µ
        question = structured_requirements.get("project_task") or user_input[:200] if user_input else "å¾…å®š"
        deliverables_list = [d.get("deliverable_name", d.get("deliverable_id")) for d in deliverable_answers]
        answer_summary = self._generate_combined_summary(deliverable_answers)
        timeline = structured_requirements.get("timeline", "å¾…å®š")
        budget_range = structured_requirements.get("budget_range", "å¾…å®š")
        
        result = {
            "deliverable_answers": deliverable_answers,
            "expert_support_chain": expert_support_chain,
            "question": question,
            "answer": answer_summary,
            "deliverables": deliverables_list,
            "timeline": timeline if isinstance(timeline, str) else "å¾…å®š",
            "budget_range": budget_range if isinstance(budget_range, str) else "å¾…å®š"
        }
        
        logger.info(f"ğŸ¯ [v7.0] æå–å®Œæˆ: {len(deliverable_answers)} ä¸ªäº¤ä»˜ç‰©ç­”æ¡ˆ, {len(expert_support_chain)} ä¸ªæ”¯æ’‘ä¸“å®¶")
        return result

    def _find_owner_result(self, agent_results: Dict[str, Any], owner_role: str) -> Optional[Dict[str, Any]]:
        """
        åœ¨ agent_results ä¸­æŸ¥æ‰¾åŒ¹é…çš„ä¸“å®¶è¾“å‡º
        
        owner_role å¯èƒ½æ˜¯å®Œæ•´æè¿°å¦‚ "V2_è®¾è®¡æ€»ç›‘_å®¤å†…ç­–ç•¥æ–¹å‘"
        agent_results key å¯èƒ½æ˜¯ "V2_è®¾è®¡æ€»ç›‘_2-1"
        éœ€è¦é€šè¿‡å‰ç¼€åŒ¹é…
        """
        # ç²¾ç¡®åŒ¹é…
        if owner_role in agent_results:
            return agent_results[owner_role]
        
        # æå–å‰ç¼€ï¼ˆå¦‚ V2_è®¾è®¡æ€»ç›‘ï¼‰
        parts = owner_role.split("_")
        if len(parts) >= 2:
            prefix = f"{parts[0]}_{parts[1]}"  # V2_è®¾è®¡æ€»ç›‘
            
            # æŸ¥æ‰¾ä»¥æ­¤å‰ç¼€å¼€å¤´çš„ä¸“å®¶
            for key in agent_results.keys():
                if key.startswith(prefix):
                    logger.debug(f"æ‰¾åˆ°åŒ¹é…ä¸“å®¶: {owner_role} -> {key}")
                    return agent_results[key]
        
        return None

    def _extract_owner_deliverable_output(self, owner_result: Dict[str, Any], deliverable_id: str) -> str:
        """
        ä»è´£ä»»è€…è¾“å‡ºä¸­æå–é’ˆå¯¹ç‰¹å®šäº¤ä»˜ç‰©çš„ç­”æ¡ˆ
        
        ä¼˜å…ˆé¡ºåºï¼š
        1. structured_data.task_execution_report.deliverable_outputs ä¸­åŒ¹é…çš„å†…å®¹
        2. structured_output.task_results ä¸­åŒ¹é… deliverable_id çš„å†…å®¹
        3. structured_data ä¸­çš„ä¸»è¦å†…å®¹
        4. analysis æˆ– content å­—æ®µ
        
        ğŸ”§ v7.6: å¢å¼ºå¤„ç†åµŒå¥— JSON å­—ç¬¦ä¸²å’Œé‡å¤å†…å®¹
        """
        if not owner_result:
            return "æš‚æ— è¾“å‡º"
        
        # ğŸ”§ v7.6: ä¼˜å…ˆä» structured_data.task_execution_report.deliverable_outputs æå–
        structured_data = owner_result.get("structured_data", {})
        if structured_data and isinstance(structured_data, dict):
            task_execution_report = structured_data.get("task_execution_report", {})
            if task_execution_report and isinstance(task_execution_report, dict):
                deliverable_outputs = task_execution_report.get("deliverable_outputs", [])
                if deliverable_outputs and isinstance(deliverable_outputs, list):
                    for output in deliverable_outputs:
                        if not isinstance(output, dict):
                            continue
                        output_name = output.get("deliverable_name", "")
                        content = output.get("content", "")
                        
                        if content:
                            # ğŸ”§ å¤„ç†åµŒå¥— JSON å­—ç¬¦ä¸²ï¼ˆLLM å¯èƒ½è¿”å› markdown ä»£ç å—ï¼‰
                            cleaned_content = self._clean_nested_json_content(content)
                            if cleaned_content:
                                logger.debug(f"âœ… ä» deliverable_outputs æå–å†…å®¹: {output_name[:30]}")
                                return cleaned_content
        
        # å°è¯•ä» TaskOrientedExpertOutput ç»“æ„ä¸­æå–
        structured_output = owner_result.get("structured_output", {})
        if structured_output and isinstance(structured_output, dict):
            task_results = structured_output.get("task_results", [])
            for task in task_results:
                if task.get("deliverable_id") == deliverable_id:
                    content = task.get("content", "")
                    if content:
                        return self._clean_nested_json_content(content)
            
            # å¦‚æœæ²¡æœ‰åŒ¹é…çš„ deliverable_idï¼Œè¿”å›ç¬¬ä¸€ä¸ª task çš„å†…å®¹
            if task_results:
                first_task = task_results[0]
                content = first_task.get("content", "")
                if content:
                    return self._clean_nested_json_content(content)
        
        # ä» structured_data ä¸­æå–æ ¸å¿ƒè¾“å‡ºå­—æ®µ
        if structured_data and isinstance(structured_data, dict):
            # å°è¯•æå–æ ¸å¿ƒè¾“å‡ºå­—æ®µ
            for key in ["core_output", "deliverable_output", "main_content", "analysis_result", "recommendation"]:
                if key in structured_data:
                    value = structured_data[key]
                    if isinstance(value, str) and value:
                        return self._clean_nested_json_content(value)
                    elif isinstance(value, dict):
                        return self._format_dict_as_readable(value)
            
            # ğŸ”§ v7.6: ä¸å†å°†æ•´ä¸ª structured_data ä½œä¸º JSON è¿”å›
            # è€Œæ˜¯å°è¯•æå–æœ‰æ„ä¹‰çš„å†…å®¹
            # è·³è¿‡å…ƒæ•°æ®å­—æ®µ
            skip_keys = {"protocol_execution", "execution_metadata", "task_completion_summary", "content"}
            meaningful_data = {k: v for k, v in structured_data.items() if k not in skip_keys and v}
            if meaningful_data:
                return self._format_dict_as_readable(meaningful_data)
        
        # å›é€€åˆ° analysis æˆ– content å­—æ®µ
        analysis = owner_result.get("analysis", "")
        if analysis:
            return self._clean_nested_json_content(analysis)
        
        content = owner_result.get("content", "")
        if content:
            return self._clean_nested_json_content(content)
        
        return "æš‚æ— è¾“å‡º"

    def _clean_nested_json_content(self, content: Any) -> str:
        """
        æ¸…ç†åµŒå¥—çš„ JSON å†…å®¹
        
        å¤„ç† LLM è¿”å›çš„ markdown ä»£ç å—åŒ…è£¹çš„ JSONï¼Œ
        æå–å®é™…æœ‰æ„ä¹‰çš„å†…å®¹è€Œä¸æ˜¯åŸå§‹ JSON å­—ç¬¦ä¸²
        """
        if not content:
            return ""
        
        # å¦‚æœæ˜¯å­—å…¸æˆ–åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå¯è¯»æ ¼å¼
        if isinstance(content, (dict, list)):
            return self._format_dict_as_readable(content)
        
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²
        if isinstance(content, str):
            text = content.strip()
            
            # ç§»é™¤ markdown ä»£ç å—åŒ…è£¹
            if text.startswith("```json"):
                text = text[7:]
            elif text.startswith("```"):
                text = text[3:]
            if text.endswith("```"):
                text = text[:-3]
            text = text.strip()
            
            # å°è¯•è§£æä¸º JSON
            if text.startswith("{") or text.startswith("["):
                try:
                    parsed = json.loads(text)
                    # å¦‚æœè§£ææˆåŠŸï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«åµŒå¥—çš„ task_execution_report
                    if isinstance(parsed, dict):
                        # æå–æœ‰æ„ä¹‰çš„å†…å®¹
                        if "task_execution_report" in parsed:
                            ter = parsed["task_execution_report"]
                            if isinstance(ter, dict) and "deliverable_outputs" in ter:
                                outputs = ter["deliverable_outputs"]
                                if outputs and isinstance(outputs, list):
                                    # é€’å½’æå–ç¬¬ä¸€ä¸ªäº¤ä»˜ç‰©çš„å†…å®¹
                                    first_output = outputs[0]
                                    if isinstance(first_output, dict):
                                        inner_content = first_output.get("content", "")
                                        if inner_content:
                                            return self._clean_nested_json_content(inner_content)
                        # æ ¼å¼åŒ–ä¸ºå¯è¯»å†…å®¹
                        return self._format_dict_as_readable(parsed)
                    elif isinstance(parsed, list):
                        return self._format_dict_as_readable(parsed)
                except json.JSONDecodeError:
                    pass
            
            # è¿”å›æ¸…ç†åçš„æ–‡æœ¬
            return text
        
        return str(content)

    def _format_dict_as_readable(self, data: Any, indent: int = 0) -> str:
        """
        å°†å­—å…¸/åˆ—è¡¨æ ¼å¼åŒ–ä¸ºäººç±»å¯è¯»çš„ Markdown æ ¼å¼
        è€Œä¸æ˜¯åŸå§‹ JSON
        """
        if data is None:
            return ""
        
        lines = []
        prefix = "  " * indent
        
        if isinstance(data, dict):
            for key, value in data.items():
                # è·³è¿‡å…ƒæ•°æ®å­—æ®µ
                if key in {"completion_status", "completion_rate", "quality_self_assessment", "notes",
                           "protocol_status", "compliance_confirmation", "challenge_details", "reinterpretation",
                           "confidence", "execution_time_estimate", "execution_notes", "dependencies_satisfied"}:
                    continue
                
                # æ ¼å¼åŒ–é”®å
                readable_key = key.replace("_", " ").title()
                
                if isinstance(value, dict):
                    lines.append(f"{prefix}**{readable_key}**:")
                    lines.append(self._format_dict_as_readable(value, indent + 1))
                elif isinstance(value, list):
                    lines.append(f"{prefix}**{readable_key}**:")
                    for item in value:
                        if isinstance(item, dict):
                            lines.append(self._format_dict_as_readable(item, indent + 1))
                        else:
                            lines.append(f"{prefix}  - {item}")
                elif value:
                    lines.append(f"{prefix}**{readable_key}**: {value}")
        elif isinstance(data, list):
            for item in data:
                if isinstance(item, dict):
                    lines.append(self._format_dict_as_readable(item, indent))
                    lines.append("")  # ç©ºè¡Œåˆ†éš”
                else:
                    lines.append(f"{prefix}- {item}")
        else:
            lines.append(f"{prefix}{data}")
        
        return "\n".join(lines)

    def _extract_quality_score(self, owner_result: Dict[str, Any]) -> Optional[float]:
        """ä»ä¸“å®¶è¾“å‡ºä¸­æå–è´¨é‡åˆ†æ•°"""
        if not owner_result:
            return None
        
        # ä» structured_output æå–
        structured_output = owner_result.get("structured_output", {})
        if structured_output and isinstance(structured_output, dict):
            # ä» protocol_execution æå–
            protocol_execution = structured_output.get("protocol_execution", {})
            if protocol_execution:
                confidence = protocol_execution.get("confidence_level")
                if confidence is not None:
                    try:
                        return float(confidence) * 100  # è½¬æ¢ä¸ºç™¾åˆ†åˆ¶
                    except (TypeError, ValueError):
                        pass
            
            # ä» task_results æå–
            task_results = structured_output.get("task_results", [])
            if task_results:
                completeness_scores = [
                    t.get("completeness_score", 0) 
                    for t in task_results 
                    if isinstance(t.get("completeness_score"), (int, float))
                ]
                if completeness_scores:
                    return sum(completeness_scores) / len(completeness_scores) * 100
        
        # ä» confidence å­—æ®µæå–
        confidence = owner_result.get("confidence")
        if confidence is not None:
            try:
                return float(confidence) * 100
            except (TypeError, ValueError):
                pass
        
        return None

    def _generate_answer_summary(self, full_answer: str) -> str:
        """ç”Ÿæˆç­”æ¡ˆæ‘˜è¦ï¼ˆ200å­—ä»¥å†…ï¼‰"""
        if not full_answer or full_answer == "æš‚æ— è¾“å‡º":
            return "æš‚æ— æ‘˜è¦"
        
        # ç®€å•æˆªå–å‰200å­—
        if len(full_answer) <= 200:
            return full_answer
        
        # å°è¯•åœ¨å¥å­è¾¹ç•Œæˆªæ–­
        truncated = full_answer[:200]
        last_period = max(truncated.rfind("ã€‚"), truncated.rfind("ï¼"), truncated.rfind("ï¼Ÿ"))
        if last_period > 100:
            return truncated[:last_period + 1]
        
        return truncated + "..."

    def _generate_combined_summary(self, deliverable_answers: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆå¤šäº¤ä»˜ç‰©çš„ç»¼åˆæ‘˜è¦ï¼ˆå‘åå…¼å®¹ç”¨ï¼‰"""
        if not deliverable_answers:
            return "æš‚æ— æ ¸å¿ƒç­”æ¡ˆ"
        
        summaries = []
        for da in deliverable_answers:
            name = da.get("deliverable_name", da.get("deliverable_id", "æœªçŸ¥"))
            summary = da.get("answer_summary", "")
            if summary:
                summaries.append(f"ã€{name}ã€‘{summary}")
        
        if not summaries:
            return "æš‚æ— æ ¸å¿ƒç­”æ¡ˆ"
        
        return " ".join(summaries)

    def _extract_supporter_contribution(
        self, 
        role_id: str, 
        agent_result: Dict[str, Any], 
        deliverable_metadata: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """æå–æ”¯æ’‘ä¸“å®¶çš„è´¡çŒ®ä¿¡æ¯"""
        if not agent_result:
            return None
        
        # ç¡®å®šè§’è‰²åç§°
        role_name = agent_result.get("role_name", role_id)
        
        # æå–è´¡çŒ®æ‘˜è¦
        analysis = agent_result.get("analysis", "")
        content = agent_result.get("content", "")
        contribution_text = analysis or content or ""
        
        if not contribution_text:
            structured_data = agent_result.get("structured_data", {})
            if structured_data:
                contribution_text = json.dumps(structured_data, ensure_ascii=False)[:500]
        
        if not contribution_text:
            return None
        
        # ç”Ÿæˆæ‘˜è¦
        contribution_summary = contribution_text[:200] + "..." if len(contribution_text) > 200 else contribution_text
        
        # ç¡®å®šå…³è”çš„äº¤ä»˜ç‰©
        related_deliverables = []
        for d_id, d_meta in deliverable_metadata.items():
            supporters = d_meta.get("supporters", [])
            if any(role_id.startswith(s.split("_")[0] + "_" + s.split("_")[1]) if len(s.split("_")) >= 2 else s == role_id for s in supporters):
                related_deliverables.append(d_id)
        
        return {
            "role_id": role_id,
            "role_name": role_name,
            "contribution_type": "support",
            "contribution_summary": contribution_summary,
            "related_deliverables": related_deliverables
        }

