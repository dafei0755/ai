# -*- coding: utf-8 -*-
"""
Celery å¼‚æ­¥ä»»åŠ¡å®šä¹‰

æ”¯æŒå¤šç”¨æˆ·å¹¶å‘åˆ†æçš„ä»»åŠ¡é˜Ÿåˆ—
"""

import asyncio
import io
import sys
from datetime import datetime
from typing import Any, Dict, Optional

from celery import current_task, shared_task
from celery.exceptions import SoftTimeLimitExceeded
from loguru import logger

# è®¾ç½®ç¼–ç 
if sys.platform == "win32":
    # Avoid swapping stdio streams at import time; it can break pytest/logging.
    for _stream in (getattr(sys, "stdout", None), getattr(sys, "stderr", None)):
        if _stream is None:
            continue
        if hasattr(_stream, "reconfigure"):
            try:
                _stream.reconfigure(encoding="utf-8", errors="replace")
            except Exception:
                pass

# å¯¼å…¥ Celery åº”ç”¨
from intelligent_project_analyzer.services.celery_app import celery_app


def run_async(coro):
    """åœ¨åŒæ­¥ç¯å¢ƒä¸­è¿è¡Œå¼‚æ­¥ä»£ç """
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # å¦‚æœå·²æœ‰äº‹ä»¶å¾ªç¯è¿è¡Œï¼Œåˆ›å»ºæ–°çš„
            import concurrent.futures

            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(asyncio.run, coro)
                return future.result()
        else:
            return loop.run_until_complete(coro)
    except RuntimeError:
        # æ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºæ–°çš„
        return asyncio.run(coro)


@celery_app.task(bind=True, name="analyze_project")
def analyze_project(self, session_id: str, user_input: str, user_id: str = "celery_user") -> Dict[str, Any]:
    """
    å¼‚æ­¥åˆ†æé¡¹ç›®ä»»åŠ¡

    Args:
        session_id: ä¼šè¯ID
        user_input: ç”¨æˆ·è¾“å…¥
        user_id: ç”¨æˆ·ID

    Returns:
        åˆ†æç»“æœ
    """
    logger.info(f"ğŸš€ [Celery] å¼€å§‹åˆ†æä»»åŠ¡: {session_id}")

    try:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        self.update_state(
            state="PROGRESS",
            meta={"session_id": session_id, "progress": 0.05, "current_stage": "åˆå§‹åŒ–", "message": "æ­£åœ¨å¯åŠ¨åˆ†ææµç¨‹..."},
        )

        # æ‰§è¡Œå¼‚æ­¥åˆ†æ
        result = run_async(_run_workflow(self, session_id, user_input, user_id))

        logger.info(f"âœ… [Celery] åˆ†æä»»åŠ¡å®Œæˆ: {session_id}")
        return result

    except SoftTimeLimitExceeded:
        logger.warning(f"â° [Celery] ä»»åŠ¡è¶…æ—¶: {session_id}")
        return {"session_id": session_id, "status": "timeout", "error": "åˆ†æä»»åŠ¡è¶…æ—¶ï¼Œè¯·å°è¯•ç®€åŒ–è¾“å…¥"}
    except Exception as e:
        logger.error(f"âŒ [Celery] ä»»åŠ¡å¤±è´¥: {session_id}, é”™è¯¯: {str(e)}")
        import traceback

        return {"session_id": session_id, "status": "failed", "error": str(e), "traceback": traceback.format_exc()}


async def _run_workflow(task, session_id: str, user_input: str, user_id: str) -> Dict[str, Any]:
    """
    æ‰§è¡Œå·¥ä½œæµçš„å¼‚æ­¥å†…éƒ¨å‡½æ•°
    """
    from intelligent_project_analyzer.core.state import StateManager
    from intelligent_project_analyzer.services.redis_session_manager import RedisSessionManager
    from intelligent_project_analyzer.workflow.main_workflow import MainWorkflow

    # åˆå§‹åŒ– Redis ä¼šè¯ç®¡ç†å™¨
    session_manager = RedisSessionManager()
    await session_manager.connect()

    try:
        # æ›´æ–°ä¼šè¯çŠ¶æ€
        await session_manager.update(
            session_id, {"status": "running", "progress": 0.1, "task_id": task.request.id}  # ä¿å­˜ Celery ä»»åŠ¡ID
        )

        # åˆ›å»ºå·¥ä½œæµ
        workflow = MainWorkflow()
        workflow.build()

        task.update_state(
            state="PROGRESS",
            meta={"session_id": session_id, "progress": 0.15, "current_stage": "å·¥ä½œæµåˆå§‹åŒ–", "message": "æ­£åœ¨åˆ›å»ºåˆ†æå·¥ä½œæµ..."},
        )

        # åˆ›å»ºåˆå§‹çŠ¶æ€
        initial_state = StateManager.create_initial_state(user_input=user_input, session_id=session_id, user_id=user_id)

        config = {"configurable": {"thread_id": session_id}, "recursion_limit": 100}

        # ğŸ¯ v7.21: èŠ‚ç‚¹è¿›åº¦æ˜ å°„ï¼ˆä¸ main_workflow.py å®é™…èŠ‚ç‚¹åç§°å¯¹é½ï¼‰
        node_progress_map = {
            # è¾“å…¥éªŒè¯é˜¶æ®µ (0-15%)
            "unified_input_validator_initial": 0.05,
            "unified_input_validator_secondary": 0.10,
            # éœ€æ±‚åˆ†æé˜¶æ®µ (15-35%)
            "requirements_analyst": 0.15,
            "feasibility_analyst": 0.20,
            "calibration_questionnaire": 0.25,
            "requirements_confirmation": 0.35,
            # é¡¹ç›®è§„åˆ’é˜¶æ®µ (35-55%)
            "project_director": 0.40,
            "role_task_unified_review": 0.45,
            "quality_preflight": 0.50,
            # ä¸“å®¶æ‰§è¡Œé˜¶æ®µ (55-80%)
            "batch_executor": 0.55,
            "agent_executor": 0.70,
            "batch_aggregator": 0.75,
            "batch_router": 0.76,
            "batch_strategy_review": 0.78,
            # å®¡æ ¸èšåˆé˜¶æ®µ (80-100%)
            "detect_challenges": 0.80,
            "analysis_review": 0.85,
            "result_aggregator": 0.90,
            "report_guard": 0.95,
            "pdf_generator": 0.98,
        }

        # æµå¼æ‰§è¡Œå·¥ä½œæµ
        events = []
        final_state = None

        async for chunk in workflow.graph.astream(initial_state, config):
            events.append(chunk)

            # æ›´æ–°è¿›åº¦
            for node_name, node_output in chunk.items():
                if node_name == "__interrupt__":
                    # å¤„ç†ä¸­æ–­ï¼ˆéœ€è¦ç”¨æˆ·è¾“å…¥ï¼‰
                    interrupt_data = chunk["__interrupt__"]
                    await session_manager.update(
                        session_id,
                        {
                            "status": "waiting_for_input",
                            "interrupt_data": _serialize_interrupt(interrupt_data),
                            "current_node": "interrupt",
                        },
                    )

                    task.update_state(
                        state="WAITING",
                        meta={
                            "session_id": session_id,
                            "progress": node_progress_map.get(node_name, 0.5),
                            "current_stage": "ç­‰å¾…ç”¨æˆ·è¾“å…¥",
                            "message": "éœ€è¦æ‚¨çš„ç¡®è®¤æ‰èƒ½ç»§ç»­",
                            "interrupt_data": _serialize_interrupt(interrupt_data),
                        },
                    )

                    # è¿”å›ç­‰å¾…çŠ¶æ€ï¼Œè®© API å¤„ç†æ¢å¤é€»è¾‘
                    return {
                        "session_id": session_id,
                        "status": "waiting_for_input",
                        "interrupt_data": _serialize_interrupt(interrupt_data),
                    }

                # æ›´æ–°è¿›åº¦
                new_progress = node_progress_map.get(node_name, 0.5)
                detail = ""
                if isinstance(node_output, dict):
                    detail = node_output.get("detail", node_output.get("current_stage", ""))

                # ğŸ”¥ é˜²æ­¢è¿›åº¦å›é€€ï¼šè·å–å½“å‰è¿›åº¦å¹¶å–æœ€å¤§å€¼
                current_data = await session_manager.get(session_id)
                old_progress = current_data.get("progress", 0) if current_data else 0
                progress = max(new_progress, old_progress if isinstance(old_progress, (int, float)) else 0)

                await session_manager.update(
                    session_id, {"progress": progress, "current_node": node_name, "detail": detail}
                )

                task.update_state(
                    state="PROGRESS",
                    meta={
                        "session_id": session_id,
                        "progress": progress,
                        "current_stage": node_name,
                        "detail": detail,
                        "message": f"æ­£åœ¨æ‰§è¡Œ: {node_name}",
                    },
                )

                final_state = node_output

        # æå–æœ€ç»ˆæŠ¥å‘Š
        final_report = None
        if final_state and isinstance(final_state, dict):
            final_report = final_state.get("final_report") or final_state.get("report_text")

        # æ›´æ–°å®ŒæˆçŠ¶æ€
        await session_manager.update(
            session_id,
            {
                "status": "completed",
                "progress": 1.0,
                "events": events,
                "final_report": final_report,
                "completed_at": datetime.now().isoformat(),
            },
        )

        return {"session_id": session_id, "status": "completed", "progress": 1.0, "final_report": final_report}

    finally:
        await session_manager.disconnect()


def _serialize_interrupt(interrupt_data) -> Dict[str, Any]:
    """åºåˆ—åŒ–ä¸­æ–­æ•°æ®"""
    if interrupt_data is None:
        return {}

    if isinstance(interrupt_data, tuple) and len(interrupt_data) > 0:
        interrupt_obj = interrupt_data[0]
        if hasattr(interrupt_obj, "value"):
            return {"value": interrupt_obj.value}

    return {"raw": str(interrupt_data)}


@celery_app.task(bind=True, name="resume_analysis")
def resume_analysis(self, session_id: str, resume_value: Any) -> Dict[str, Any]:
    """
    æ¢å¤åˆ†æä»»åŠ¡ï¼ˆå¤„ç†ç”¨æˆ·è¾“å…¥åç»§ç»­ï¼‰

    Args:
        session_id: ä¼šè¯ID
        resume_value: ç”¨æˆ·æäº¤çš„å€¼

    Returns:
        åˆ†æç»“æœ
    """
    logger.info(f"ğŸ”„ [Celery] æ¢å¤åˆ†æä»»åŠ¡: {session_id}")

    try:
        result = run_async(_resume_workflow(self, session_id, resume_value))
        return result
    except Exception as e:
        logger.error(f"âŒ [Celery] æ¢å¤ä»»åŠ¡å¤±è´¥: {session_id}, é”™è¯¯: {str(e)}")
        import traceback

        return {"session_id": session_id, "status": "failed", "error": str(e), "traceback": traceback.format_exc()}


async def _resume_workflow(task, session_id: str, resume_value: Any) -> Dict[str, Any]:
    """
    æ¢å¤å·¥ä½œæµçš„å¼‚æ­¥å†…éƒ¨å‡½æ•°
    """
    from langgraph.types import Command

    from intelligent_project_analyzer.services.redis_session_manager import RedisSessionManager

    session_manager = RedisSessionManager()
    await session_manager.connect()

    try:
        # è·å–ä¼šè¯æ•°æ®
        session = await session_manager.get(session_id)
        if not session:
            return {"session_id": session_id, "status": "failed", "error": "ä¼šè¯ä¸å­˜åœ¨"}

        # è·å–å·¥ä½œæµå®ä¾‹ï¼ˆéœ€è¦ä»å…¨å±€æˆ–é‡å»ºï¼‰
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯èƒ½éœ€è¦æ›´å¤æ‚çš„çŠ¶æ€æ¢å¤
        from intelligent_project_analyzer.workflow.main_workflow import MainWorkflow

        workflow = MainWorkflow()
        workflow.build()

        config = {"configurable": {"thread_id": session_id}, "recursion_limit": 100}

        # ä½¿ç”¨ Command æ¢å¤æ‰§è¡Œ
        resume_command = Command(resume=resume_value)

        # ç»§ç»­æ‰§è¡Œ
        # æ³¨æ„ï¼šå®é™…å®ç°å¯èƒ½éœ€è¦æ£€æŸ¥ç‚¹æœºåˆ¶æ¥å®Œæ•´æ¢å¤çŠ¶æ€
        # è¿™é‡Œæ˜¯ç®€åŒ–ç‰ˆæœ¬

        await session_manager.update(session_id, {"status": "running", "interrupt_data": None})

        task.update_state(
            state="PROGRESS",
            meta={"session_id": session_id, "progress": 0.5, "current_stage": "æ¢å¤æ‰§è¡Œ", "message": "æ­£åœ¨æ¢å¤åˆ†ææµç¨‹..."},
        )

        # TODO: å®ç°å®Œæ•´çš„æ£€æŸ¥ç‚¹æ¢å¤é€»è¾‘
        # è¿™éœ€è¦ LangGraph çš„æŒä¹…åŒ–åŠŸèƒ½æ”¯æŒ

        return {"session_id": session_id, "status": "resumed", "message": "ä»»åŠ¡å·²æ¢å¤"}

    finally:
        await session_manager.disconnect()


@celery_app.task(name="cleanup_expired_sessions")
def cleanup_expired_sessions() -> Dict[str, Any]:
    """
    æ¸…ç†è¿‡æœŸä¼šè¯ï¼ˆå®šæœŸä»»åŠ¡ï¼‰
    """
    logger.info("ğŸ§¹ [Celery] å¼€å§‹æ¸…ç†è¿‡æœŸä¼šè¯")

    try:
        result = run_async(_cleanup_sessions())
        logger.info(f"âœ… [Celery] æ¸…ç†å®Œæˆ: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ [Celery] æ¸…ç†å¤±è´¥: {str(e)}")
        return {"status": "failed", "error": str(e)}


async def _cleanup_sessions() -> Dict[str, Any]:
    """æ¸…ç†è¿‡æœŸä¼šè¯çš„å¼‚æ­¥å®ç°"""
    from intelligent_project_analyzer.services.redis_session_manager import RedisSessionManager

    session_manager = RedisSessionManager()
    await session_manager.connect()

    try:
        # Redis TTL ä¼šè‡ªåŠ¨æ¸…ç†ï¼Œè¿™é‡Œå¯ä»¥åšé¢å¤–æ¸…ç†é€»è¾‘
        # æ¯”å¦‚æ¸…ç†å­¤ç«‹çš„å·¥ä½œæµå®ä¾‹ç­‰
        return {"status": "success", "message": "Redis TTL è‡ªåŠ¨ç®¡ç†ä¼šè¯è¿‡æœŸ"}
    finally:
        await session_manager.disconnect()


# ==================== ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢å·¥å…·å‡½æ•° ====================


def get_task_status(task_id: str) -> Dict[str, Any]:
    """
    è·å– Celery ä»»åŠ¡çŠ¶æ€

    Args:
        task_id: Celery ä»»åŠ¡ID

    Returns:
        ä»»åŠ¡çŠ¶æ€ä¿¡æ¯
    """
    from celery.result import AsyncResult

    result = AsyncResult(task_id, app=celery_app)

    response = {
        "task_id": task_id,
        "status": result.status,
        "ready": result.ready(),
        "successful": result.successful() if result.ready() else None,
    }

    # æ·»åŠ è¿›åº¦ä¿¡æ¯
    if result.status == "PROGRESS" or result.status == "WAITING":
        response["meta"] = result.info
    elif result.ready():
        response["result"] = result.result

    return response


def get_queue_length(queue_name: str = "analysis") -> int:
    """
    è·å–é˜Ÿåˆ—ä¸­ç­‰å¾…çš„ä»»åŠ¡æ•°é‡

    Args:
        queue_name: é˜Ÿåˆ—åç§°

    Returns:
        é˜Ÿåˆ—é•¿åº¦
    """
    from intelligent_project_analyzer.services.celery_app import celery_app

    with celery_app.pool.acquire(block=True) as conn:
        return conn.default_channel.client.llen(queue_name)
