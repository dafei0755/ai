"""
æ ¸å¿ƒä»»åŠ¡æ‹†è§£æœåŠ¡

v7.80.1: å°†ç”¨æˆ·æ¨¡ç³Šè¾“å…¥æ‹†è§£ä¸ºç»“æ„åŒ–çš„å¯æ‰§è¡Œä»»åŠ¡åˆ—è¡¨
v7.110.0: æ™ºèƒ½åŒ–ä»»åŠ¡æ•°é‡è¯„ä¼°ï¼Œæ ¹æ®è¾“å…¥å¤æ‚åº¦åŠ¨æ€è°ƒæ•´ï¼ˆ3-12ä¸ªå¼¹æ€§èŒƒå›´ï¼‰
ç”¨äº Step 1: ä»»åŠ¡æ¢³ç†
"""

import json
import re
from pathlib import Path
from typing import Any, Dict, List, Optional

import yaml
from loguru import logger


class TaskComplexityAnalyzer:
    """
    ä»»åŠ¡å¤æ‚åº¦åˆ†æå™¨ - v7.110.0

    æ ¹æ®ç”¨æˆ·è¾“å…¥çš„ä¿¡æ¯å¯†åº¦å’Œå¤æ‚ç¨‹åº¦ï¼Œæ™ºèƒ½è¯„ä¼°å»ºè®®çš„ä»»åŠ¡æ•°é‡ã€‚
    ä¸å†ç¡¬ç¼–ç 5-7ä¸ªä»»åŠ¡ï¼Œè€Œæ˜¯åŸºäºå®é™…éœ€æ±‚åŠ¨æ€è°ƒæ•´ï¼ˆ3-12ä¸ªèŒƒå›´ï¼‰ã€‚
    """

    # é…ç½®å‚æ•°
    MIN_TASKS = 3  # æœ€å°‘ä»»åŠ¡æ•°ï¼ˆç®€å•éœ€æ±‚ï¼‰
    MAX_TASKS = 12  # æœ€å¤šä»»åŠ¡æ•°ï¼ˆå¤æ‚éœ€æ±‚ï¼‰
    BASE_TASKS = 5  # åŸºå‡†ä»»åŠ¡æ•°

    @classmethod
    def analyze(cls, user_input: str, structured_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        åˆ†æç”¨æˆ·è¾“å…¥å¤æ‚åº¦ï¼Œè¿”å›å»ºè®®çš„ä»»åŠ¡æ•°é‡èŒƒå›´

        Args:
            user_input: ç”¨æˆ·åŸå§‹è¾“å…¥
            structured_data: ç»“æ„åŒ–æ•°æ®ï¼ˆå¯é€‰ï¼‰

        Returns:
            {
                "recommended_min": int,  # å»ºè®®æœ€å°‘ä»»åŠ¡æ•°
                "recommended_max": int,  # å»ºè®®æœ€å¤šä»»åŠ¡æ•°
                "complexity_score": float,  # å¤æ‚åº¦å¾—åˆ†ï¼ˆ0-1ï¼‰
                "reasoning": str  # åˆ¤æ–­ä¾æ®
            }
        """
        complexity_score = 0.0
        reasoning_parts = []

        # 1. è¾“å…¥é•¿åº¦è¯„åˆ†ï¼ˆåŸºç¡€ç»´åº¦ï¼‰
        input_length = len(user_input)
        if input_length < 50:
            length_score = 0.2
            reasoning_parts.append("è¾“å…¥è¾ƒç®€çŸ­")
        elif input_length < 150:
            length_score = 0.4
            reasoning_parts.append("è¾“å…¥ä¸­ç­‰é•¿åº¦")
        elif input_length < 300:
            length_score = 0.6
            reasoning_parts.append("è¾“å…¥è¾ƒè¯¦ç»†")
        else:
            length_score = 0.8
            reasoning_parts.append("è¾“å…¥éå¸¸è¯¦ç»†")
        complexity_score += length_score * 0.15

        # 2. ä¿¡æ¯ç»´åº¦æ•°é‡ï¼ˆå…³é”®ç»´åº¦ï¼‰
        dimension_count = 0
        dimensions = {
            "ç©ºé—´è§„æ¨¡": [r"\d+å¹³ç±³", r"\d+ã¡", r"\d+å¹³æ–¹ç±³", r"\d+ä¸‡å¹³", r"åˆ«å¢…", r"ä½å®…", r"åŠå…¬", r"å•†ä¸š"],
            "é¢„ç®—çº¦æŸ": [r"\d+ä¸‡", r"é¢„ç®—", r"èµ„é‡‘", r"æˆæœ¬", r"æœ‰é™"],
            "å¯¹æ ‡æ¡ˆä¾‹": [r"å¯¹æ ‡", r"å‚è€ƒ", r"æ¡ˆä¾‹", r"æ ‡æ†", r"åƒ.*ä¸€æ ·"],
            "æ–‡åŒ–è¦ç´ ": [r"æ–‡åŒ–", r"ä¼ ç»Ÿ", r"åœ¨åœ°", r"å†å²", r"ç‰¹è‰²", r"æ¸”æ‘", r"è€è¡—"],
            "å®¢ç¾¤åˆ†æ": [r"\d+å²", r"å®¢ç¾¤", r"ç”¨æˆ·", r"è®¿å®¢", r"å±…æ°‘", r"å¥³æ€§", r"ç”·æ€§", r"äººç¾¤"],
            "æ ¸å¿ƒå¼ åŠ›": [r'"[^"]+".*ä¸.*"[^"]+"', r"vs", r"å¯¹ç«‹", r"çŸ›ç›¾", r"å¹³è¡¡"],
            "å“ç‰Œä¸»é¢˜": [r"[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*", r"ä¸ºä¸»é¢˜", r"å“ç‰Œ"],
            "è®¾è®¡é£æ ¼": [r"é£æ ¼", r"æ‚å¿—çº§", r"é«˜çº§æ„Ÿ", r"ç¾å­¦", r"æ°›å›´"],
            "åŠŸèƒ½éœ€æ±‚": [r"åŠŸèƒ½", r"éœ€æ±‚", r"ä½¿ç”¨", r"åœºæ™¯", r"åŠ¨çº¿"],
            "æ—¶é—´çº¦æŸ": [r"\d+ä¸ªæœˆ", r"\d+å¹´", r"å·¥æœŸ", r"äº¤ä»˜"],
            "åœ°ç†ä½ç½®": [r"ä¸Šæµ·", r"åŒ—äº¬", r"æ·±åœ³", r"è›‡å£", r"å¼„å ‚", r"è€åŸåŒº"],
            "ç‰¹æ®Šåœºæ™¯": [r"è€æˆ¿ç¿»æ–°", r"æ—§æ”¹", r"èœå¸‚åœº", r"å†å²å»ºç­‘", r"æ–‡ä¿"],
        }

        for dimension_name, patterns in dimensions.items():
            if any(re.search(pattern, user_input, re.IGNORECASE) for pattern in patterns):
                dimension_count += 1

        dimension_score = min(dimension_count / 8, 1.0)  # 8ä¸ªç»´åº¦ä¸ºæ»¡åˆ†
        complexity_score += dimension_score * 0.35
        reasoning_parts.append(f"åŒ…å«{dimension_count}ä¸ªä¿¡æ¯ç»´åº¦")

        # 3. ç»“æ„åŒ–æ•°æ®æ·±åº¦ï¼ˆå¢å¼ºç»´åº¦ï¼‰
        structured_score = 0.0
        if structured_data:
            # æ£€æŸ¥å…³é”®å­—æ®µ
            key_fields = [
                "design_challenge",
                "character_narrative",
                "physical_context",
                "analysis_layers",
                "project_type",
                "core_goals",
            ]
            filled_fields = sum(1 for field in key_fields if structured_data.get(field))
            structured_score = filled_fields / len(key_fields)
            reasoning_parts.append(f"ç»“æ„åŒ–æ•°æ®åŒ…å«{filled_fields}ä¸ªå…³é”®å­—æ®µ")
        complexity_score += structured_score * 0.25

        # 4. ç‰¹æ®Šåœºæ™¯è¯†åˆ«ï¼ˆå¯èƒ½éœ€è¦æ›´å¤šä»»åŠ¡ï¼‰
        special_scenes = {
            "è¯—æ„è¡¨è¾¾": [r"[å¦‚ä¼¼è‹¥åƒ].*[èˆ¬æ ·]", r"æ„è±¡", r"éšå–»", r"è¯—æ„"],
            "å¤šå…ƒå¯¹ç«‹": [r'"[^"]+".*"[^"]+".*"[^"]+"', r"å…¼é¡¾.*ä¸.*ä¸"],  # 3ä¸ªä»¥ä¸Šå¯¹ç«‹æ¦‚å¿µ
            "è·¨ç•Œèåˆ": [r"èåˆ", r"ç»“åˆ", r"è·¨ç•Œ", r"æ··æ­"],
            "æ–‡åŒ–æ·±åº¦": [r"æ–‡åŒ–.*æ–‡åŒ–", r"ä¼ ç»Ÿ.*ç°ä»£", r"å†å².*æœªæ¥"],
        }
        special_count = 0
        for scene_name, patterns in special_scenes.items():
            if any(re.search(pattern, user_input, re.IGNORECASE) for pattern in patterns):
                special_count += 1
                reasoning_parts.append(f"æ£€æµ‹åˆ°{scene_name}")

        special_score = min(special_count / 3, 1.0)
        complexity_score += special_score * 0.15

        # 5. å¥å­æ•°é‡ï¼ˆåæ˜ æ€è€ƒæ·±åº¦ï¼‰
        sentence_count = len(re.split(r"[ã€‚ï¼ï¼Ÿ\n]", user_input))
        if sentence_count >= 5:
            sentence_score = 0.1
            reasoning_parts.append(f"åŒ…å«{sentence_count}ä¸ªå¥å­/æ®µè½")
        else:
            sentence_score = 0.05
        complexity_score += sentence_score

        # 6. å…·ä½“æ•°å­—çš„å‡ºç°ï¼ˆåæ˜ ç²¾ç¡®åº¦ï¼‰
        number_matches = re.findall(r"\d+", user_input)
        if len(number_matches) >= 3:
            complexity_score += 0.1
            reasoning_parts.append("åŒ…å«å¤šä¸ªå…·ä½“æ•°æ®")

        # è®¡ç®—å»ºè®®ä»»åŠ¡æ•°é‡èŒƒå›´
        # å¤æ‚åº¦ 0-0.3 -> 3-5ä¸ªä»»åŠ¡ï¼ˆç®€å•ï¼‰
        # å¤æ‚åº¦ 0.3-0.6 -> 5-8ä¸ªä»»åŠ¡ï¼ˆä¸­ç­‰ï¼‰
        # å¤æ‚åº¦ 0.6-1.0 -> 8-12ä¸ªä»»åŠ¡ï¼ˆå¤æ‚ï¼‰

        if complexity_score < 0.3:
            recommended_min = cls.MIN_TASKS
            recommended_max = cls.BASE_TASKS
        elif complexity_score < 0.6:
            recommended_min = cls.BASE_TASKS
            recommended_max = 8
        else:
            recommended_min = 7
            recommended_max = cls.MAX_TASKS

        reasoning = "; ".join(reasoning_parts)

        logger.info(f"ğŸ“Š [TaskComplexityAnalyzer] å¤æ‚åº¦={complexity_score:.2f}, å»ºè®®ä»»åŠ¡æ•°={recommended_min}-{recommended_max}")

        return {
            "recommended_min": recommended_min,
            "recommended_max": recommended_max,
            "complexity_score": complexity_score,
            "reasoning": reasoning,
        }


class CoreTaskDecomposer:
    """
    æ ¸å¿ƒä»»åŠ¡æ‹†è§£å™¨

    ä½¿ç”¨ LLM å°†ç”¨æˆ·è¾“å…¥æ‹†è§£ä¸ºç²¾å‡†çš„ã€å¯æ‰§è¡Œçš„ä»»åŠ¡åˆ—è¡¨ã€‚
    """

    _instance = None
    _config = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if self._config is None:
            self._load_config()

    def _load_config(self) -> None:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_path = Path(__file__).parent.parent / "config" / "prompts" / "core_task_decomposer.yaml"
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                self._config = yaml.safe_load(f)
            logger.info(f"âœ… [CoreTaskDecomposer] é…ç½®åŠ è½½æˆåŠŸ: {config_path}")
        except Exception as e:
            logger.warning(f"âš ï¸ [CoreTaskDecomposer] é…ç½®åŠ è½½å¤±è´¥: {e}")
            self._config = {}

    @property
    def config(self) -> Dict[str, Any]:
        """è·å–é…ç½®"""
        return self._config or {}

    def build_prompt(
        self,
        user_input: str,
        structured_data: Optional[Dict[str, Any]] = None,
        task_count_range: Optional[tuple] = None,
    ) -> str:
        """
        æ„å»º LLM è°ƒç”¨çš„ promptï¼ˆv7.110.0 æ”¯æŒåŠ¨æ€ä»»åŠ¡æ•°é‡ï¼‰

        Args:
            user_input: ç”¨æˆ·åŸå§‹è¾“å…¥
            structured_data: éœ€æ±‚åˆ†æé˜¶æ®µäº§å‡ºçš„ç»“æ„åŒ–æ•°æ®ï¼ˆå¯é€‰ï¼‰
            task_count_range: å»ºè®®çš„ä»»åŠ¡æ•°é‡èŒƒå›´ (min, max)ï¼Œå¦‚ (3, 5) æˆ– (8, 12)

        Returns:
            å®Œæ•´çš„ prompt å­—ç¬¦ä¸²
        """
        # æ„å»ºç»“æ„åŒ–æ•°æ®æ‘˜è¦
        structured_summary = ""
        if structured_data:
            summary_parts = []

            # æå–å…³é”®å­—æ®µ
            project_task = structured_data.get("project_task", "")
            if project_task:
                summary_parts.append(f"é¡¹ç›®ä»»åŠ¡: {project_task}")

            character_narrative = structured_data.get("character_narrative", "")
            if character_narrative:
                summary_parts.append(f"äººç‰©å™äº‹: {character_narrative}")

            physical_context = structured_data.get("physical_context", "")
            if physical_context:
                summary_parts.append(f"ç‰©ç†åœºæ™¯: {physical_context}")

            project_type = structured_data.get("project_type", "")
            if project_type:
                summary_parts.append(f"é¡¹ç›®ç±»å‹: {project_type}")

            # L1-L5 åˆ†æå±‚ä¿¡æ¯
            analysis_layers = structured_data.get("analysis_layers", {})
            if analysis_layers:
                l3_tension = analysis_layers.get("L3_core_tension", "")
                if l3_tension:
                    summary_parts.append(f"æ ¸å¿ƒå¼ åŠ›: {l3_tension}")

            structured_summary = "\n".join(summary_parts) if summary_parts else "æš‚æ— è¡¥å……ä¿¡æ¯"

        # è·å– prompt æ¨¡æ¿
        system_prompt = self.config.get("system_prompt", "")
        user_template = self.config.get("user_prompt_template", "")

        # ğŸ”§ v7.118: ä¿®å¤ KeyError - ä½¿ç”¨ task_count_range å‚æ•°
        task_min = task_count_range[0] if task_count_range else 5
        task_max = task_count_range[1] if task_count_range else 7

        # å¡«å……ç”¨æˆ· promptï¼ˆåŒ…å«ä»»åŠ¡æ•°é‡å ä½ç¬¦ï¼‰
        user_prompt = user_template.format(
            user_input=user_input,
            structured_data_summary=structured_summary,
            task_count_min=task_min,
            task_count_max=task_max,
        )

        return f"{system_prompt}\n\n{user_prompt}"

    def parse_response(self, response: str) -> List[Dict[str, Any]]:
        """
        è§£æ LLM å“åº”ï¼Œæå–ä»»åŠ¡åˆ—è¡¨

        Args:
            response: LLM è¿”å›çš„åŸå§‹å“åº”

        Returns:
            ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å« id, title, description, source_keywords, task_type, priority
        """
        try:
            # ğŸ†• P1: å¢å¼º JSON æå–èƒ½åŠ› - æ·»åŠ  debug æ—¥å¿—
            logger.debug(f"[CoreTaskDecomposer] LLM åŸå§‹å“åº” (å‰500å­—ç¬¦): {response[:500]}")

            response_text = response.strip()

            # ç­–ç•¥ 1: ç§»é™¤ markdown ä»£ç å—æ ‡è®°
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            elif response_text.startswith("```"):
                response_text = response_text[3:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]

            response_text = response_text.strip()

            # ç­–ç•¥ 2: å¦‚æœè¿˜ä¸æ˜¯ JSONï¼Œå°è¯•æå– { } ä¹‹é—´çš„å†…å®¹
            if not response_text.startswith("{"):
                # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª {
                start_idx = response_text.find("{")
                if start_idx != -1:
                    # æŸ¥æ‰¾åŒ¹é…çš„ }ï¼ˆå¹³è¡¡æ‹¬å·ï¼‰
                    brace_count = 0
                    end_idx = -1
                    for i in range(start_idx, len(response_text)):
                        if response_text[i] == "{":
                            brace_count += 1
                        elif response_text[i] == "}":
                            brace_count -= 1
                            if brace_count == 0:
                                end_idx = i + 1
                                break

                    if end_idx != -1:
                        response_text = response_text[start_idx:end_idx]
                        logger.info(f"âœ… [CoreTaskDecomposer] ä½¿ç”¨æ‹¬å·å¹³è¡¡æå– JSON (é•¿åº¦: {len(response_text)})")

            # ç­–ç•¥ 3: å°è¯•ç§»é™¤ JSON ä¸­çš„æ³¨é‡Šï¼ˆ// æˆ– /* */ï¼‰
            import re

            response_text = re.sub(r"//.*?$", "", response_text, flags=re.MULTILINE)  # ç§»é™¤å•è¡Œæ³¨é‡Š
            response_text = re.sub(r"/\*.*?\*/", "", response_text, flags=re.DOTALL)  # ç§»é™¤å¤šè¡Œæ³¨é‡Š

            # è§£æ JSON
            data = json.loads(response_text)

            # æå–ä»»åŠ¡åˆ—è¡¨
            tasks = data.get("tasks", [])

            # å¦‚æœ tasks ä¸ºç©ºï¼Œå°è¯•ä»å…¶ä»–å¯èƒ½çš„å­—æ®µæå–
            if not tasks:
                # å¯èƒ½æ˜¯ç›´æ¥è¿”å›äº†ä»»åŠ¡æ•°ç»„
                if isinstance(data, list):
                    tasks = data
                # æˆ–è€…ä»»åŠ¡åœ¨å…¶ä»–å­—æ®µä¸­
                elif "task_list" in data:
                    tasks = data["task_list"]
                elif "core_tasks" in data:
                    tasks = data["core_tasks"]

            # éªŒè¯å’Œè§„èŒƒåŒ–ä»»åŠ¡
            validated_tasks = []
            for i, task in enumerate(tasks):
                validated_task = {
                    "id": task.get("id", f"task_{i + 1}"),
                    "title": task.get("title", "æœªå‘½åä»»åŠ¡"),
                    "description": task.get("description", ""),
                    "source_keywords": task.get("source_keywords", []),
                    "task_type": task.get("task_type", "research"),
                    "priority": task.get("priority", "medium"),
                    # ğŸ†• v7.106: æ·»åŠ ä¾èµ–å’Œæ‰§è¡Œé¡ºåº
                    "dependencies": task.get("dependencies", []),
                    "execution_order": task.get("execution_order", i + 1),
                    # ğŸ†• v7.109+: æ·»åŠ æœç´¢å’Œæ¦‚å¿µå›¾é…ç½®
                    "support_search": task.get("support_search", False),
                    "needs_concept_image": task.get("needs_concept_image", False),
                    "concept_image_count": task.get("concept_image_count", 1 if task.get("needs_concept_image") else 0),
                }
                validated_tasks.append(validated_task)

                # ğŸ”¥ v7.119: æ·»åŠ æ¦‚å¿µå›¾ç›¸å…³æ—¥å¿—
                if validated_task["needs_concept_image"]:
                    logger.info(f"ğŸ¨ [ä»»åŠ¡ {validated_task['id']}] éœ€è¦ç”Ÿæˆæ¦‚å¿µå›¾")
                    logger.info(f"  ğŸ“Š æ•°é‡: {validated_task['concept_image_count']} å¼ ")
                    logger.info(f"  ğŸ“ ä»»åŠ¡æ ‡é¢˜: {validated_task['title']}")
                if validated_task["support_search"]:
                    logger.debug(f"ğŸ” [ä»»åŠ¡ {validated_task['id']}] æ”¯æŒæœç´¢åŠŸèƒ½")

            logger.info(f"âœ… [CoreTaskDecomposer] æˆåŠŸè§£æ {len(validated_tasks)} ä¸ªä»»åŠ¡")

            # ç»Ÿè®¡æ¦‚å¿µå›¾ç›¸å…³ä»»åŠ¡
            tasks_with_images = [t for t in validated_tasks if t.get("needs_concept_image")]
            total_image_count = sum(t.get("concept_image_count", 0) for t in validated_tasks)
            if tasks_with_images:
                logger.info(f"ğŸ“¸ [æ¦‚å¿µå›¾ç»Ÿè®¡] {len(tasks_with_images)} ä¸ªä»»åŠ¡éœ€è¦æ¦‚å¿µå›¾ï¼Œå…± {total_image_count} å¼ ")
                for t in tasks_with_images:
                    logger.debug(f"  â€¢ {t['title']}: {t.get('concept_image_count', 0)} å¼ ")

            return validated_tasks

        except json.JSONDecodeError as e:
            logger.error(f"âŒ [CoreTaskDecomposer] JSON è§£æå¤±è´¥: {e}")
            logger.debug(f"[CoreTaskDecomposer] è§£æå¤±è´¥çš„æ–‡æœ¬: {response_text[:200]}")
            return self._fallback_decompose(response)
        except Exception as e:
            logger.error(f"âŒ [CoreTaskDecomposer] å“åº”è§£æå¼‚å¸¸: {e}")
            return []

    def _fallback_decompose(self, text: str) -> List[Dict[str, Any]]:
        """
        å›é€€ç­–ç•¥ï¼šå½“ LLM å“åº”æ ¼å¼ä¸æ­£ç¡®æ—¶ï¼Œä½¿ç”¨ç®€å•çš„è§„åˆ™æå–

        Args:
            text: åŸå§‹æ–‡æœ¬

        Returns:
            ç®€åŒ–çš„ä»»åŠ¡åˆ—è¡¨
        """
        logger.warning("âš ï¸ [CoreTaskDecomposer] ä½¿ç”¨å›é€€ç­–ç•¥æå–ä»»åŠ¡")

        # å°è¯•ä»æ–‡æœ¬ä¸­æå–ç¼–å·åˆ—è¡¨
        lines = text.split("\n")
        tasks = []

        for i, line in enumerate(lines):
            line = line.strip()
            # åŒ¹é… "1." "1ã€" "1)" "- " ç­‰æ ¼å¼
            if line and (
                (line[0].isdigit() and len(line) > 2 and line[1] in ".ã€)")
                or line.startswith("- ")
                or line.startswith("â€¢ ")
            ):
                # æå–ä»»åŠ¡å†…å®¹
                content = line.lstrip("0123456789.ã€)- â€¢").strip()
                if content and len(content) > 5:
                    # ğŸ†• v7.109+: ç®€å•è§„åˆ™åˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢å’Œæ¦‚å¿µå›¾
                    is_research_task = any(kw in content for kw in ["ç ”ç©¶", "è°ƒç ”", "åˆ†æ", "å¯¹æ ‡", "æ¡ˆä¾‹"])
                    is_design_task = any(kw in content for kw in ["è®¾è®¡", "æ–¹æ¡ˆ", "è§„åˆ’", "æ¡†æ¶"])

                    tasks.append(
                        {
                            "id": f"task_{len(tasks) + 1}",
                            "title": content[:50] if len(content) > 50 else content,
                            "description": content,
                            "source_keywords": [],
                            "task_type": "research" if is_research_task else "design" if is_design_task else "analysis",
                            "priority": "medium",
                            "dependencies": [],
                            "execution_order": len(tasks) + 1,
                            # ğŸ†• v7.109+: é»˜è®¤é…ç½®
                            "support_search": is_research_task,  # ç ”ç©¶ç±»ä»»åŠ¡é»˜è®¤éœ€è¦æœç´¢
                            "needs_concept_image": is_design_task,  # è®¾è®¡ç±»ä»»åŠ¡é»˜è®¤éœ€è¦æ¦‚å¿µå›¾
                            "concept_image_count": 1 if is_design_task else 0,
                        }
                    )

        # é™åˆ¶ä»»åŠ¡æ•°é‡
        return tasks[:7] if tasks else []

    async def _infer_task_metadata_async(
        self, tasks: List[Dict[str, Any]], user_input: str = "", structured_data: Optional[Dict[str, Any]] = None
    ) -> None:
        """
        å¼‚æ­¥æ¨æ–­ä»»åŠ¡å…ƒæ•°æ®ï¼ˆåŠ¨æœºç±»å‹ã€æ¨ç†ä¾æ®ç­‰ï¼‰

        v7.106: ä¸ºæ¯ä¸ªä»»åŠ¡æ·»åŠ ï¼š
        - motivation_type: åŠ¨æœºç±»å‹IDï¼ˆå¦‚'cultural'ï¼‰
        - motivation_label: åŠ¨æœºç±»å‹ä¸­æ–‡æ ‡ç­¾ï¼ˆå¦‚'æ–‡åŒ–è®¤åŒ'ï¼‰
        - ai_reasoning: LLMæ¨ç†ä¾æ®
        - confidence_score: ç½®ä¿¡åº¦åˆ†æ•°

        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨
            user_input: ç”¨æˆ·åŸå§‹è¾“å…¥
            structured_data: éœ€æ±‚åˆ†æé˜¶æ®µäº§å‡ºçš„ç»“æ„åŒ–æ•°æ®
        """
        if not tasks:
            return

        from ..services.motivation_engine import get_motivation_engine

        engine = get_motivation_engine()
        logger.info(f"ğŸ”§ [v7.106] ä½¿ç”¨åŠ¨æœºè¯†åˆ«å¼•æ“å¤„ç† {len(tasks)} ä¸ªä»»åŠ¡")

        # ğŸš€ v7.122: å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æ¨æ–­ä»»åŠ¡ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰
        import asyncio
        import time

        start_time = time.time()

        # åˆ›å»ºæ‰€æœ‰æ¨æ–­åç¨‹
        async def infer_single_task(task):
            """å•ä¸ªä»»åŠ¡çš„æ¨æ–­åŒ…è£…å‡½æ•°"""
            try:
                # æ‰§è¡Œå¼‚æ­¥æ¨æ–­
                result = await engine.infer(task=task, user_input=user_input, structured_data=structured_data)

                # è¿”å›æˆåŠŸç»“æœ
                return {"task": task, "success": True, "result": result}

            except Exception as e:
                # è¿”å›å¤±è´¥ç»“æœ
                return {"task": task, "success": False, "error": e}

        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        inference_coroutines = [infer_single_task(task) for task in tasks]
        results = await asyncio.gather(*inference_coroutines)

        # å¤„ç†ç»“æœå¹¶æ›´æ–°ä»»åŠ¡
        for result_data in results:
            task = result_data["task"]

            if result_data["success"]:
                result = result_data["result"]
                task["motivation_type"] = result.primary
                task["motivation_label"] = result.primary_label
                task["ai_reasoning"] = result.reasoning
                task["confidence_score"] = result.confidence
                logger.info(f"   âœ… {task['title'][:30]}: {result.primary_label} ({result.confidence:.2f})")
            else:
                error = result_data["error"]
                logger.warning(f"âš ï¸ ä»»åŠ¡ '{task.get('title', 'unknown')}' åŠ¨æœºæ¨æ–­å¤±è´¥: {error}")
                # é™çº§åˆ°é»˜è®¤
                task["motivation_type"] = "mixed"
                task["motivation_label"] = "ç»¼åˆ"
                task["ai_reasoning"] = "æ¨æ–­å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç±»å‹"
                task["confidence_score"] = 0.3

        elapsed_time = time.time() - start_time
        logger.info(f"âš¡ [å¹¶è¡Œä¼˜åŒ–] {len(tasks)} ä¸ªä»»åŠ¡æ¨æ–­å®Œæˆï¼Œè€—æ—¶ {elapsed_time:.2f}s")


async def decompose_core_tasks(
    user_input: str, structured_data: Optional[Dict[str, Any]] = None, llm: Optional[Any] = None
) -> List[Dict[str, Any]]:
    """
    å¼‚æ­¥æ‰§è¡Œæ ¸å¿ƒä»»åŠ¡æ‹†è§£ï¼ˆv7.110.0 æ™ºèƒ½åŒ–ç‰ˆæœ¬ï¼‰

    Args:
        user_input: ç”¨æˆ·åŸå§‹è¾“å…¥
        structured_data: éœ€æ±‚åˆ†æé˜¶æ®µäº§å‡ºçš„ç»“æ„åŒ–æ•°æ®ï¼ˆå¯é€‰ï¼‰
        llm: LLM å®ä¾‹ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤ LLMï¼‰

    Returns:
        ä»»åŠ¡åˆ—è¡¨ï¼ˆ3-12ä¸ªï¼Œæ ¹æ®è¾“å…¥å¤æ‚åº¦åŠ¨æ€å†³å®šï¼‰
    """
    decomposer = CoreTaskDecomposer()

    # ğŸ†• v7.110.0: æ™ºèƒ½åˆ†æè¾“å…¥å¤æ‚åº¦ï¼ŒåŠ¨æ€å†³å®šä»»åŠ¡æ•°é‡
    complexity_analysis = TaskComplexityAnalyzer.analyze(user_input, structured_data)
    recommended_min = complexity_analysis["recommended_min"]
    recommended_max = complexity_analysis["recommended_max"]
    complexity_score = complexity_analysis["complexity_score"]

    logger.info(f"ğŸ¯ [æ™ºèƒ½ä»»åŠ¡æ•°é‡] æ¨èèŒƒå›´: {recommended_min}-{recommended_max}ä¸ª (å¤æ‚åº¦={complexity_score:.2f})")
    logger.info(f"   åˆ†æä¾æ®: {complexity_analysis['reasoning']}")

    # æ„å»º promptï¼ˆä¼ å…¥æ™ºèƒ½è®¡ç®—çš„ä»»åŠ¡æ•°é‡ï¼‰
    prompt = decomposer.build_prompt(user_input, structured_data, task_count_range=(recommended_min, recommended_max))

    # å¦‚æœæ²¡æœ‰æä¾› LLMï¼Œä½¿ç”¨é»˜è®¤
    if llm is None:
        from ..services.llm_factory import LLMFactory

        llm = LLMFactory.create_llm()

    try:
        # è°ƒç”¨ LLM
        from langchain_core.messages import HumanMessage, SystemMessage

        system_prompt = decomposer.config.get("system_prompt", "")
        user_template = decomposer.config.get("user_prompt_template", "")

        # æ„å»ºç»“æ„åŒ–æ•°æ®æ‘˜è¦
        structured_summary = ""
        if structured_data:
            summary_parts = []
            for key in ["project_task", "character_narrative", "physical_context", "project_type"]:
                if structured_data.get(key):
                    summary_parts.append(f"{key}: {structured_data[key]}")
            structured_summary = "\n".join(summary_parts) if summary_parts else "æš‚æ— "

        # ğŸ†• v7.117: ç›´æ¥ä½¿ç”¨å ä½ç¬¦ä¼ å…¥ä»»åŠ¡æ•°é‡ï¼ˆä¸å†ä½¿ç”¨æ­£åˆ™æ›¿æ¢ï¼‰
        user_prompt = user_template.format(
            user_input=user_input,
            structured_data_summary=structured_summary,
            task_count_min=recommended_min,
            task_count_max=recommended_max,
        )

        # ğŸ”§ v7.117: å¢å¼ºè°ƒè¯•æ—¥å¿—
        logger.info(f"ğŸ¯ [æ™ºèƒ½ä»»åŠ¡æ•°é‡] è¾“å…¥é•¿åº¦={len(user_input)}å­—ç¬¦")
        logger.info(f"ğŸ¯ [æ™ºèƒ½ä»»åŠ¡æ•°é‡] å¤æ‚åº¦åˆ†æ: {complexity_analysis['reasoning']}")
        logger.info(f"ğŸ“ [Prompt] ä»»åŠ¡æ•°é‡è¦æ±‚: {recommended_min}-{recommended_max}ä¸ª")

        messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_prompt)]

        response = await llm.ainvoke(messages)
        response_text = response.content if hasattr(response, "content") else str(response)

        # è§£æå“åº”
        tasks = decomposer.parse_response(response_text)

        if not tasks:
            # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•å›é€€
            logger.warning("âš ï¸ LLM ä»»åŠ¡æ‹†è§£ä¸ºç©ºï¼Œä½¿ç”¨å›é€€ç­–ç•¥")
            tasks = _simple_fallback_decompose(user_input, structured_data, complexity_analysis)
        else:
            # ğŸ†• v7.110.0: æ™ºèƒ½æˆªæ–­ - ä¸è¶…è¿‡æ¨èæœ€å¤§å€¼
            if len(tasks) > recommended_max:
                logger.warning(f"âš ï¸ LLMç”Ÿæˆäº†{len(tasks)}ä¸ªä»»åŠ¡ï¼Œè¶…è¿‡æ¨èä¸Šé™{recommended_max}ï¼Œæ™ºèƒ½æˆªæ–­")
                # ä¼˜å…ˆä¿ç•™ high priority çš„ä»»åŠ¡
                high_priority_tasks = [t for t in tasks if t.get("priority") == "high"]
                other_tasks = [t for t in tasks if t.get("priority") != "high"]
                tasks = (high_priority_tasks + other_tasks)[:recommended_max]

            # å¦‚æœå°‘äºæ¨èæœ€å°å€¼ï¼Œè®°å½•è­¦å‘Šä½†ä¸å¼ºåˆ¶è¡¥é½ï¼ˆè®©LLMåˆ¤æ–­æ›´çµæ´»ï¼‰
            if len(tasks) < recommended_min:
                logger.info(f"â„¹ï¸ LLMç”Ÿæˆäº†{len(tasks)}ä¸ªä»»åŠ¡ï¼Œå°‘äºæ¨èæœ€å°å€¼{recommended_min}ï¼ˆå¯èƒ½æ˜¯ç®€å•éœ€æ±‚ï¼‰")

        # ğŸ†• v7.106: ä½¿ç”¨åŠ¨æœºè¯†åˆ«å¼•æ“ä¸ºä»»åŠ¡æ·»åŠ motivation_labelå­—æ®µ
        if tasks:
            await decomposer._infer_task_metadata_async(tasks, user_input, structured_data)

        logger.info(f"âœ… [ä»»åŠ¡æ‹†è§£å®Œæˆ] æœ€ç»ˆç”Ÿæˆ{len(tasks)}ä¸ªä»»åŠ¡")
        return tasks

    except Exception as e:
        logger.error(f"âŒ [decompose_core_tasks] LLM è°ƒç”¨å¤±è´¥: {e}")
        # å›é€€åˆ°ç®€å•æ‹†è§£
        return _simple_fallback_decompose(user_input, structured_data, complexity_analysis)


def _simple_fallback_decompose(
    user_input: str,
    structured_data: Optional[Dict[str, Any]] = None,
    complexity_analysis: Optional[Dict[str, Any]] = None,
) -> List[Dict[str, Any]]:
    """
    å¢å¼ºç‰ˆå›é€€æ‹†è§£ç­–ç•¥ï¼ˆv7.110.0 æ™ºèƒ½åŒ–ç‰ˆæœ¬ï¼‰

    å½“ LLM è°ƒç”¨å¤±è´¥æ—¶ï¼ŒåŸºäºç»“æ„åŒ–æ•°æ®çš„å¤šç»´åº¦ä¿¡æ¯ç”Ÿæˆé«˜è´¨é‡ä»»åŠ¡åˆ—è¡¨ã€‚
    ä¸å†ç¡¬ç¼–ç 5-7ä¸ªï¼Œè€Œæ˜¯æ ¹æ®å¤æ‚åº¦åˆ†æåŠ¨æ€è°ƒæ•´ä»»åŠ¡æ•°é‡ã€‚

    å‚è€ƒï¼šæ ¡å‡†é—®å·çš„æ™ºèƒ½è¡¥é½æœºåˆ¶ï¼ˆquestionnaire_optimization_summary.mdï¼‰

    Args:
        user_input: ç”¨æˆ·åŸå§‹è¾“å…¥
        structured_data: éœ€æ±‚åˆ†æäº§å‡ºçš„ç»“æ„åŒ–æ•°æ®ï¼ˆåŒ…å« design_challenge, character_narrative ç­‰ï¼‰
        complexity_analysis: å¤æ‚åº¦åˆ†æç»“æœï¼ˆå¯é€‰ï¼‰

    Returns:
        ä»»åŠ¡åˆ—è¡¨ï¼ˆ3-12ä¸ªï¼Œæ ¹æ®å¤æ‚åº¦åŠ¨æ€å†³å®šï¼‰
    """

    tasks = []
    logger.info("ğŸ”„ [Fallback] ä½¿ç”¨å¢å¼ºç‰ˆå›é€€ç­–ç•¥ï¼ŒåŸºäºç»“æ„åŒ–æ•°æ®ç”Ÿæˆä»»åŠ¡")

    # ğŸ†• v7.110.0: è·å–æ™ºèƒ½æ¨èçš„ä»»åŠ¡æ•°é‡èŒƒå›´
    if complexity_analysis is None:
        complexity_analysis = TaskComplexityAnalyzer.analyze(user_input, structured_data)

    recommended_min = complexity_analysis["recommended_min"]
    recommended_max = complexity_analysis["recommended_max"]
    logger.info(f"ğŸ¯ [Fallbackæ™ºèƒ½] ç›®æ ‡ä»»åŠ¡æ•°: {recommended_min}-{recommended_max}ä¸ª")

    # ä»ç»“æ„åŒ–æ•°æ®ä¸­æå–å…³é”®ä¿¡æ¯
    design_challenge = ""
    character_narrative = ""
    project_type = ""
    if structured_data:
        design_challenge = structured_data.get("design_challenge", "")
        character_narrative = structured_data.get("character_narrative", "")
        project_type = structured_data.get("project_type", "")
        logger.info(f"ğŸ“Š [Fallback] æå–åˆ°ç»“æ„åŒ–æ•°æ®: design_challenge={bool(design_challenge)}, project_type={project_type}")

    # ==========================================================================
    # 1. æå–æ ¸å¿ƒå¼ åŠ›ä»»åŠ¡ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
    # ==========================================================================
    tension_a = ""
    tension_b = ""
    if design_challenge:
        # æ¨¡å¼1: "A"...ä¸..."B" æ ¼å¼ï¼ˆä¸­æ–‡å¼•å·ï¼‰
        match = re.search(r'"([^"]{2,30})"[^"]{0,50}ä¸[^"]{0,50}"([^"]{2,30})"', design_challenge)
        if match:
            tension_a = match.group(1).strip()
            tension_b = match.group(2).strip()
            logger.info(f'âœ… [Fallback] æå–æ ¸å¿ƒå¼ åŠ›: "{tension_a}" vs "{tension_b}"')
            tasks.append(
                {
                    "id": f"task_{len(tasks) + 1}",
                    "title": f"{tension_a}ä¸{tension_b}çš„å¹³è¡¡ç­–ç•¥ç ”ç©¶",
                    "description": f"ç ”ç©¶å¦‚ä½•åœ¨è®¾è®¡ä¸­å¹³è¡¡{tension_a}å’Œ{tension_b}çš„éœ€æ±‚ï¼Œæ‰¾åˆ°æœ€ä½³è§£å†³æ–¹æ¡ˆ",
                    "source_keywords": [tension_a, tension_b],
                    "task_type": "research",
                    "priority": "high",
                    "dependencies": [],
                    "execution_order": len(tasks) + 1,
                }
            )
        else:
            # æ¨¡å¼2: A vs B æˆ– Aä¸å…¶å¯¹B æ ¼å¼
            match = re.search(r"(.{5,30}?)[çš„éœ€æ±‚]*(?:vs|ä¸å…¶å¯¹)(.{5,30}?)[çš„éœ€æ±‚]*", design_challenge)
            if match:
                tension_a = match.group(1).strip()
                tension_b = match.group(2).strip()
                logger.info(f"âœ… [Fallback] æå–æ ¸å¿ƒå¼ åŠ› (æ¨¡å¼2): {tension_a} vs {tension_b}")
                tasks.append(
                    {
                        "id": f"task_{len(tasks) + 1}",
                        "title": f"{tension_a}ä¸{tension_b}çš„å¹³è¡¡ç­–ç•¥ç ”ç©¶",
                        "description": f"ç ”ç©¶å¦‚ä½•åœ¨è®¾è®¡ä¸­å¹³è¡¡{tension_a}å’Œ{tension_b}çš„éœ€æ±‚ï¼Œæ‰¾åˆ°æœ€ä½³è§£å†³æ–¹æ¡ˆ",
                        "source_keywords": [tension_a, tension_b],
                        "task_type": "research",
                        "priority": "high",
                        "dependencies": [],
                        "execution_order": len(tasks) + 1,
                    }
                )

    # ==========================================================================
    # 2. æå–å¯¹æ ‡æ¡ˆä¾‹ä»»åŠ¡
    # ==========================================================================
    benchmarking_keywords = ["å¯¹æ ‡", "å‚è€ƒ", "æ¡ˆä¾‹", "æ ‡æ†", "å¯¹æ¯”"]
    for keyword in benchmarking_keywords:
        if keyword in user_input and not any(keyword in t["title"] for t in tasks):
            # å°è¯•æå–å…·ä½“çš„å¯¹æ ‡å¯¹è±¡
            benchmark_patterns = [
                r"å¯¹æ ‡([^ï¼Œã€‚ã€ï¼ï¼Ÿ\n]{2,30})",  # "å¯¹æ ‡è‹å·é»„æ¡¥èœå¸‚åœº"
                r"å‚è€ƒ([^ï¼Œã€‚ã€ï¼ï¼Ÿ\n]{2,30})",
                r"([^ï¼Œã€‚ã€ï¼ï¼Ÿ\n]{2,30})æ¡ˆä¾‹",
            ]
            benchmark_target = ""
            for pattern in benchmark_patterns:
                match = re.search(pattern, user_input)
                if match:
                    benchmark_target = match.group(1).strip()
                    break

            if benchmark_target:
                tasks.append(
                    {
                        "id": f"task_{len(tasks) + 1}",
                        "title": f"å¯¹æ ‡æ¡ˆä¾‹æ·±åº¦ç ”ç©¶",
                        "description": f"é‡ç‚¹ç ”ç©¶{benchmark_target}ï¼Œå¹¶è°ƒç ”å…¶ä»–æˆåŠŸæ¡ˆä¾‹çš„è®¾è®¡ç­–ç•¥å’Œåˆ›æ–°è¦ç´ ",
                        "source_keywords": [keyword, benchmark_target],
                        "task_type": "research",
                        "priority": "high",
                        "dependencies": [],
                        "execution_order": len(tasks) + 1,
                    }
                )
            else:
                tasks.append(
                    {
                        "id": f"task_{len(tasks) + 1}",
                        "title": "æ ‡æ†æ¡ˆä¾‹ç ”ç©¶",
                        "description": "è°ƒç ”è¡Œä¸šå†…æˆåŠŸæ¡ˆä¾‹çš„è®¾è®¡ç­–ç•¥ã€è¿è¥æ¨¡å¼å’Œåˆ›æ–°è¦ç´ ",
                        "source_keywords": [keyword],
                        "task_type": "research",
                        "priority": "high",
                        "dependencies": [],
                        "execution_order": len(tasks) + 1,
                    }
                )
            break

    # ==========================================================================
    # 3. æå–æ–‡åŒ–è¦ç´ ä»»åŠ¡
    # ==========================================================================
    culture_keywords = ["æ–‡åŒ–", "ä¼ ç»Ÿ", "åœ¨åœ°", "å†å²", "ç‰¹è‰²"]
    for keyword in culture_keywords:
        if keyword in user_input and not any(keyword in t["title"] for t in tasks):
            # å°è¯•æå–å…·ä½“çš„æ–‡åŒ–å¯¹è±¡
            culture_patterns = [
                r"([^ï¼Œã€‚ã€ï¼ï¼Ÿ\n]{2,15})[æ–‡åŒ–ä¼ ç»Ÿ]",  # "è›‡å£æ¸”æ‘æ–‡åŒ–"
                r"èå…¥([^ï¼Œã€‚ã€ï¼ï¼Ÿ\n]{2,15})",
            ]
            culture_target = ""
            for pattern in culture_patterns:
                match = re.search(pattern, user_input)
                if match:
                    culture_target = match.group(1).strip()
                    if any(c in culture_target for c in ["æ–‡åŒ–", "ä¼ ç»Ÿ", "ç‰¹è‰²"]):
                        break

            if culture_target:
                tasks.append(
                    {
                        "id": f"task_{len(tasks) + 1}",
                        "title": f"{culture_target}æ–‡åŒ–æ´å¯Ÿä¸æç‚¼",
                        "description": f"æ·±å…¥è°ƒç ”{culture_target}çš„å†å²æ–‡è„‰å’Œç²¾ç¥å†…æ ¸ï¼Œæç‚¼å¯èå…¥è®¾è®¡çš„æ–‡åŒ–å…ƒç´ ",
                        "source_keywords": [keyword, culture_target],
                        "task_type": "research",
                        "priority": "high",
                        "dependencies": [],
                        "execution_order": len(tasks) + 1,
                    }
                )
            else:
                tasks.append(
                    {
                        "id": f"task_{len(tasks) + 1}",
                        "title": "æ–‡åŒ–è°ƒç ”ä¸å…ƒç´ æç‚¼",
                        "description": "è°ƒç ”é¡¹ç›®ç›¸å…³çš„æ–‡åŒ–èƒŒæ™¯ï¼Œæç‚¼å¯èå…¥è®¾è®¡çš„æ–‡åŒ–å…ƒç´ å’Œç¬¦å·",
                        "source_keywords": [keyword],
                        "task_type": "research",
                        "priority": "high",
                        "dependencies": [],
                        "execution_order": len(tasks) + 1,
                    }
                )
            break

    # ==========================================================================
    # 4. æå–å®¢ç¾¤åˆ†æä»»åŠ¡
    # ==========================================================================
    audience_keywords = ["å®¢ç¾¤", "ç”¨æˆ·", "è®¿å®¢", "å±…æ°‘", "å®¢æˆ·", "äººç¾¤", "å²", "å¥³æ€§", "ç”·æ€§"]
    for keyword in audience_keywords:
        if keyword in user_input and not any("å®¢ç¾¤" in t["title"] or "ç”Ÿæ´»æ–¹å¼" in t["title"] for t in tasks):
            # å°è¯•æå–å…·ä½“çš„å®¢ç¾¤æè¿°
            audience_patterns = [
                r"(\d+å²[^ï¼Œã€‚ã€ï¼ï¼Ÿ\n]{0,10})",  # "35å²å•èº«å¥³æ€§"
                r"([^ï¼Œã€‚ã€ï¼ï¼Ÿ\n]{2,15})[å®¢ç¾¤ç”¨æˆ·è®¿å®¢]",
                r"å…¼é¡¾([^ï¼Œã€‚ã€ï¼ï¼Ÿ\n]{5,50})",  # "å…¼é¡¾è›‡å£è€å±…æ°‘è¡—åŠã€é¦™æ¸¯è®¿å®¢..."
            ]
            audience_target = ""
            for pattern in audience_patterns:
                match = re.search(pattern, user_input)
                if match:
                    audience_target = match.group(1).strip()
                    break

            # ç‰¹åˆ«å¤„ç†: 35å²å•èº«å¥³æ€§ç­‰å¹´é¾„+ç‰¹å¾çš„æƒ…å†µ
            age_match = re.search(r"(\d+)å²([^ï¼Œã€‚ã€ï¼ï¼Ÿ\n]{0,10})", user_input)
            if age_match:
                age = age_match.group(1)
                traits = age_match.group(2).strip()
                tasks.append(
                    {
                        "id": f"task_{len(tasks) + 1}",
                        "title": f"{age}å²{traits}ç”Ÿæ´»æ–¹å¼ç ”ç©¶",
                        "description": f"æ·±å…¥åˆ†æ{age}å²{traits}çš„å®¡ç¾åå¥½ã€ç”Ÿæ´»åœºæ™¯ã€ç¤¾äº¤éœ€æ±‚å’Œç²¾ç¥è¿½æ±‚",
                        "source_keywords": [f"{age}å²", traits],
                        "task_type": "analysis",
                        "priority": "high",
                        "dependencies": [],
                        "execution_order": len(tasks) + 1,
                    }
                )
            elif audience_target:
                tasks.append(
                    {
                        "id": f"task_{len(tasks) + 1}",
                        "title": "å¤šå…ƒå®¢ç¾¤åˆ†æ",
                        "description": f"åˆ†æ{audience_target}çš„éœ€æ±‚å·®å¼‚ä¸å…±æ€§ï¼Œæ‰¾åˆ°è®¾è®¡çš„å¹³è¡¡ç‚¹",
                        "source_keywords": [keyword, audience_target],
                        "task_type": "analysis",
                        "priority": "high",
                        "dependencies": [],
                        "execution_order": len(tasks) + 1,
                    }
                )
            else:
                tasks.append(
                    {
                        "id": f"task_{len(tasks) + 1}",
                        "title": "ç›®æ ‡å®¢ç¾¤ç ”ç©¶",
                        "description": "åˆ†æç›®æ ‡ç”¨æˆ·çš„éœ€æ±‚ç‰¹å¾ã€ä½¿ç”¨ä¹ æƒ¯å’ŒæœŸæœ›ä½“éªŒ",
                        "source_keywords": [keyword],
                        "task_type": "analysis",
                        "priority": "high",
                        "dependencies": [],
                        "execution_order": len(tasks) + 1,
                    }
                )
            break

    # ==========================================================================
    # 5. æå–å“ç‰Œç›¸å…³ä»»åŠ¡ï¼ˆæ–°å¢ï¼šé’ˆå¯¹ Tiffany ç­‰å“ç‰Œä¸»é¢˜ï¼‰
    # ==========================================================================
    # æ£€æµ‹å“ç‰Œåï¼ˆé€šå¸¸æ˜¯è‹±æ–‡æˆ–å“ç‰Œå…³é”®è¯ï¼‰
    brand_patterns = [
        r"([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)",  # Tiffany, Louis Vuitton
        r"([^ï¼Œã€‚ã€ï¼ï¼Ÿ\n]{2,10})ä¸ºä¸»é¢˜",  # "è’‚èŠ™å°¼ä¸ºä¸»é¢˜"
        r"([^ï¼Œã€‚ã€ï¼ï¼Ÿ\n]{2,10})å“ç‰Œ",
    ]
    brand_name = ""
    for pattern in brand_patterns:
        match = re.search(pattern, user_input)
        if match:
            potential_brand = match.group(1).strip()
            # è¿‡æ»¤æ‰ä¸å¤ªåƒå“ç‰Œåçš„å†…å®¹
            if len(potential_brand) >= 2 and not any(c in potential_brand for c in ["å¹³ç±³", "å¹´", "è®¾è®¡", "é¡¹ç›®"]):
                brand_name = potential_brand
                break

    if brand_name and not any(brand_name in t["title"] for t in tasks):
        tasks.append(
            {
                "id": f"task_{len(tasks) + 1}",
                "title": f"{brand_name}å“ç‰Œæ–‡åŒ–æ´å¯Ÿ",
                "description": f"ç ”ç©¶{brand_name}çš„å“ç‰Œç²¾ç¥ã€è‰²å½©ä½“ç³»ã€ç»å…¸è®¾è®¡å…ƒç´ å’Œæ ¸å¿ƒä»·å€¼è§‚",
                "source_keywords": [brand_name, "å“ç‰Œ"],
                "task_type": "research",
                "priority": "high",
                "dependencies": [],
                "execution_order": len(tasks) + 1,
            }
        )

    # ==========================================================================
    # 6. æå–ç©ºé—´/è§„æ¨¡ç›¸å…³ä»»åŠ¡
    # ==========================================================================
    space_match = re.search(r"(\d+)å¹³ç±³", user_input)
    space_type_keywords = ["åˆ«å¢…", "ä½å®…", "å…¬å¯“", "åŠå…¬", "å•†ä¸š", "é›¶å”®", "é…’åº—"]
    space_type = next((kw for kw in space_type_keywords if kw in user_input), "")

    if (space_match or space_type) and not any("ç©ºé—´" in t["title"] or "è§„åˆ’" in t["title"] for t in tasks):
        if space_match and space_type:
            area = space_match.group(1)
            tasks.append(
                {
                    "id": f"task_{len(tasks) + 1}",
                    "title": f"{area}å¹³ç±³{space_type}ç©ºé—´åŠŸèƒ½è§„åˆ’",
                    "description": f"åˆ¶å®šç¬¦åˆç”¨æˆ·éœ€æ±‚çš„{area}å¹³ç±³{space_type}ç©ºé—´å¸ƒå±€ç­–ç•¥å’ŒåŠŸèƒ½åˆ†åŒºæ–¹æ¡ˆ",
                    "source_keywords": [f"{area}å¹³ç±³", space_type],
                    "task_type": "design",
                    "priority": "high",
                    "dependencies": [],
                    "execution_order": len(tasks) + 1,
                }
            )
        elif space_type:
            tasks.append(
                {
                    "id": f"task_{len(tasks) + 1}",
                    "title": f"{space_type}ç©ºé—´è®¾è®¡æ¡†æ¶",
                    "description": f"è¾“å‡º{space_type}ç©ºé—´çš„æ•´ä½“è®¾è®¡æ¡†æ¶å’ŒåŠŸèƒ½è§„åˆ’ç­–ç•¥",
                    "source_keywords": [space_type],
                    "task_type": "design",
                    "priority": "high",
                    "dependencies": [],
                    "execution_order": len(tasks) + 1,
                }
            )

    # ==========================================================================
    # 7. è¡¥å……ï¼šæœ€ç»ˆäº¤ä»˜ç‰©ä»»åŠ¡
    # ==========================================================================
    deliverable_keywords = ["æ¡†æ¶", "æ–¹æ¡ˆ", "æ¦‚å¿µ", "è®¾è®¡æ€è·¯"]
    for keyword in deliverable_keywords:
        if keyword in user_input and not any(keyword in t["title"] for t in tasks):
            # æå–äº¤ä»˜ç‰©ç±»å‹
            deliverable_patterns = [
                r"([^ï¼Œã€‚ã€ï¼ï¼Ÿ\n]{2,10})æ¡†æ¶",
                r"([^ï¼Œã€‚ã€ï¼ï¼Ÿ\n]{2,10})æ–¹æ¡ˆ",
                r"([^ï¼Œã€‚ã€ï¼ï¼Ÿ\n]{2,10})è®¾è®¡æ€è·¯",
            ]
            deliverable_target = ""
            for pattern in deliverable_patterns:
                match = re.search(pattern, user_input)
                if match:
                    deliverable_target = match.group(1).strip()
                    break

            if deliverable_target:
                tasks.append(
                    {
                        "id": f"task_{len(tasks) + 1}",
                        "title": f"{deliverable_target}è®¾è®¡æ¡†æ¶è¾“å‡º",
                        "description": f"è¾“å‡º{deliverable_target}çš„æ•´ä½“è®¾è®¡æ¡†æ¶ï¼Œèåˆå‰æœŸç ”ç©¶æˆæœå’Œè®¾è®¡ç­–ç•¥",
                        "source_keywords": [keyword, deliverable_target],
                        "task_type": "design",
                        "priority": "high",
                        "dependencies": [],
                        "execution_order": len(tasks) + 1,
                    }
                )
            break

    # ==========================================================================
    # 8. ğŸ†• v7.110.0: æ™ºèƒ½è¡¥é½å’Œæˆªæ–­ï¼ˆä¸å†å›ºå®š5-7ä¸ªï¼‰
    # ==========================================================================
    current_count = len(tasks)

    if current_count < recommended_min:
        logger.warning(f"âš ï¸ [Fallback] å½“å‰ä»…ç”Ÿæˆ {current_count} ä¸ªä»»åŠ¡ï¼Œè¡¥å……é€šç”¨ä»»åŠ¡è‡³{recommended_min}ä¸ª")
        # è¡¥å……é€šç”¨ä»»åŠ¡
        generic_tasks = [
            {"title": "é¡¹ç›®éœ€æ±‚æ˜ç¡®", "description": "æ˜ç¡®é¡¹ç›®çš„æ ¸å¿ƒéœ€æ±‚ã€æœŸæœ›ç›®æ ‡å’Œå…³é”®çº¦æŸæ¡ä»¶", "task_type": "analysis", "priority": "high"},
            {"title": "è®¾è®¡ç­–ç•¥åˆ¶å®š", "description": "åˆ¶å®šæ•´ä½“è®¾è®¡ç­–ç•¥å’Œå®æ–½è·¯å¾„ï¼Œç¡®å®šè®¾è®¡çš„æ ¸å¿ƒæ–¹å‘", "task_type": "design", "priority": "medium"},
            {"title": "è¡Œä¸šè¶‹åŠ¿ç ”ç©¶", "description": "ç ”ç©¶ç›¸å…³é¢†åŸŸçš„æœ€æ–°è¶‹åŠ¿ã€æˆåŠŸæ¡ˆä¾‹å’Œåˆ›æ–°å®è·µ", "task_type": "research", "priority": "medium"},
            {"title": "è®¾è®¡æ¡†æ¶è¾“å‡º", "description": "æ•´åˆå‰æœŸç ”ç©¶æˆæœï¼Œè¾“å‡ºæ•´ä½“è®¾è®¡æ¡†æ¶å’Œå®æ–½å»ºè®®", "task_type": "output", "priority": "medium"},
        ]

        for generic_task in generic_tasks:
            if len(tasks) >= recommended_min:
                break
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç±»ä¼¼ä»»åŠ¡
            if not any(generic_task["title"] in t["title"] or t["title"] in generic_task["title"] for t in tasks):
                tasks.append(
                    {
                        "id": f"task_{len(tasks) + 1}",
                        **generic_task,
                        "source_keywords": [],
                        "dependencies": [],
                        "execution_order": len(tasks) + 1,
                    }
                )

    # ğŸ†• v7.110.0: æ™ºèƒ½æˆªæ–­ - ä¸è¶…è¿‡æ¨èæœ€å¤§å€¼ï¼ˆä¼˜å…ˆä¿ç•™é«˜ä¼˜å…ˆçº§ï¼‰
    if len(tasks) > recommended_max:
        logger.warning(f"âš ï¸ [Fallback] ç”Ÿæˆäº†{len(tasks)}ä¸ªä»»åŠ¡ï¼Œè¶…è¿‡æ¨èä¸Šé™{recommended_max}ï¼Œæ™ºèƒ½æˆªæ–­")
        # ä¼˜å…ˆä¿ç•™ high priority çš„ä»»åŠ¡
        high_priority_tasks = [t for t in tasks if t.get("priority") == "high"]
        other_tasks = [t for t in tasks if t.get("priority") != "high"]
        tasks = (high_priority_tasks + other_tasks)[:recommended_max]

        # é‡æ–°ç¼–å·
        for idx, task in enumerate(tasks, 1):
            task["id"] = f"task_{idx}"
            task["execution_order"] = idx

    logger.info(
        f"âœ… [Fallback] ç”Ÿæˆ {len(tasks)} ä¸ªä»»åŠ¡ï¼ˆæ¨èèŒƒå›´{recommended_min}-{recommended_max}ï¼ŒåŒ…å« {sum(1 for t in tasks if t['priority'] == 'high')} ä¸ªé«˜ä¼˜å…ˆçº§ä»»åŠ¡ï¼‰"
    )

    return tasks


# åŒæ­¥ç‰ˆæœ¬ï¼ˆç”¨äºéå¼‚æ­¥ä¸Šä¸‹æ–‡ï¼‰
def decompose_core_tasks_sync(
    user_input: str, structured_data: Optional[Dict[str, Any]] = None, llm: Optional[Any] = None
) -> List[Dict[str, Any]]:
    """
    åŒæ­¥æ‰§è¡Œæ ¸å¿ƒä»»åŠ¡æ‹†è§£

    Args:
        user_input: ç”¨æˆ·åŸå§‹è¾“å…¥
        structured_data: éœ€æ±‚åˆ†æé˜¶æ®µäº§å‡ºçš„ç»“æ„åŒ–æ•°æ®ï¼ˆå¯é€‰ï¼‰
        llm: LLM å®ä¾‹ï¼ˆå¯é€‰ï¼‰

    Returns:
        ä»»åŠ¡åˆ—è¡¨
    """
    import asyncio

    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # å¦‚æœå·²ç»åœ¨äº‹ä»¶å¾ªç¯ä¸­ï¼Œä½¿ç”¨å›é€€ç­–ç•¥
            logger.warning("âš ï¸ æ£€æµ‹åˆ°è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œä½¿ç”¨å›é€€ç­–ç•¥")
            return _simple_fallback_decompose(user_input)
        else:
            return loop.run_until_complete(decompose_core_tasks(user_input, structured_data, llm))
    except RuntimeError:
        # æ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºä¸€ä¸ª
        return asyncio.run(decompose_core_tasks(user_input, structured_data, llm))
