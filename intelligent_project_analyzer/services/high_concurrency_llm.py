# -*- coding: utf-8 -*-
"""
é«˜å¹¶å‘ LLM å®¢æˆ·ç«¯

æ•´åˆï¼š
1. å¤š API Key è´Ÿè½½å‡è¡¡
2. å¤šæä¾›å•†æ•…éšœè½¬ç§»
3. è¯·æ±‚é™æµ
4. è‡ªåŠ¨é‡è¯•
5. ç»“æœç¼“å­˜
"""

import asyncio
import hashlib
import json
import time
from typing import Any, Dict, List, Optional, Callable
from functools import lru_cache
from loguru import logger

from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage

from intelligent_project_analyzer.services.key_balancer import (
    key_balancer,
    APIKeyInfo,
    get_api_key_with_callback
)
from intelligent_project_analyzer.services.rate_limiter import (
    rate_limit_manager,
    RateLimitExceeded
)


# æä¾›å•†é…ç½®
PROVIDER_CONFIGS = {
    "openai": {
        "base_url": None,
        "default_model": "gpt-4o-mini",
    },
    "deepseek": {
        "base_url": "https://api.deepseek.com/v1",
        "default_model": "deepseek-chat",
    },
    "qwen": {
        "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
        "default_model": "qwen-plus",
    },
    "openrouter": {
        "base_url": "https://openrouter.ai/api/v1",
        "default_model": "openai/gpt-4o-mini",
    },
}


class SimpleCache:
    """ç®€å•çš„ LRU ç¼“å­˜"""
    
    def __init__(self, max_size: int = 1000, ttl: int = 3600):
        self.max_size = max_size
        self.ttl = ttl
        self._cache: Dict[str, tuple] = {}  # key -> (value, timestamp)
    
    def _hash_key(self, prompt: str, model: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{model}:{prompt}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def get(self, prompt: str, model: str) -> Optional[str]:
        """è·å–ç¼“å­˜"""
        key = self._hash_key(prompt, model)
        if key in self._cache:
            value, timestamp = self._cache[key]
            if time.time() - timestamp < self.ttl:
                return value
            else:
                del self._cache[key]
        return None
    
    def set(self, prompt: str, model: str, value: str):
        """è®¾ç½®ç¼“å­˜"""
        if len(self._cache) >= self.max_size:
            # åˆ é™¤æœ€æ—§çš„
            oldest_key = min(self._cache.keys(), key=lambda k: self._cache[k][1])
            del self._cache[oldest_key]
        
        key = self._hash_key(prompt, model)
        self._cache[key] = (value, time.time())
    
    @property
    def stats(self) -> Dict[str, Any]:
        return {
            "size": len(self._cache),
            "max_size": self.max_size,
            "ttl": self.ttl
        }


class HighConcurrencyLLM:
    """
    é«˜å¹¶å‘ LLM å®¢æˆ·ç«¯
    
    ç‰¹æ€§ï¼š
    - å¤š Key è‡ªåŠ¨è½®è¯¢
    - å¤šæä¾›å•†æ•…éšœè½¬ç§»
    - è¯·æ±‚é™æµ
    - è‡ªåŠ¨é‡è¯•
    - ç»“æœç¼“å­˜
    
    Usage:
        llm = HighConcurrencyLLM(preferred_provider="openai")
        
        # åŒæ­¥è°ƒç”¨
        result = llm.invoke("Hello")
        
        # å¼‚æ­¥è°ƒç”¨
        result = await llm.ainvoke("Hello")
        
        # å¹¶å‘æ‰¹é‡è°ƒç”¨
        results = await llm.abatch(["Hello", "Hi", "Hey"])
    """
    
    def __init__(
        self,
        preferred_provider: str = "openai",
        model: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 4096,
        timeout: int = 60,
        max_retries: int = 3,
        enable_cache: bool = True,
        cache_ttl: int = 3600,
        enable_fallback: bool = True,
    ):
        self.preferred_provider = preferred_provider
        self.model = model or PROVIDER_CONFIGS.get(preferred_provider, {}).get("default_model", "gpt-4o-mini")
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout
        self.max_retries = max_retries
        self.enable_fallback = enable_fallback
        
        # ç¼“å­˜
        self.cache = SimpleCache(ttl=cache_ttl) if enable_cache else None
        
        # ç»Ÿè®¡
        self._total_calls = 0
        self._cache_hits = 0
        self._fallback_count = 0
        
        logger.info(
            f"ğŸš€ é«˜å¹¶å‘ LLM åˆå§‹åŒ–: provider={preferred_provider}, "
            f"model={self.model}, fallback={enable_fallback}"
        )
    
    def _create_llm(self, provider: str, api_key: str) -> ChatOpenAI:
        """åˆ›å»º LLM å®ä¾‹"""
        config = PROVIDER_CONFIGS.get(provider, {})
        
        params = {
            "model": self.model if provider == self.preferred_provider else config.get("default_model", self.model),
            "api_key": api_key,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "timeout": self.timeout,
        }
        
        if config.get("base_url"):
            params["base_url"] = config["base_url"]
        
        return ChatOpenAI(**params)
    
    def _get_prompt_str(self, input: Any) -> str:
        """æå– prompt å­—ç¬¦ä¸²ï¼ˆç”¨äºç¼“å­˜ï¼‰"""
        if isinstance(input, str):
            return input
        elif isinstance(input, list):
            return str([m.content if hasattr(m, 'content') else str(m) for m in input])
        else:
            return str(input)
    
    def invoke(self, input: Any, **kwargs) -> Any:
        """åŒæ­¥è°ƒç”¨"""
        self._total_calls += 1
        prompt_str = self._get_prompt_str(input)
        
        # æ£€æŸ¥ç¼“å­˜
        if self.cache:
            cached = self.cache.get(prompt_str, self.model)
            if cached:
                self._cache_hits += 1
                logger.debug(f"ğŸ“¦ ç¼“å­˜å‘½ä¸­")
                return AIMessage(content=cached)
        
        # è·å–é™æµå™¨
        limiter = rate_limit_manager.get_limiter(self.preferred_provider)
        
        # é‡è¯•å¾ªç¯
        last_error = None
        providers_tried = []
        
        for attempt in range(self.max_retries):
            # è·å– Key
            key_info, provider = key_balancer.get_key_with_fallback(self.preferred_provider)
            
            if not key_info:
                logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„ API Key")
                raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„ API Key")
            
            if provider != self.preferred_provider:
                self._fallback_count += 1
            
            providers_tried.append(provider)
            
            try:
                # è·å–é™æµè®¸å¯
                if not limiter.acquire_sync():
                    logger.warning(f"âš ï¸ é™æµç­‰å¾…è¶…æ—¶ï¼Œå°è¯•ä¸‹ä¸€ä¸ª Key")
                    continue
                
                try:
                    # åˆ›å»º LLM å¹¶è°ƒç”¨
                    llm = self._create_llm(provider, key_info.key)
                    result = llm.invoke(input, **kwargs)
                    
                    # æŠ¥å‘ŠæˆåŠŸ
                    key_balancer.report_success(key_info)
                    
                    # ç¼“å­˜ç»“æœ
                    if self.cache and hasattr(result, 'content'):
                        self.cache.set(prompt_str, self.model, result.content)
                    
                    return result
                    
                finally:
                    limiter.release_sync()
                    
            except Exception as e:
                last_error = e
                error_str = str(e).lower()
                
                # åˆ¤æ–­é”™è¯¯ç±»å‹
                if "rate limit" in error_str or "429" in error_str:
                    key_balancer.report_failure(key_info, is_rate_limit=True)
                elif "quota" in error_str or "exceeded" in error_str:
                    key_balancer.report_failure(key_info, is_quota_exceeded=True)
                elif "invalid" in error_str or "unauthorized" in error_str or "401" in error_str:
                    key_balancer.report_failure(key_info, is_invalid=True)
                else:
                    key_balancer.report_failure(key_info)
                
                logger.warning(f"âš ï¸ è°ƒç”¨å¤±è´¥ (attempt {attempt + 1}): {e}")
                
                if not self.enable_fallback:
                    break
        
        raise RuntimeError(f"æ‰€æœ‰é‡è¯•å¤±è´¥: {last_error}, å°è¯•è¿‡: {providers_tried}")
    
    async def ainvoke(self, input: Any, **kwargs) -> Any:
        """å¼‚æ­¥è°ƒç”¨"""
        self._total_calls += 1
        prompt_str = self._get_prompt_str(input)
        
        # æ£€æŸ¥ç¼“å­˜
        if self.cache:
            cached = self.cache.get(prompt_str, self.model)
            if cached:
                self._cache_hits += 1
                return AIMessage(content=cached)
        
        # è·å–é™æµå™¨
        limiter = rate_limit_manager.get_limiter(self.preferred_provider)
        
        last_error = None
        providers_tried = []
        
        for attempt in range(self.max_retries):
            key_info, provider = key_balancer.get_key_with_fallback(self.preferred_provider)
            
            if not key_info:
                raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„ API Key")
            
            if provider != self.preferred_provider:
                self._fallback_count += 1
            
            providers_tried.append(provider)
            
            try:
                if not await limiter.acquire_async():
                    continue
                
                try:
                    llm = self._create_llm(provider, key_info.key)
                    result = await llm.ainvoke(input, **kwargs)
                    
                    key_balancer.report_success(key_info)
                    
                    if self.cache and hasattr(result, 'content'):
                        self.cache.set(prompt_str, self.model, result.content)
                    
                    return result
                    
                finally:
                    limiter.release_async()
                    
            except Exception as e:
                last_error = e
                error_str = str(e).lower()
                
                if "rate limit" in error_str or "429" in error_str:
                    key_balancer.report_failure(key_info, is_rate_limit=True)
                elif "quota" in error_str:
                    key_balancer.report_failure(key_info, is_quota_exceeded=True)
                elif "invalid" in error_str or "401" in error_str:
                    key_balancer.report_failure(key_info, is_invalid=True)
                else:
                    key_balancer.report_failure(key_info)
                
                logger.warning(f"âš ï¸ å¼‚æ­¥è°ƒç”¨å¤±è´¥ (attempt {attempt + 1}): {e}")
                
                if not self.enable_fallback:
                    break
        
        raise RuntimeError(f"æ‰€æœ‰é‡è¯•å¤±è´¥: {last_error}")
    
    async def abatch(
        self,
        inputs: List[Any],
        max_concurrent: int = 5,
        enable_adaptive: bool = True,  # ğŸ†• P3ä¼˜åŒ–ï¼šå¯ç”¨è‡ªé€‚åº”å¹¶å‘æ§åˆ¶
        **kwargs
    ) -> List[Any]:
        """
        å¼‚æ­¥æ‰¹é‡è°ƒç”¨ï¼ˆæ§åˆ¶å¹¶å‘ï¼‰

        ğŸš€ P3ä¼˜åŒ–ï¼šæ”¯æŒè‡ªé€‚åº”å¹¶å‘æ§åˆ¶ï¼ŒåŸºäº 429 é”™è¯¯åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°

        Args:
            inputs: è¾“å…¥åˆ—è¡¨
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            enable_adaptive: æ˜¯å¦å¯ç”¨è‡ªé€‚åº”å¹¶å‘æ§åˆ¶ï¼ˆé»˜è®¤Trueï¼‰
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            ç»“æœåˆ—è¡¨
        """
        # ğŸš€ P3ä¼˜åŒ–ï¼šä½¿ç”¨è‡ªé€‚åº”ä¿¡å·é‡
        if enable_adaptive:
            semaphore = AdaptiveSemaphore(
                initial_concurrent=max_concurrent,
                min_concurrent=max(1, max_concurrent // 2),  # æœ€å°ä¸ºåˆå§‹å€¼çš„ä¸€åŠ
                max_concurrent=max_concurrent * 2,  # æœ€å¤§ä¸ºåˆå§‹å€¼çš„ä¸¤å€
                increase_threshold=10,  # è¿ç»­æˆåŠŸ10æ¬¡åå¢åŠ 
                increase_step=1,
                decrease_step=1
            )
        else:
            # ä½¿ç”¨å›ºå®šä¿¡å·é‡ï¼ˆå‘åå…¼å®¹ï¼‰
            semaphore = asyncio.Semaphore(max_concurrent)

        async def limited_call(input):
            if enable_adaptive:
                async with semaphore:
                    try:
                        result = await self.ainvoke(input, **kwargs)
                        semaphore.report_success()  # æŠ¥å‘ŠæˆåŠŸ
                        return result
                    except Exception as e:
                        error_str = str(e).lower()
                        if "429" in error_str or "rate limit" in error_str:
                            semaphore.report_rate_limit()  # æŠ¥å‘Šé™æµ
                        raise
            else:
                async with semaphore:
                    return await self.ainvoke(input, **kwargs)

        tasks = [limited_call(input) for input in inputs]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†å¼‚å¸¸
        final_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"âŒ æ‰¹é‡è°ƒç”¨ç¬¬ {i} é¡¹å¤±è´¥: {result}")
                final_results.append(None)
            else:
                final_results.append(result)

        # ğŸš€ P3ä¼˜åŒ–ï¼šè®°å½•è‡ªé€‚åº”ç»Ÿè®¡
        if enable_adaptive and isinstance(semaphore, AdaptiveSemaphore):
            stats = semaphore.stats
            logger.info(
                f"ğŸ“Š [AdaptiveSemaphore] æ‰¹é‡è°ƒç”¨å®Œæˆ: "
                f"å¹¶å‘æ•°={stats['current_concurrent']}, "
                f"é™æµæ¬¡æ•°={stats['rate_limit_count']}, "
                f"è°ƒæ•´æ¬¡æ•°={stats['increase_count'] + stats['decrease_count']}"
            )

        return final_results
    
    @property
    def stats(self) -> Dict[str, Any]:
        """ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_calls": self._total_calls,
            "cache_hits": self._cache_hits,
            "cache_hit_rate": f"{self._cache_hits / max(1, self._total_calls):.1%}",
            "fallback_count": self._fallback_count,
            "cache_stats": self.cache.stats if self.cache else None,
            "key_stats": key_balancer.get_all_stats(),
        }


# ==================== ä¾¿æ·å‡½æ•° ====================

_default_llm: Optional[HighConcurrencyLLM] = None


def get_high_concurrency_llm(**kwargs) -> HighConcurrencyLLM:
    """è·å–å…¨å±€é«˜å¹¶å‘ LLM å®ä¾‹"""
    global _default_llm
    if _default_llm is None:
        _default_llm = HighConcurrencyLLM(**kwargs)
    return _default_llm


async def quick_invoke(prompt: str, **kwargs) -> str:
    """å¿«é€Ÿè°ƒç”¨"""
    llm = get_high_concurrency_llm()
    result = await llm.ainvoke(prompt, **kwargs)
    return result.content if hasattr(result, 'content') else str(result)


# ==================== P3 ä¼˜åŒ–ï¼šè‡ªé€‚åº”å¹¶å‘æ§åˆ¶ ====================

class AdaptiveSemaphore:
    """
    è‡ªé€‚åº”ä¿¡å·é‡ - åŸºäº 429 é”™è¯¯åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°

    ğŸš€ P3 ä¼˜åŒ–ï¼šè‡ªåŠ¨é€‚åº” API é™æµï¼Œæå‡ååé‡ 10-20%

    ç‰¹æ€§ï¼š
    - æˆåŠŸæ—¶é€æ­¥å¢åŠ å¹¶å‘æ•°ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
    - é‡åˆ° 429 é”™è¯¯æ—¶ç«‹å³å‡å°‘å¹¶å‘æ•°ï¼ˆå¿«é€Ÿå“åº”ï¼‰
    - æ”¯æŒå¹¶å‘æ•°ä¸Šä¸‹é™
    - è®°å½•è°ƒæ•´å†å²

    ä½¿ç”¨ç¤ºä¾‹ï¼š
        sem = AdaptiveSemaphore(initial=5, min_concurrent=1, max_concurrent=10)

        async with sem:
            result = await some_api_call()

        # æŠ¥å‘Šç»“æœ
        if success:
            sem.report_success()
        elif is_rate_limit:
            sem.report_rate_limit()
    """

    def __init__(
        self,
        initial_concurrent: int = 5,
        min_concurrent: int = 1,
        max_concurrent: int = 10,
        increase_threshold: int = 10,  # è¿ç»­æˆåŠŸNæ¬¡åå¢åŠ å¹¶å‘
        increase_step: int = 1,  # æ¯æ¬¡å¢åŠ çš„å¹¶å‘æ•°
        decrease_step: int = 1,  # æ¯æ¬¡å‡å°‘çš„å¹¶å‘æ•°
    ):
        """
        åˆå§‹åŒ–è‡ªé€‚åº”ä¿¡å·é‡

        Args:
            initial_concurrent: åˆå§‹å¹¶å‘æ•°
            min_concurrent: æœ€å°å¹¶å‘æ•°
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            increase_threshold: è¿ç»­æˆåŠŸå¤šå°‘æ¬¡åå¢åŠ å¹¶å‘
            increase_step: æ¯æ¬¡å¢åŠ çš„å¹¶å‘æ•°
            decrease_step: æ¯æ¬¡å‡å°‘çš„å¹¶å‘æ•°
        """
        self.current_concurrent = initial_concurrent
        self.min_concurrent = min_concurrent
        self.max_concurrent = max_concurrent
        self.increase_threshold = increase_threshold
        self.increase_step = increase_step
        self.decrease_step = decrease_step

        # ğŸ”§ ä½¿ç”¨è®¡æ•°å™¨è€Œéå›ºå®š Semaphoreï¼Œæ”¯æŒåŠ¨æ€è°ƒæ•´
        self._active_count = 0  # å½“å‰æ´»è·ƒçš„å¹¶å‘æ•°
        self._lock = asyncio.Lock()  # ä¿æŠ¤å¹¶å‘æ•°è°ƒæ•´
        self._waiters = []  # ç­‰å¾…é˜Ÿåˆ—

        # ç»Ÿè®¡ä¿¡æ¯
        self._success_count = 0  # è¿ç»­æˆåŠŸæ¬¡æ•°
        self._rate_limit_count = 0  # æ€»é™æµæ¬¡æ•°
        self._increase_count = 0  # å¢åŠ å¹¶å‘æ¬¡æ•°
        self._decrease_count = 0  # å‡å°‘å¹¶å‘æ¬¡æ•°
        self._adjustment_history = []  # è°ƒæ•´å†å²

        logger.info(
            f"ğŸš€ [AdaptiveSemaphore] åˆå§‹åŒ–: "
            f"initial={initial_concurrent}, "
            f"range=[{min_concurrent}, {max_concurrent}], "
            f"threshold={increase_threshold}"
        )

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self.acquire()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        self.release()
        return False

    async def acquire(self):
        """è·å–ä¿¡å·é‡ï¼ˆæ”¯æŒåŠ¨æ€è°ƒæ•´ï¼‰"""
        while True:
            async with self._lock:
                if self._active_count < self.current_concurrent:
                    self._active_count += 1
                    return

            # ç­‰å¾…é‡Šæ”¾
            event = asyncio.Event()
            self._waiters.append(event)
            await event.wait()

    def release(self):
        """é‡Šæ”¾ä¿¡å·é‡ï¼ˆæ”¯æŒåŠ¨æ€è°ƒæ•´ï¼‰"""
        async def _release():
            async with self._lock:
                self._active_count -= 1

                # å”¤é†’ç­‰å¾…è€…
                if self._waiters:
                    waiter = self._waiters.pop(0)
                    waiter.set()

        asyncio.create_task(_release())

    def report_success(self):
        """
        æŠ¥å‘ŠæˆåŠŸè°ƒç”¨

        è¿ç»­æˆåŠŸè¾¾åˆ°é˜ˆå€¼æ—¶ï¼Œå°è¯•å¢åŠ å¹¶å‘æ•°
        """
        self._success_count += 1

        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°å¢åŠ é˜ˆå€¼
        if (
            self._success_count >= self.increase_threshold
            and self.current_concurrent < self.max_concurrent
        ):
            asyncio.create_task(self._increase_concurrency())

    def report_rate_limit(self):
        """
        æŠ¥å‘Š 429 é™æµé”™è¯¯

        ç«‹å³å‡å°‘å¹¶å‘æ•°ï¼Œå¿«é€Ÿå“åº” API é™æµ
        """
        self._rate_limit_count += 1
        self._success_count = 0  # é‡ç½®æˆåŠŸè®¡æ•°

        # ç«‹å³å‡å°‘å¹¶å‘æ•°
        if self.current_concurrent > self.min_concurrent:
            asyncio.create_task(self._decrease_concurrency())

    async def _increase_concurrency(self):
        """å¢åŠ å¹¶å‘æ•°ï¼ˆä¿å®ˆç­–ç•¥ï¼‰"""
        async with self._lock:
            old_concurrent = self.current_concurrent
            new_concurrent = min(
                self.current_concurrent + self.increase_step,
                self.max_concurrent
            )

            if new_concurrent > old_concurrent:
                self.current_concurrent = new_concurrent
                self._success_count = 0  # é‡ç½®æˆåŠŸè®¡æ•°
                self._increase_count += 1

                # è®°å½•è°ƒæ•´å†å²
                self._adjustment_history.append({
                    "action": "increase",
                    "from": old_concurrent,
                    "to": new_concurrent,
                    "reason": f"è¿ç»­æˆåŠŸ {self.increase_threshold} æ¬¡",
                    "timestamp": time.time()
                })

                logger.info(
                    f"ğŸ“ˆ [AdaptiveSemaphore] å¢åŠ å¹¶å‘æ•°: "
                    f"{old_concurrent} â†’ {new_concurrent} "
                    f"(è¿ç»­æˆåŠŸ {self.increase_threshold} æ¬¡)"
                )

                # ğŸ”§ å”¤é†’ç­‰å¾…è€…ä»¥åˆ©ç”¨æ–°å¢çš„å¹¶å‘æ§½ä½
                while self._waiters and self._active_count < self.current_concurrent:
                    waiter = self._waiters.pop(0)
                    waiter.set()

    async def _decrease_concurrency(self):
        """å‡å°‘å¹¶å‘æ•°ï¼ˆå¿«é€Ÿå“åº”ï¼‰"""
        async with self._lock:
            old_concurrent = self.current_concurrent
            new_concurrent = max(
                self.current_concurrent - self.decrease_step,
                self.min_concurrent
            )

            if new_concurrent < old_concurrent:
                self.current_concurrent = new_concurrent
                self._decrease_count += 1

                # è®°å½•è°ƒæ•´å†å²
                self._adjustment_history.append({
                    "action": "decrease",
                    "from": old_concurrent,
                    "to": new_concurrent,
                    "reason": "é‡åˆ° 429 é™æµé”™è¯¯",
                    "timestamp": time.time()
                })

                logger.warning(
                    f"ğŸ“‰ [AdaptiveSemaphore] å‡å°‘å¹¶å‘æ•°: "
                    f"{old_concurrent} â†’ {new_concurrent} "
                    f"(é‡åˆ° 429 é™æµ)"
                )

    @property
    def stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "current_concurrent": self.current_concurrent,
            "active_count": self._active_count,  # ğŸ†• å½“å‰æ´»è·ƒå¹¶å‘æ•°
            "waiters_count": len(self._waiters),  # ğŸ†• ç­‰å¾…é˜Ÿåˆ—é•¿åº¦
            "min_concurrent": self.min_concurrent,
            "max_concurrent": self.max_concurrent,
            "success_count": self._success_count,
            "rate_limit_count": self._rate_limit_count,
            "increase_count": self._increase_count,
            "decrease_count": self._decrease_count,
            "adjustment_history": self._adjustment_history[-10:],  # æœ€è¿‘10æ¬¡è°ƒæ•´
        }

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self._success_count = 0
        self._rate_limit_count = 0
        self._increase_count = 0
        self._decrease_count = 0
        self._adjustment_history = []
