"""
LLMå·¥å‚æ¨¡å— - 2025å¹´å·¥å‚æ¨¡å¼ + è‡ªåŠ¨é™çº§

æä¾›ç»Ÿä¸€çš„LLMå®ä¾‹åˆ›å»ºæ¥å£,æ”¯æŒä¾èµ–æ³¨å…¥ã€é…ç½®ç®¡ç†å’Œè‡ªåŠ¨é™çº§
"""

from typing import Optional
from langchain_openai import ChatOpenAI
from loguru import logger
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import httpcore
import openai

from intelligent_project_analyzer.settings import settings, LLMConfig


class LLMFactory:
    """
    LLMå·¥å‚ - 2025å¹´ä¾èµ–æ³¨å…¥æ¨¡å¼ + è‡ªåŠ¨é™çº§
    
    ä¼˜åŠ¿:
    - ç»Ÿä¸€çš„LLMåˆ›å»ºæ¥å£
    - æ”¯æŒé…ç½®æ³¨å…¥
    - æ˜“äºæµ‹è¯•(å¯Mock)
    - é¿å…é…ç½®æ¼‚ç§»
    - ğŸ†• è‡ªåŠ¨é™çº§: OpenAIå¤±è´¥æ—¶åˆ‡æ¢åˆ°Qwen
    """
    
    @staticmethod
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((httpcore.ConnectError, openai.APIConnectionError, ConnectionError)),
        reraise=True
    )
    def create_llm(config: Optional[LLMConfig] = None, **kwargs) -> ChatOpenAI:
        """
        åˆ›å»ºLLMå®ä¾‹ (æ”¯æŒ OpenRouter å¤š Key è´Ÿè½½å‡è¡¡ + SSLé‡è¯•)

        ğŸ†• v7.4.2: ä¼˜å…ˆä½¿ç”¨ OpenRouter å¤š Key è´Ÿè½½å‡è¡¡
        - å¦‚æœé…ç½®äº†å¤šä¸ª OpenRouter Keysï¼Œè‡ªåŠ¨å¯ç”¨è´Ÿè½½å‡è¡¡
        - å¦‚æœç¦ç”¨è‡ªåŠ¨é™çº§ï¼Œåªä½¿ç”¨é…ç½®çš„æä¾›å•†
        - é‡è¯•ç­–ç•¥: æŒ‡æ•°é€€é¿,æœ€å¤š3æ¬¡,é’ˆå¯¹ç½‘ç»œè¿æ¥é”™è¯¯

        Args:
            config: LLMé…ç½®å¯¹è±¡,å¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å…¨å±€settings
            **kwargs: é¢å¤–çš„LLMå‚æ•°,ä¼šè¦†ç›–configä¸­çš„å€¼

        Returns:
            ChatOpenAIå®ä¾‹

        Example:
            # ä½¿ç”¨é»˜è®¤é…ç½® (OpenRouter è´Ÿè½½å‡è¡¡)
            llm = LLMFactory.create_llm()

            # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
            custom_config = LLMConfig(
                model="gpt-4.1",
                max_tokens=32000,
                api_key="..."
            )
            llm = LLMFactory.create_llm(config=custom_config)

            # è¦†ç›–ç‰¹å®šå‚æ•° (å¤´è„‘é£æš´åœºæ™¯)
            llm = LLMFactory.create_llm(temperature=0.9, max_tokens=32000)
        """
        import os

        # ğŸ†• v7.4.2: æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ OpenRouter ä¸”é…ç½®äº†å¤šä¸ª Keys
        primary_provider = os.getenv("LLM_PROVIDER", "openai").lower()
        auto_fallback = os.getenv("LLM_AUTO_FALLBACK", "false").lower() == "true"

        # å¦‚æœæ˜¯ OpenRouter ä¸”é…ç½®äº†å¤šä¸ª Keysï¼Œä½¿ç”¨è´Ÿè½½å‡è¡¡
        if primary_provider == "openrouter":
            openrouter_keys = os.getenv("OPENROUTER_API_KEYS", "")
            if openrouter_keys and "," in openrouter_keys:
                logger.info("ğŸ”„ æ£€æµ‹åˆ°å¤šä¸ª OpenRouter Keysï¼Œå¯ç”¨è´Ÿè½½å‡è¡¡")
                try:
                    return LLMFactory.create_openrouter_balanced_llm(**kwargs)
                except Exception as e:
                    logger.warning(f"âš ï¸ OpenRouter è´Ÿè½½å‡è¡¡åˆ›å»ºå¤±è´¥: {e}")
                    # ç»§ç»­ä½¿ç”¨åŸå§‹æ–¹æ³•

        # å…¨å±€æ•è· LLM è¿æ¥å¼‚å¸¸ï¼Œè¿”å›å‹å¥½æç¤º
        try:
            # ...existing code...
            # å¦‚æœç¦ç”¨è‡ªåŠ¨é™çº§ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ–¹æ³•
            if not auto_fallback:
                logger.info(f"ğŸ“Œ è‡ªåŠ¨é™çº§å·²ç¦ç”¨ï¼Œåªä½¿ç”¨ {primary_provider}")
                return LLMFactory._create_llm_original(config, **kwargs)

            # å°è¯•ä½¿ç”¨å¤šLLMå·¥å‚åˆ›å»º(æ”¯æŒè‡ªåŠ¨é™çº§)
            from intelligent_project_analyzer.services.multi_llm_factory import MultiLLMFactory, FallbackLLM
            fallback_chain = [primary_provider]
            if primary_provider == "openai":
                if os.getenv("OPENROUTER_API_KEY") and os.getenv("OPENROUTER_API_KEY") != "your_openrouter_api_key_here":
                    fallback_chain.append("openrouter")
                if os.getenv("DEEPSEEK_API_KEY") and os.getenv("DEEPSEEK_API_KEY") != "your_deepseek_api_key_here":
                    fallback_chain.append("deepseek")
            elif primary_provider == "openrouter":
                if os.getenv("OPENAI_API_KEY") and os.getenv("OPENAI_API_KEY") != "your_openai_api_key_here":
                    fallback_chain.append("openai")
                if os.getenv("DEEPSEEK_API_KEY") and os.getenv("DEEPSEEK_API_KEY") != "your_deepseek_api_key_here":
                    fallback_chain.append("deepseek")
            elif primary_provider == "qwen":
                if os.getenv("OPENAI_API_KEY") and os.getenv("OPENAI_API_KEY") != "your_openai_api_key_here":
                    fallback_chain.append("openai")
                if os.getenv("OPENROUTER_API_KEY") and os.getenv("OPENROUTER_API_KEY") != "your_openrouter_api_key_here":
                    fallback_chain.append("openrouter")
                if os.getenv("DEEPSEEK_API_KEY") and os.getenv("DEEPSEEK_API_KEY") != "your_deepseek_api_key_here":
                    fallback_chain.append("deepseek")
            elif primary_provider == "deepseek":
                if os.getenv("OPENROUTER_API_KEY") and os.getenv("OPENROUTER_API_KEY") != "your_openrouter_api_key_here":
                    fallback_chain.append("openrouter")
                if os.getenv("OPENAI_API_KEY") and os.getenv("OPENAI_API_KEY") != "your_openai_api_key_here":
                    fallback_chain.append("openai")
            # ä½¿ç”¨é™çº§é“¾åˆ›å»ºLLM
            if len(fallback_chain) > 1:
                logger.info(f"ğŸ”„ å¯ç”¨è‡ªåŠ¨é™çº§: {' â†’ '.join(fallback_chain)}")
                return FallbackLLM(
                    providers=fallback_chain,
                    temperature=kwargs.get("temperature", config.temperature if config else settings.llm.temperature),
                    max_tokens=kwargs.get("max_tokens", config.max_tokens if config else settings.llm.max_tokens),
                    timeout=kwargs.get("timeout", config.timeout if config else settings.llm.timeout),
                    max_retries=kwargs.get("max_retries", config.max_retries if config else settings.llm.max_retries),
                )
            else:
                return MultiLLMFactory.create_llm(
                    provider=primary_provider,
                    temperature=kwargs.get("temperature", config.temperature if config else settings.llm.temperature),
                    max_tokens=kwargs.get("max_tokens", config.max_tokens if config else settings.llm.max_tokens),
                    timeout=kwargs.get("timeout", config.timeout if config else settings.llm.timeout),
                    max_retries=kwargs.get("max_retries", config.max_retries if config else settings.llm.max_retries),
                    **kwargs
                )
        except (openai.APIConnectionError, httpcore.ConnectError, ConnectionError) as e:
            logger.error(f"âŒ LLMæœåŠ¡è¿æ¥å¼‚å¸¸: {e}")
            raise RuntimeError("LLMæœåŠ¡è¿æ¥å¼‚å¸¸ï¼Œè¯·ç¨åé‡è¯•ã€‚");
        except Exception as e:
            logger.error(f"âŒ LLMå®ä¾‹åˆ›å»ºå¼‚å¸¸: {e}")
            raise

        # å¦‚æœç¦ç”¨è‡ªåŠ¨é™çº§ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ–¹æ³•
        if not auto_fallback:
            logger.info(f"ğŸ“Œ è‡ªåŠ¨é™çº§å·²ç¦ç”¨ï¼Œåªä½¿ç”¨ {primary_provider}")
            return LLMFactory._create_llm_original(config, **kwargs)

        # å°è¯•ä½¿ç”¨å¤šLLMå·¥å‚åˆ›å»º(æ”¯æŒè‡ªåŠ¨é™çº§)
        try:
            from intelligent_project_analyzer.services.multi_llm_factory import MultiLLMFactory, FallbackLLM

            # å®šä¹‰é™çº§é“¾
            fallback_chain = [primary_provider]

            # æ ¹æ®ä¸»æä¾›å•†æ·»åŠ å¤‡é€‰
            if primary_provider == "openai":
                # OpenAI å®˜æ–¹ â†’ OpenRouter (GPT) â†’ DeepSeek
                if os.getenv("OPENROUTER_API_KEY") and os.getenv("OPENROUTER_API_KEY") != "your_openrouter_api_key_here":
                    fallback_chain.append("openrouter")
                if os.getenv("DEEPSEEK_API_KEY") and os.getenv("DEEPSEEK_API_KEY") != "your_deepseek_api_key_here":
                    fallback_chain.append("deepseek")
            elif primary_provider == "openrouter":
                # OpenRouter â†’ OpenAI â†’ DeepSeek
                if os.getenv("OPENAI_API_KEY") and os.getenv("OPENAI_API_KEY") != "your_openai_api_key_here":
                    fallback_chain.append("openai")
                if os.getenv("DEEPSEEK_API_KEY") and os.getenv("DEEPSEEK_API_KEY") != "your_deepseek_api_key_here":
                    fallback_chain.append("deepseek")
            elif primary_provider == "qwen":
                # Qwen â†’ OpenAI â†’ OpenRouter â†’ DeepSeek
                if os.getenv("OPENAI_API_KEY") and os.getenv("OPENAI_API_KEY") != "your_openai_api_key_here":
                    fallback_chain.append("openai")
                if os.getenv("OPENROUTER_API_KEY") and os.getenv("OPENROUTER_API_KEY") != "your_openrouter_api_key_here":
                    fallback_chain.append("openrouter")
                if os.getenv("DEEPSEEK_API_KEY") and os.getenv("DEEPSEEK_API_KEY") != "your_deepseek_api_key_here":
                    fallback_chain.append("deepseek")
            elif primary_provider == "deepseek":
                # DeepSeek â†’ OpenRouter â†’ OpenAI
                if os.getenv("OPENROUTER_API_KEY") and os.getenv("OPENROUTER_API_KEY") != "your_openrouter_api_key_here":
                    fallback_chain.append("openrouter")
                if os.getenv("OPENAI_API_KEY") and os.getenv("OPENAI_API_KEY") != "your_openai_api_key_here":
                    fallback_chain.append("openai")

            # ä½¿ç”¨é™çº§é“¾åˆ›å»ºLLM
            if len(fallback_chain) > 1:
                logger.info(f"ğŸ”„ å¯ç”¨è‡ªåŠ¨é™çº§: {' â†’ '.join(fallback_chain)}")
                return FallbackLLM(
                    providers=fallback_chain,
                    temperature=kwargs.get("temperature", config.temperature if config else settings.llm.temperature),
                    max_tokens=kwargs.get("max_tokens", config.max_tokens if config else settings.llm.max_tokens),
                    timeout=kwargs.get("timeout", config.timeout if config else settings.llm.timeout),
                    max_retries=kwargs.get("max_retries", config.max_retries if config else settings.llm.max_retries),
                )
            else:
                # æ²¡æœ‰å¤‡é€‰,ç›´æ¥åˆ›å»º
                return MultiLLMFactory.create_llm(
                    provider=primary_provider,
                    temperature=kwargs.get("temperature", config.temperature if config else settings.llm.temperature),
                    max_tokens=kwargs.get("max_tokens", config.max_tokens if config else settings.llm.max_tokens),
                    timeout=kwargs.get("timeout", config.timeout if config else settings.llm.timeout),
                    max_retries=kwargs.get("max_retries", config.max_retries if config else settings.llm.max_retries),
                    **kwargs
                )
        except Exception as e:
            logger.warning(f"âš ï¸ MultiLLMé™çº§å¤±è´¥,ä½¿ç”¨åŸå§‹æ–¹æ³•: {e}")
            # é™çº§åˆ°åŸå§‹æ–¹æ³•
            return LLMFactory._create_llm_original(config, **kwargs)
    
    @staticmethod
    def _create_llm_original(config: Optional[LLMConfig] = None, **kwargs) -> ChatOpenAI:
        """
        åˆ›å»ºLLMå®ä¾‹ (åŸå§‹æ–¹æ³•,æ— é™çº§)
        
        Args:
            config: LLMé…ç½®å¯¹è±¡,å¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å…¨å±€settings
            **kwargs: é¢å¤–çš„LLMå‚æ•°,ä¼šè¦†ç›–configä¸­çš„å€¼
            
        Returns:
            ChatOpenAIå®ä¾‹
        """
        # ä½¿ç”¨æä¾›çš„é…ç½®æˆ–å…¨å±€é…ç½®
        cfg = config or settings.llm
        
        # æ„å»ºLLMå‚æ•°
        llm_params = {
            "model": cfg.model,
            "max_tokens": cfg.max_tokens,
            "timeout": cfg.timeout,
            "temperature": cfg.temperature,
            "api_key": cfg.api_key,
            "max_retries": cfg.max_retries,  # åº”ç”¨é‡è¯•é…ç½®
        }
        
        # å¦‚æœæœ‰è‡ªå®šä¹‰API Base
        if cfg.api_base:
            llm_params["base_url"] = cfg.api_base
        
        # åº”ç”¨kwargsè¦†ç›–
        llm_params.update(kwargs)
        
        logger.info(
            f"åˆ›å»ºLLMå®ä¾‹: model={llm_params['model']}, "
            f"max_tokens={llm_params['max_tokens']}, "
            f"max_retries={llm_params.get('max_retries', cfg.max_retries)}"
        )
        
        return ChatOpenAI(**llm_params)
    
    @staticmethod
    def create_streaming_llm(config: Optional[LLMConfig] = None, **kwargs) -> ChatOpenAI:
        """
        åˆ›å»ºæµå¼LLMå®ä¾‹
        
        Args:
            config: LLMé…ç½®å¯¹è±¡
            **kwargs: é¢å¤–çš„LLMå‚æ•°
            
        Returns:
            æ”¯æŒæµå¼è¾“å‡ºçš„ChatOpenAIå®ä¾‹
        """
        # å¼ºåˆ¶å¯ç”¨streaming
        kwargs["streaming"] = True
        
        llm = LLMFactory.create_llm(config=config, **kwargs)
        logger.info("åˆ›å»ºæµå¼LLMå®ä¾‹")
        
        return llm
    
    @staticmethod
    def create_structured_llm(
        config: Optional[LLMConfig] = None,
        schema: Optional[type] = None,
        **kwargs
    ) -> ChatOpenAI:
        """
        åˆ›å»ºç»“æ„åŒ–è¾“å‡ºLLMå®ä¾‹
        
        Args:
            config: LLMé…ç½®å¯¹è±¡
            schema: Pydanticæ¨¡å‹ç±»,ç”¨äºç»“æ„åŒ–è¾“å‡º
            **kwargs: é¢å¤–çš„LLMå‚æ•°
            
        Returns:
            æ”¯æŒç»“æ„åŒ–è¾“å‡ºçš„ChatOpenAIå®ä¾‹
        """
        llm = LLMFactory.create_llm(config=config, **kwargs)
        
        if schema:
            # ä½¿ç”¨with_structured_outputç»‘å®šschema
            llm = llm.with_structured_output(schema)
            logger.info(f"åˆ›å»ºç»“æ„åŒ–è¾“å‡ºLLMå®ä¾‹: schema={schema.__name__}")
        
        return llm
    
    @staticmethod
    def get_default_config() -> LLMConfig:
        """
        è·å–é»˜è®¤LLMé…ç½®
        
        Returns:
            å…¨å±€settingsä¸­çš„LLMé…ç½®
        """
        return settings.llm
    
    @staticmethod
    def validate_config(config: LLMConfig) -> bool:
        """
        éªŒè¯LLMé…ç½®
        
        Args:
            config: è¦éªŒè¯çš„é…ç½®
            
        Returns:
            é…ç½®æ˜¯å¦æœ‰æ•ˆ
        """
        if not config.api_key:
            logger.error("LLMé…ç½®æ— æ•ˆ: ç¼ºå°‘API Key")
            return False
        
        if config.max_tokens < 100:
            logger.warning(f"max_tokensè¿‡å°: {config.max_tokens}, å»ºè®®è‡³å°‘1000")
        
        if config.timeout < 10:
            logger.warning(f"timeoutè¿‡çŸ­: {config.timeout}ç§’, å»ºè®®è‡³å°‘30ç§’")
        
        logger.info("LLMé…ç½®éªŒè¯é€šè¿‡")
        return True

    @staticmethod
    def create_high_concurrency_llm(
        provider: str = "openai",
        enable_cache: bool = True,
        enable_fallback: bool = True,
        **kwargs
    ):
        """
        ğŸ†• v3.9: åˆ›å»ºé«˜å¹¶å‘ LLM å®ä¾‹

        æ”¯æŒ:
        - å¤š Key è´Ÿè½½å‡è¡¡
        - å¤šæä¾›å•†æ•…éšœè½¬ç§»
        - è¯·æ±‚é™æµ
        - ç»“æœç¼“å­˜

        Args:
            provider: é¦–é€‰æä¾›å•†
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            enable_fallback: æ˜¯å¦å¯ç”¨æ•…éšœè½¬ç§»
            **kwargs: å…¶ä»– LLM å‚æ•°

        Returns:
            HighConcurrencyLLM å®ä¾‹

        Example:
            llm = LLMFactory.create_high_concurrency_llm("openai")
            result = await llm.ainvoke("Hello")
        """
        try:
            from intelligent_project_analyzer.services.high_concurrency_llm import HighConcurrencyLLM

            return HighConcurrencyLLM(
                preferred_provider=provider,
                enable_cache=enable_cache,
                enable_fallback=enable_fallback,
                **kwargs
            )
        except Exception as e:
            logger.warning(f"âš ï¸ é«˜å¹¶å‘LLMåˆ›å»ºå¤±è´¥,å›é€€åˆ°æ™®é€šLLM: {e}")
            return LLMFactory.create_llm(**kwargs)

    @staticmethod
    def create_openrouter_balanced_llm(
        model: str = "openai/gpt-4o-2024-11-20",
        strategy: str = "round_robin",
        **kwargs
    ):
        """
        ğŸ†• v7.4.2: åˆ›å»º OpenRouter è´Ÿè½½å‡è¡¡ LLM å®ä¾‹

        ä½¿ç”¨å¤šä¸ª API Key è¿›è¡Œè´Ÿè½½å‡è¡¡ï¼Œæé«˜ç¨³å®šæ€§å’Œååé‡ã€‚

        ç‰¹æ€§:
        - å¤š Key è½®è¯¢/éšæœº/æœ€å°‘ä½¿ç”¨ç­–ç•¥
        - è‡ªåŠ¨å¥åº·æ£€æŸ¥å’Œæ•…éšœè½¬ç§»
        - é€Ÿç‡é™åˆ¶ä¿æŠ¤
        - ä½¿ç”¨ç»Ÿè®¡å’Œç›‘æ§

        Args:
            model: OpenRouter æ¨¡å‹åç§°
            strategy: è´Ÿè½½å‡è¡¡ç­–ç•¥ (round_robin | random | least_used)
            **kwargs: å…¶ä»– LLM å‚æ•°

        Returns:
            ChatOpenAI å®ä¾‹ï¼ˆé€šè¿‡è´Ÿè½½å‡è¡¡å™¨ï¼‰

        Example:
            # åœ¨ .env ä¸­é…ç½®å¤šä¸ª Keys:
            # OPENROUTER_API_KEYS=key1,key2,key3

            llm = LLMFactory.create_openrouter_balanced_llm()
            response = llm.invoke("Hello")
        """
        try:
            from intelligent_project_analyzer.services.openrouter_load_balancer import (
                get_global_balancer,
                LoadBalancerConfig
            )

            # åˆ›å»ºé…ç½®
            config = LoadBalancerConfig(strategy=strategy)

            # è·å–å…¨å±€è´Ÿè½½å‡è¡¡å™¨
            balancer = get_global_balancer(
                config=config,
                model=model,
                **kwargs
            )

            # è¿”å› LLM å®ä¾‹
            return balancer.get_llm()

        except Exception as e:
            logger.warning(f"âš ï¸ OpenRouter è´Ÿè½½å‡è¡¡å™¨åˆ›å»ºå¤±è´¥ï¼Œå›é€€åˆ°æ™®é€š LLM: {e}")
            return LLMFactory.create_llm(**kwargs)

