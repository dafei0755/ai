"""
å¤šLLMæä¾›å•†å·¥å‚ - æ”¯æŒå¤šå¤‡é€‰åˆ‡æ¢

æ”¯æŒçš„æä¾›å•†:
- OpenAI (å®˜æ–¹API)
- OpenRouter (å›½å†…å¯ç”¨çš„ OpenAI ä»£ç†)
- DeepSeek (å›½å†…å¿«é€Ÿ)
- é˜¿é‡Œé€šä¹‰åƒé—®
- Anthropic Claude
- Azure OpenAI
"""

import os
from typing import Optional, Literal
from langchain_openai import ChatOpenAI, AzureChatOpenAI
from langchain_anthropic import ChatAnthropic
from loguru import logger
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å¯¼å…¥å…¨å±€é…ç½® (æä¾›é»˜è®¤å€¼)
try:
    from intelligent_project_analyzer.settings import settings
    DEFAULT_MAX_TOKENS = settings.llm.max_tokens
    DEFAULT_TEMPERATURE = settings.llm.temperature
    DEFAULT_TIMEOUT = settings.llm.timeout
except ImportError:
    # é™çº§é»˜è®¤å€¼ (ä¸.envé…ç½®ä¿æŒä¸€è‡´)
    DEFAULT_MAX_TOKENS = 32000  # æ”¯æŒå®Œæ•´ç»“æ„åŒ–æŠ¥å‘Š
    DEFAULT_TEMPERATURE = 0.7   # è®¾è®¡è¡Œä¸šåˆ›æ„ä¸ç¨³å®šå¹³è¡¡
    DEFAULT_TIMEOUT = 600       # 10åˆ†é’Ÿåº”å¯¹é•¿è¾“å‡º

LLMProvider = Literal["openai", "deepseek", "qwen", "anthropic", "azure", "openrouter"]


class MultiLLMFactory:
    """
    å¤šLLMæä¾›å•†å·¥å‚
    
    ä½¿ç”¨æ–¹æ³•:
        # æ–¹å¼1: ä»ç¯å¢ƒå˜é‡è‡ªåŠ¨é€‰æ‹©
        llm = MultiLLMFactory.create_llm()
        
        # æ–¹å¼2: æ‰‹åŠ¨æŒ‡å®šæä¾›å•†
        llm = MultiLLMFactory.create_llm(provider="deepseek")
        
        # æ–¹å¼3: ä½¿ç”¨å¤‡é€‰é“¾ (è‡ªåŠ¨é™çº§)
        llm = MultiLLMFactory.create_with_fallback(
            providers=["openai", "deepseek", "qwen"]
        )
    """
    
    # æä¾›å•†é…ç½®æ˜ å°„
    PROVIDER_CONFIGS = {
        "openai": {
            "api_key_env": "OPENAI_API_KEY",
            "model_env": "OPENAI_MODEL",
            "base_url_env": "OPENAI_BASE_URL",
            "default_model": "gpt-4.1",  # ä¸.envä¸€è‡´
            "class": ChatOpenAI
        },
        "deepseek": {
            "api_key_env": "DEEPSEEK_API_KEY",
            "model_env": "DEEPSEEK_MODEL",
            "base_url_env": "DEEPSEEK_BASE_URL",
            "default_model": "deepseek-chat",  # ä¸.envå®˜æ–¹æ¨èä¸€è‡´
            "default_base_url": "https://api.deepseek.com",
            "class": ChatOpenAI
        },
        "qwen": {
            "api_key_env": "QWEN_API_KEY",  # é˜¿é‡Œäº‘ç™¾ç‚¼API Key
            "model_env": "QWEN_MODEL",
            "base_url_env": "QWEN_BASE_URL",
            "default_model": "qwen-max",  # ä¸.envå®˜æ–¹æ¨èä¸€è‡´
            "default_base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",  # åŒ—äº¬åœ°åŸŸ
            "class": ChatOpenAI
        },
        "anthropic": {
            "api_key_env": "ANTHROPIC_API_KEY",
            "model_env": "ANTHROPIC_MODEL",
            "default_model": "claude-3-5-sonnet-20241022",  # ä¸.envå®˜æ–¹æ¨èä¸€è‡´
            "class": ChatAnthropic
        },
        "azure": {
            "api_key_env": "AZURE_OPENAI_API_KEY",
            "endpoint_env": "AZURE_OPENAI_ENDPOINT",
            "deployment_env": "AZURE_OPENAI_DEPLOYMENT",
            "version_env": "AZURE_API_VERSION",
            "default_version": "2024-02-15-preview",
            "class": AzureChatOpenAI
        },
        "openrouter": {
            "api_key_env": "OPENROUTER_API_KEY",
            "model_env": "OPENROUTER_MODEL",
            "base_url_env": "OPENROUTER_BASE_URL",
            "default_model": "openai/gpt-4o",  # OpenRouter éœ€è¦åŠ æä¾›å•†å‰ç¼€
            "default_base_url": "https://openrouter.ai/api/v1",
            "class": ChatOpenAI,
            "extra_headers": True  # æ ‡è®°éœ€è¦æ·»åŠ è‡ªå®šä¹‰ headers
        }
    }
    
    @classmethod
    def create_llm(
        cls,
        provider: Optional[LLMProvider] = None,
        temperature: float = None,
        max_tokens: int = None,
        timeout: int = None,
        max_retries: int = None,
        **kwargs
    ):
        """
        åˆ›å»ºLLMå®ä¾‹
        
        Args:
            provider: æä¾›å•†åç§°,å¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡LLM_PROVIDERè¯»å–
            temperature: æ¸©åº¦å‚æ•° (Noneæ—¶ä½¿ç”¨settingsé»˜è®¤å€¼)
            max_tokens: æœ€å¤§tokenæ•° (Noneæ—¶ä½¿ç”¨settingsé»˜è®¤å€¼)
            timeout: è¶…æ—¶æ—¶é—´(ç§’) (Noneæ—¶ä½¿ç”¨settingsé»˜è®¤å€¼)
            max_retries: é‡è¯•æ¬¡æ•° (Noneæ—¶ä½¿ç”¨settingsé»˜è®¤å€¼)
            **kwargs: å…¶ä»–LLMå‚æ•°
            
        Returns:
            LLMå®ä¾‹
        """
        # åº”ç”¨é»˜è®¤å€¼
        temperature = temperature if temperature is not None else DEFAULT_TEMPERATURE
        max_tokens = max_tokens if max_tokens is not None else DEFAULT_MAX_TOKENS
        timeout = timeout if timeout is not None else DEFAULT_TIMEOUT
        max_retries = max_retries if max_retries is not None else getattr(settings.llm, 'max_retries', 3)
        
        logger.debug(
            f"ğŸ“Š ä½¿ç”¨é…ç½®: temperature={temperature}, max_tokens={max_tokens}, "
            f"timeout={timeout}s, max_retries={max_retries}"
        )
        
        # ç¡®å®šä½¿ç”¨çš„æä¾›å•†
        if provider is None:
            provider = os.getenv("LLM_PROVIDER", "openai").lower()
        
        # è·å–æ¨¡å‹åç§°ç”¨äºæ—¥å¿—
        config = cls.PROVIDER_CONFIGS.get(provider, {})
        model_env = config.get("model_env", "")
        model_name = os.getenv(model_env, config.get("default_model", "unknown"))
        
        logger.info(f"ğŸ”§ Creating LLM instance: provider={provider}, model={model_name}, temperature={temperature}, max_tokens={max_tokens}")
        
        # éªŒè¯æä¾›å•†
        if provider not in cls.PROVIDER_CONFIGS:
            logger.error(f"âŒ Unknown provider: {provider}")
            raise ValueError(f"Unknown LLM provider: {provider}")
        
        # è·å–æä¾›å•†é…ç½®
        config = cls.PROVIDER_CONFIGS[provider]
        
        # æ ¹æ®æä¾›å•†ç±»å‹åˆ›å»ºå®ä¾‹
        if provider == "azure":
            return cls._create_azure_llm(config, temperature, max_tokens, timeout, max_retries, **kwargs)
        elif provider == "anthropic":
            return cls._create_anthropic_llm(config, temperature, max_tokens, timeout, max_retries, **kwargs)
        else:
            return cls._create_openai_compatible_llm(config, temperature, max_tokens, timeout, max_retries, **kwargs)
    
    @classmethod
    def _create_openai_compatible_llm(
        cls, 
        config: dict, 
        temperature: float, 
        max_tokens: int, 
        timeout: int,
        max_retries: int,
        **kwargs
    ) -> ChatOpenAI:
        """åˆ›å»ºOpenAIå…¼å®¹çš„LLM (OpenAI/DeepSeek/Qwen/OpenRouter)"""
        
        # è·å–API Key
        api_key = os.getenv(config["api_key_env"])
        if not api_key or api_key == "your_xxx_api_key_here":
            raise ValueError(f"âŒ Missing or invalid API key: {config['api_key_env']}")
        
        # è·å–æ¨¡å‹åç§°
        model = os.getenv(config["model_env"], config["default_model"])
        
        # æ„å»ºå‚æ•°
        llm_params = {
            "model": model,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "timeout": timeout,
            "max_retries": max_retries,
            "api_key": api_key,
            **kwargs
        }
        
        # å¦‚æœæœ‰base_urlé…ç½®
        if "base_url_env" in config:
            base_url = os.getenv(config["base_url_env"], config.get("default_base_url"))
            if base_url:
                llm_params["base_url"] = base_url
        
        # OpenRouter éœ€è¦é¢å¤–çš„ headers
        if config.get("extra_headers"):
            app_name = os.getenv("OPENROUTER_APP_NAME", "Intelligent Project Analyzer")
            site_url = os.getenv("OPENROUTER_SITE_URL", "https://github.com/your-repo")
            llm_params["default_headers"] = {
                "HTTP-Referer": site_url,
                "X-Title": app_name
            }
            logger.info(f"âœ… OpenRouter headers: referer={site_url}, title={app_name}")
        
        logger.info(f"âœ… Creating {model} (timeout={timeout}s, max_tokens={max_tokens})")
        
        return config["class"](**llm_params)
    
    @classmethod
    def _create_anthropic_llm(
        cls,
        config: dict,
        temperature: float,
        max_tokens: int,
        timeout: int,
        max_retries: int,
        **kwargs
    ) -> ChatAnthropic:
        """åˆ›å»ºAnthropic Claude LLM"""
        
        api_key = os.getenv(config["api_key_env"])
        if not api_key or api_key == "your_anthropic_api_key_here":
            raise ValueError(f"âŒ Missing or invalid API key: {config['api_key_env']}")
        
        model = os.getenv(config["model_env"], config["default_model"])
        
        llm_params = {
            "model": model,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "timeout": timeout,
            "max_retries": max_retries,
            "anthropic_api_key": api_key,
            **kwargs
        }
        
        logger.info(f"âœ… Creating {model} (Anthropic)")
        
        return config["class"](**llm_params)
    
    @classmethod
    def _create_azure_llm(
        cls,
        config: dict,
        temperature: float,
        max_tokens: int,
        timeout: int,
        max_retries: int,
        **kwargs
    ) -> AzureChatOpenAI:
        """åˆ›å»ºAzure OpenAI LLM"""
        
        api_key = os.getenv(config["api_key_env"])
        endpoint = os.getenv(config["endpoint_env"])
        deployment = os.getenv(config["deployment_env"])
        api_version = os.getenv(config["version_env"], config["default_version"])
        
        if not all([api_key, endpoint, deployment]):
            raise ValueError("âŒ Missing Azure OpenAI configuration")
        
        llm_params = {
            "azure_deployment": deployment,
            "azure_endpoint": endpoint,
            "api_key": api_key,
            "api_version": api_version,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "timeout": timeout,
            "max_retries": max_retries,
            **kwargs
        }
        
        logger.info(f"âœ… Creating Azure OpenAI: {deployment}")
        
        return config["class"](**llm_params)
    
    @classmethod
    def create_with_fallback(
        cls,
        providers: list[LLMProvider],
        temperature: float = None,
        max_tokens: int = None,
        timeout: int = None,
        max_retries: int = None,
        **kwargs
    ):
        """
        ä½¿ç”¨é™çº§é“¾åˆ›å»ºLLMå®ä¾‹
        
        Args:
            providers: æä¾›å•†åˆ—è¡¨,æŒ‰ä¼˜å…ˆçº§æ’åº
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            timeout: è¶…æ—¶æ—¶é—´(ç§’)
            max_retries: é‡è¯•æ¬¡æ•°
            **kwargs: å…¶ä»–LLMå‚æ•°
            
        Returns:
            æˆåŠŸåˆ›å»ºçš„ç¬¬ä¸€ä¸ªLLMå®ä¾‹
            
        Example:
            # æ¨èé…ç½®ï¼šOpenAIå®˜æ–¹ â†’ OpenRouter â†’ DeepSeek
            llm = MultiLLMFactory.create_with_fallback(
                providers=["openai", "openrouter", "deepseek"]
            )
        """
        logger.info(f"ğŸ”„ Attempting to create LLM with fallback: {providers}")
        
        last_error = None
        for provider in providers:
            try:
                llm = cls.create_llm(
                    provider=provider,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    timeout=timeout,
                    max_retries=max_retries,
                    **kwargs
                )
                logger.info(f"âœ… Successfully created LLM with provider: {provider}")
                return llm
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to create {provider} LLM: {e}")
                last_error = e
                continue
        
        # æ‰€æœ‰æä¾›å•†éƒ½å¤±è´¥
        logger.error(f"âŒ All providers failed. Last error: {last_error}")
        raise RuntimeError(f"Failed to create LLM with any provider: {providers}")
    
    @classmethod
    def get_active_provider(cls) -> str:
        """è·å–å½“å‰æ¿€æ´»çš„æä¾›å•†"""
        return os.getenv("LLM_PROVIDER", "openai").lower()
    
    @classmethod
    def list_available_providers(cls) -> list[str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æä¾›å•†"""
        available = []
        for provider, config in cls.PROVIDER_CONFIGS.items():
            api_key = os.getenv(config.get("api_key_env", ""))
            if api_key and api_key != f"your_{provider}_api_key_here":
                available.append(provider)
        return available
    
    @classmethod
    def test_provider(cls, provider: LLMProvider) -> bool:
        """æµ‹è¯•æä¾›å•†æ˜¯å¦å¯ç”¨"""
        try:
            llm = cls.create_llm(provider=provider, timeout=10)
            result = llm.invoke("Say hello")
            logger.info(f"âœ… Provider {provider} test passed: {result.content[:50]}")
            return True
        except Exception as e:
            logger.error(f"âŒ Provider {provider} test failed: {e}")
            return False


class FallbackLLM:
    """
    è¿è¡Œæ—¶é™çº§ LLM åŒ…è£…å™¨
    
    å½“ä¸» LLM è°ƒç”¨å¤±è´¥æ—¶ï¼Œè‡ªåŠ¨å°è¯•å¤‡é€‰ LLM
    
    Example:
        llm = FallbackLLM(providers=["openai", "openrouter", "deepseek"])
        response = llm.invoke("Hello")  # è‡ªåŠ¨é™çº§
    """
    
    def __init__(
        self,
        providers: list[str],
        temperature: float = None,
        max_tokens: int = None,
        timeout: int = None,
        max_retries: int = None,
        **kwargs
    ):
        """åˆå§‹åŒ–é™çº§ LLM"""
        self.providers = providers
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout
        self.max_retries = max_retries
        self.kwargs = kwargs
        
        # é¢„åˆ›å»ºæ‰€æœ‰ LLM å®ä¾‹
        self.llm_instances = {}
        for provider in providers:
            try:
                llm = MultiLLMFactory.create_llm(
                    provider=provider,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    timeout=timeout,
                    max_retries=max_retries,
                    **kwargs
                )
                self.llm_instances[provider] = llm
                logger.info(f"âœ… é¢„åˆ›å»º {provider} LLM æˆåŠŸ")
            except Exception as e:
                logger.warning(f"âš ï¸ é¢„åˆ›å»º {provider} LLM å¤±è´¥: {e}")
        
        if not self.llm_instances:
            raise RuntimeError(f"æ— æ³•åˆ›å»ºä»»ä½• LLM å®ä¾‹: {providers}")
        
        logger.info(f"ğŸ”„ é™çº§é“¾å°±ç»ª: {' â†’ '.join(self.llm_instances.keys())}")
    
    def invoke(self, *args, **kwargs):
        """è°ƒç”¨ LLMï¼Œæ”¯æŒè‡ªåŠ¨é™çº§"""
        last_error = None
        
        for provider, llm in self.llm_instances.items():
            try:
                logger.debug(f"ğŸ”§ å°è¯•ä½¿ç”¨ {provider} è°ƒç”¨ LLM...")
                response = llm.invoke(*args, **kwargs)
                logger.success(f"âœ… {provider} è°ƒç”¨æˆåŠŸ")
                return response
                
            except Exception as e:
                error_msg = str(e)
                logger.warning(f"âš ï¸ {provider} è°ƒç”¨å¤±è´¥: {error_msg[:100]}")
                last_error = e
                
                # å¦‚æœæ˜¯é…é¢é—®é¢˜ï¼Œç«‹å³å°è¯•ä¸‹ä¸€ä¸ª
                if "429" in error_msg or "quota" in error_msg.lower():
                    logger.info(f"ğŸ”„ æ£€æµ‹åˆ°é…é¢é—®é¢˜ï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæä¾›å•†...")
                    continue
                
                # å…¶ä»–é”™è¯¯ä¹Ÿå°è¯•é™çº§
                logger.info(f"ğŸ”„ å°è¯•é™çº§åˆ°ä¸‹ä¸€ä¸ªæä¾›å•†...")
                continue
        
        # æ‰€æœ‰æä¾›å•†éƒ½å¤±è´¥
        logger.error(f"âŒ æ‰€æœ‰æä¾›å•†è°ƒç”¨å¤±è´¥")
        raise RuntimeError(f"æ‰€æœ‰ LLM æä¾›å•†è°ƒç”¨å¤±è´¥: {list(self.llm_instances.keys())}") from last_error
    
    def __getattr__(self, name):
        """ä»£ç†å…¶ä»–æ–¹æ³•åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨çš„ LLM"""
        if self.llm_instances:
            first_llm = next(iter(self.llm_instances.values()))
            return getattr(first_llm, name)
        raise AttributeError(f"FallbackLLM has no attribute '{name}'")
