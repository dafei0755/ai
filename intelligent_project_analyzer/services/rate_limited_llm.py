# -*- coding: utf-8 -*-
"""
é™æµå¢å¼ºçš„ LLM åŒ…è£…å™¨

æä¾›å¸¦é™æµåŠŸèƒ½çš„ LLM è°ƒç”¨æ¥å£
"""

from typing import Any, List, Optional, Dict
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import BaseMessage
from langchain_core.outputs import ChatResult
from loguru import logger

from intelligent_project_analyzer.services.rate_limiter import (
    rate_limit_manager,
    RateLimitExceeded,
    RateLimitConfig
)


class RateLimitedLLM:
    """
    å¸¦é™æµçš„ LLM åŒ…è£…å™¨
    
    åŒ…è£…ä»»æ„ LangChain LLMï¼Œæ·»åŠ é™æµæ§åˆ¶
    
    Usage:
        from intelligent_project_analyzer.services.llm_factory import LLMFactory
        
        # åˆ›å»ºé™æµ LLM
        base_llm = LLMFactory.create_llm()
        llm = RateLimitedLLM(base_llm, provider="openai")
        
        # ä½¿ç”¨ï¼ˆè‡ªåŠ¨é™æµï¼‰
        result = llm.invoke("Hello")
        result = await llm.ainvoke("Hello")
    """
    
    def __init__(
        self,
        llm: BaseChatModel,
        provider: str = "openai",
        user_id: Optional[str] = None,
        custom_config: Optional[RateLimitConfig] = None
    ):
        """
        åˆå§‹åŒ–é™æµ LLM
        
        Args:
            llm: åŸå§‹ LLM å®ä¾‹
            provider: LLM æä¾›å•†ï¼ˆç”¨äºé€‰æ‹©é™æµé…ç½®ï¼‰
            user_id: ç”¨æˆ·IDï¼ˆç”¨äºç”¨æˆ·çº§åˆ«é™æµï¼‰
            custom_config: è‡ªå®šä¹‰é™æµé…ç½®
        """
        self._llm = llm
        self._provider = provider
        self._user_id = user_id
        
        # è·å–æˆ–åˆ›å»ºé™æµå™¨
        if custom_config:
            from intelligent_project_analyzer.services.rate_limiter import LLMRateLimiter
            self._limiter = LLMRateLimiter(provider, custom_config)
        else:
            self._limiter = rate_limit_manager.get_limiter(provider, user_id)
        
        logger.info(f"ğŸ”’ åˆ›å»ºé™æµ LLM: provider={provider}, user_id={user_id}")
    
    def invoke(self, input: Any, **kwargs) -> Any:
        """
        åŒæ­¥è°ƒç”¨ï¼ˆå¸¦é™æµï¼‰
        """
        if not self._limiter.acquire_sync():
            raise RateLimitExceeded(
                f"LLM API é™æµ: {self._provider}, "
                f"stats={self._limiter.stats}"
            )
        
        try:
            return self._llm.invoke(input, **kwargs)
        finally:
            self._limiter.release_sync()
    
    async def ainvoke(self, input: Any, **kwargs) -> Any:
        """
        å¼‚æ­¥è°ƒç”¨ï¼ˆå¸¦é™æµï¼‰
        """
        if not await self._limiter.acquire_async():
            raise RateLimitExceeded(
                f"LLM API é™æµ: {self._provider}, "
                f"stats={self._limiter.stats}"
            )
        
        try:
            return await self._llm.ainvoke(input, **kwargs)
        finally:
            self._limiter.release_async()
    
    def batch(self, inputs: List[Any], **kwargs) -> List[Any]:
        """
        æ‰¹é‡è°ƒç”¨ï¼ˆå¸¦é™æµï¼‰
        """
        results = []
        for input in inputs:
            result = self.invoke(input, **kwargs)
            results.append(result)
        return results
    
    async def abatch(self, inputs: List[Any], **kwargs) -> List[Any]:
        """
        å¼‚æ­¥æ‰¹é‡è°ƒç”¨ï¼ˆå¸¦é™æµï¼‰
        """
        results = []
        for input in inputs:
            result = await self.ainvoke(input, **kwargs)
            results.append(result)
        return results
    
    def stream(self, input: Any, **kwargs):
        """
        æµå¼è°ƒç”¨ï¼ˆå¸¦é™æµï¼‰
        """
        if not self._limiter.acquire_sync():
            raise RateLimitExceeded(f"LLM API é™æµ: {self._provider}")
        
        try:
            for chunk in self._llm.stream(input, **kwargs):
                yield chunk
        finally:
            self._limiter.release_sync()
    
    async def astream(self, input: Any, **kwargs):
        """
        å¼‚æ­¥æµå¼è°ƒç”¨ï¼ˆå¸¦é™æµï¼‰
        """
        if not await self._limiter.acquire_async():
            raise RateLimitExceeded(f"LLM API é™æµ: {self._provider}")
        
        try:
            async for chunk in self._llm.astream(input, **kwargs):
                yield chunk
        finally:
            self._limiter.release_async()
    
    @property
    def stats(self) -> Dict[str, Any]:
        """è·å–é™æµç»Ÿè®¡"""
        return self._limiter.stats
    
    @property
    def model_name(self) -> str:
        """è·å–æ¨¡å‹åç§°"""
        if hasattr(self._llm, 'model_name'):
            return self._llm.model_name
        elif hasattr(self._llm, 'model'):
            return self._llm.model
        return "unknown"
    
    def __getattr__(self, name):
        """ä»£ç†å…¶ä»–å±æ€§åˆ°åŸå§‹ LLM"""
        return getattr(self._llm, name)


def create_rate_limited_llm(
    provider: str = "openai",
    user_id: Optional[str] = None,
    **llm_kwargs
) -> RateLimitedLLM:
    """
    ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºé™æµ LLM
    
    Args:
        provider: LLM æä¾›å•†
        user_id: ç”¨æˆ·ID
        **llm_kwargs: ä¼ é€’ç»™ LLMFactory.create_llm çš„å‚æ•°
        
    Returns:
        å¸¦é™æµçš„ LLM å®ä¾‹
        
    Usage:
        llm = create_rate_limited_llm("openai", user_id="user123")
        result = await llm.ainvoke("Hello")
    """
    from intelligent_project_analyzer.services.llm_factory import LLMFactory
    
    base_llm = LLMFactory.create_llm(**llm_kwargs)
    return RateLimitedLLM(base_llm, provider=provider, user_id=user_id)
