# -*- coding: utf-8 -*-
"""
LLM API é™æµæ¨¡å—

æä¾›å¤šç§é™æµç­–ç•¥ï¼Œé˜²æ­¢ API è°ƒç”¨è¶…é™ï¼š
1. ä»¤ç‰Œæ¡¶é™æµ - æ§åˆ¶è¯·æ±‚é€Ÿç‡
2. ä¿¡å·é‡é™æµ - æ§åˆ¶å¹¶å‘æ•°é‡
3. æ»‘åŠ¨çª—å£é™æµ - æ§åˆ¶æ—¶é—´çª—å£å†…çš„è¯·æ±‚æ•°
4. ç”¨æˆ·çº§åˆ«é™æµ - æŒ‰ç”¨æˆ·éš”ç¦»é…é¢

æ”¯æŒå¤šæä¾›å•†ä¸åŒçš„é™æµé…ç½®
"""

import asyncio
import time
import threading
from typing import Dict, Optional, Callable, Any
from dataclasses import dataclass, field
from collections import deque
from functools import wraps
from loguru import logger


@dataclass
class RateLimitConfig:
    """é™æµé…ç½®"""
    # ä»¤ç‰Œæ¡¶é…ç½®
    tokens_per_second: float = 1.0      # æ¯ç§’ç”Ÿæˆçš„ä»¤ç‰Œæ•°
    bucket_size: int = 10               # ä»¤ç‰Œæ¡¶æœ€å¤§å®¹é‡
    
    # å¹¶å‘æ§åˆ¶
    max_concurrent: int = 5             # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    
    # æ»‘åŠ¨çª—å£é…ç½®
    window_size: int = 60               # çª—å£å¤§å°ï¼ˆç§’ï¼‰
    max_requests_per_window: int = 60   # çª—å£å†…æœ€å¤§è¯·æ±‚æ•°
    
    # è¶…æ—¶é…ç½®
    acquire_timeout: float = 30.0       # è·å–ä»¤ç‰Œçš„è¶…æ—¶æ—¶é—´
    
    # é‡è¯•é…ç½®
    retry_after: float = 1.0            # é™æµåçš„é‡è¯•é—´éš”


# å„ LLM æä¾›å•†çš„é»˜è®¤é™æµé…ç½®
PROVIDER_RATE_LIMITS: Dict[str, RateLimitConfig] = {
    "openai": RateLimitConfig(
        tokens_per_second=3.0,      # GPT-4: ~200 RPM â†’ 3.3 RPS
        bucket_size=10,
        max_concurrent=5,
        window_size=60,
        max_requests_per_window=200,
    ),
    "deepseek": RateLimitConfig(
        tokens_per_second=5.0,      # DeepSeek é™åˆ¶è¾ƒå®½æ¾
        bucket_size=20,
        max_concurrent=10,
        window_size=60,
        max_requests_per_window=300,
    ),
    "qwen": RateLimitConfig(
        tokens_per_second=2.0,      # é€šä¹‰åƒé—®
        bucket_size=10,
        max_concurrent=5,
        window_size=60,
        max_requests_per_window=100,
    ),
    "openrouter": RateLimitConfig(
        tokens_per_second=1.0,      # OpenRouter å…è´¹å±‚è¾ƒä¸¥æ ¼
        bucket_size=5,
        max_concurrent=3,
        window_size=60,
        max_requests_per_window=50,
    ),
    "gemini": RateLimitConfig(
        tokens_per_second=1.0,      # Gemini å…è´¹å±‚
        bucket_size=5,
        max_concurrent=3,
        window_size=60,
        max_requests_per_window=60,
    ),
}


class TokenBucket:
    """
    ä»¤ç‰Œæ¡¶é™æµå™¨
    
    ä»¥å›ºå®šé€Ÿç‡ç”Ÿæˆä»¤ç‰Œï¼Œè¯·æ±‚éœ€è¦è·å–ä»¤ç‰Œæ‰èƒ½æ‰§è¡Œ
    """
    
    def __init__(self, tokens_per_second: float, bucket_size: int):
        self.tokens_per_second = tokens_per_second
        self.bucket_size = bucket_size
        self.tokens = bucket_size  # åˆå§‹æ»¡æ¡¶
        self.last_update = time.monotonic()
        self._lock = threading.Lock()
    
    def _refill(self):
        """è¡¥å……ä»¤ç‰Œ"""
        now = time.monotonic()
        elapsed = now - self.last_update
        new_tokens = elapsed * self.tokens_per_second
        self.tokens = min(self.bucket_size, self.tokens + new_tokens)
        self.last_update = now
    
    def acquire(self, timeout: float = 30.0) -> bool:
        """
        è·å–ä¸€ä¸ªä»¤ç‰Œ
        
        Args:
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸè·å–ä»¤ç‰Œ
        """
        deadline = time.monotonic() + timeout
        
        while True:
            with self._lock:
                self._refill()
                if self.tokens >= 1:
                    self.tokens -= 1
                    return True
            
            # æ£€æŸ¥è¶…æ—¶
            if time.monotonic() >= deadline:
                return False
            
            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´å†é‡è¯•
            time.sleep(0.1)
    
    async def acquire_async(self, timeout: float = 30.0) -> bool:
        """å¼‚æ­¥è·å–ä»¤ç‰Œ"""
        deadline = time.monotonic() + timeout
        
        while True:
            with self._lock:
                self._refill()
                if self.tokens >= 1:
                    self.tokens -= 1
                    return True
            
            if time.monotonic() >= deadline:
                return False
            
            await asyncio.sleep(0.1)
    
    @property
    def available_tokens(self) -> float:
        """å½“å‰å¯ç”¨ä»¤ç‰Œæ•°"""
        with self._lock:
            self._refill()
            return self.tokens


class SlidingWindowCounter:
    """
    æ»‘åŠ¨çª—å£è®¡æ•°å™¨
    
    ç»Ÿè®¡æ—¶é—´çª—å£å†…çš„è¯·æ±‚æ•°é‡
    """
    
    def __init__(self, window_size: int, max_requests: int):
        self.window_size = window_size
        self.max_requests = max_requests
        self.requests: deque = deque()
        self._lock = threading.Lock()
    
    def _cleanup(self):
        """æ¸…ç†è¿‡æœŸè¯·æ±‚"""
        now = time.monotonic()
        cutoff = now - self.window_size
        while self.requests and self.requests[0] < cutoff:
            self.requests.popleft()
    
    def allow(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦å…è®¸è¯·æ±‚
        
        Returns:
            True å¦‚æœå…è®¸ï¼ŒFalse å¦‚æœè¶…é™
        """
        with self._lock:
            self._cleanup()
            if len(self.requests) < self.max_requests:
                self.requests.append(time.monotonic())
                return True
            return False
    
    @property
    def current_count(self) -> int:
        """å½“å‰çª—å£å†…çš„è¯·æ±‚æ•°"""
        with self._lock:
            self._cleanup()
            return len(self.requests)
    
    @property
    def remaining(self) -> int:
        """å‰©ä½™å¯ç”¨è¯·æ±‚æ•°"""
        return max(0, self.max_requests - self.current_count)


class ConcurrencyLimiter:
    """
    å¹¶å‘é™åˆ¶å™¨
    
    ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶åŒæ—¶æ‰§è¡Œçš„è¯·æ±‚æ•°
    """
    
    def __init__(self, max_concurrent: int):
        self.max_concurrent = max_concurrent
        self._semaphore = asyncio.Semaphore(max_concurrent)
        self._sync_semaphore = threading.Semaphore(max_concurrent)
        self._active = 0
        self._lock = threading.Lock()
    
    async def acquire_async(self):
        """å¼‚æ­¥è·å–ä¿¡å·é‡"""
        await self._semaphore.acquire()
        with self._lock:
            self._active += 1
    
    def release_async(self):
        """å¼‚æ­¥é‡Šæ”¾ä¿¡å·é‡"""
        self._semaphore.release()
        with self._lock:
            self._active -= 1
    
    def acquire_sync(self, timeout: float = 30.0) -> bool:
        """åŒæ­¥è·å–ä¿¡å·é‡"""
        acquired = self._sync_semaphore.acquire(timeout=timeout)
        if acquired:
            with self._lock:
                self._active += 1
        return acquired
    
    def release_sync(self):
        """åŒæ­¥é‡Šæ”¾ä¿¡å·é‡"""
        self._sync_semaphore.release()
        with self._lock:
            self._active -= 1
    
    @property
    def active_count(self) -> int:
        """å½“å‰æ´»è·ƒè¯·æ±‚æ•°"""
        with self._lock:
            return self._active
    
    @property
    def available(self) -> int:
        """å¯ç”¨å¹¶å‘æ•°"""
        return self.max_concurrent - self.active_count


class LLMRateLimiter:
    """
    LLM API ç»¼åˆé™æµå™¨
    
    æ•´åˆä»¤ç‰Œæ¡¶ã€æ»‘åŠ¨çª—å£å’Œå¹¶å‘æ§åˆ¶
    """
    
    def __init__(self, provider: str = "openai", config: Optional[RateLimitConfig] = None):
        """
        åˆå§‹åŒ–é™æµå™¨
        
        Args:
            provider: LLM æä¾›å•†åç§°
            config: è‡ªå®šä¹‰é™æµé…ç½®ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.provider = provider
        self.config = config or PROVIDER_RATE_LIMITS.get(provider, RateLimitConfig())
        
        # åˆå§‹åŒ–å„é™æµç»„ä»¶
        self.token_bucket = TokenBucket(
            tokens_per_second=self.config.tokens_per_second,
            bucket_size=self.config.bucket_size
        )
        
        self.sliding_window = SlidingWindowCounter(
            window_size=self.config.window_size,
            max_requests=self.config.max_requests_per_window
        )
        
        self.concurrency_limiter = ConcurrencyLimiter(
            max_concurrent=self.config.max_concurrent
        )
        
        # ç»Ÿè®¡
        self._total_requests = 0
        self._rejected_requests = 0
        self._lock = threading.Lock()
        
        logger.info(
            f"ğŸš¦ LLM é™æµå™¨åˆå§‹åŒ–: provider={provider}, "
            f"rate={self.config.tokens_per_second}/s, "
            f"concurrent={self.config.max_concurrent}, "
            f"window={self.config.max_requests_per_window}/{self.config.window_size}s"
        )
    
    async def acquire_async(self) -> bool:
        """
        å¼‚æ­¥è·å–æ‰§è¡Œè®¸å¯
        
        Returns:
            True å¦‚æœè·å–æˆåŠŸï¼ŒFalse å¦‚æœè¢«é™æµ
        """
        # 1. æ£€æŸ¥æ»‘åŠ¨çª—å£
        if not self.sliding_window.allow():
            logger.warning(f"âš ï¸ [{self.provider}] æ»‘åŠ¨çª—å£é™æµ: {self.sliding_window.current_count}/{self.config.max_requests_per_window}")
            self._record_rejection()
            return False
        
        # 2. è·å–ä»¤ç‰Œ
        if not await self.token_bucket.acquire_async(timeout=self.config.acquire_timeout):
            logger.warning(f"âš ï¸ [{self.provider}] ä»¤ç‰Œæ¡¶é™æµ")
            self._record_rejection()
            return False
        
        # 3. è·å–å¹¶å‘è®¸å¯
        await self.concurrency_limiter.acquire_async()
        
        self._record_request()
        return True
    
    def release_async(self):
        """å¼‚æ­¥é‡Šæ”¾å¹¶å‘è®¸å¯"""
        self.concurrency_limiter.release_async()
    
    def acquire_sync(self) -> bool:
        """
        åŒæ­¥è·å–æ‰§è¡Œè®¸å¯
        
        Returns:
            True å¦‚æœè·å–æˆåŠŸï¼ŒFalse å¦‚æœè¢«é™æµ
        """
        # 1. æ£€æŸ¥æ»‘åŠ¨çª—å£
        if not self.sliding_window.allow():
            logger.warning(f"âš ï¸ [{self.provider}] æ»‘åŠ¨çª—å£é™æµ")
            self._record_rejection()
            return False
        
        # 2. è·å–ä»¤ç‰Œ
        if not self.token_bucket.acquire(timeout=self.config.acquire_timeout):
            logger.warning(f"âš ï¸ [{self.provider}] ä»¤ç‰Œæ¡¶é™æµ")
            self._record_rejection()
            return False
        
        # 3. è·å–å¹¶å‘è®¸å¯
        if not self.concurrency_limiter.acquire_sync(timeout=self.config.acquire_timeout):
            logger.warning(f"âš ï¸ [{self.provider}] å¹¶å‘é™æµ")
            self._record_rejection()
            return False
        
        self._record_request()
        return True
    
    def release_sync(self):
        """åŒæ­¥é‡Šæ”¾å¹¶å‘è®¸å¯"""
        self.concurrency_limiter.release_sync()
    
    def _record_request(self):
        """è®°å½•è¯·æ±‚"""
        with self._lock:
            self._total_requests += 1
    
    def _record_rejection(self):
        """è®°å½•æ‹’ç»"""
        with self._lock:
            self._rejected_requests += 1
    
    @property
    def stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "provider": self.provider,
            "total_requests": self._total_requests,
            "rejected_requests": self._rejected_requests,
            "rejection_rate": self._rejected_requests / max(1, self._total_requests + self._rejected_requests),
            "available_tokens": self.token_bucket.available_tokens,
            "window_remaining": self.sliding_window.remaining,
            "concurrent_active": self.concurrency_limiter.active_count,
            "concurrent_available": self.concurrency_limiter.available,
        }


class GlobalRateLimitManager:
    """
    å…¨å±€é™æµç®¡ç†å™¨
    
    ç®¡ç†æ‰€æœ‰ LLM æä¾›å•†çš„é™æµå™¨ï¼Œæ”¯æŒç”¨æˆ·çº§åˆ«éš”ç¦»
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        self._limiters: Dict[str, LLMRateLimiter] = {}
        self._user_limiters: Dict[str, Dict[str, LLMRateLimiter]] = {}
        self._initialized = True
        
        logger.info("ğŸš¦ å…¨å±€é™æµç®¡ç†å™¨å·²åˆå§‹åŒ–")
    
    def get_limiter(self, provider: str = "openai", user_id: Optional[str] = None) -> LLMRateLimiter:
        """
        è·å–é™æµå™¨
        
        Args:
            provider: LLM æä¾›å•†
            user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼Œç”¨äºç”¨æˆ·çº§åˆ«é™æµï¼‰
            
        Returns:
            é™æµå™¨å®ä¾‹
        """
        if user_id:
            # ç”¨æˆ·çº§åˆ«é™æµ
            if user_id not in self._user_limiters:
                self._user_limiters[user_id] = {}
            
            if provider not in self._user_limiters[user_id]:
                # ç”¨æˆ·çº§åˆ«é…ç½®ï¼ˆæ›´ä¸¥æ ¼ï¼‰
                base_config = PROVIDER_RATE_LIMITS.get(provider, RateLimitConfig())
                user_config = RateLimitConfig(
                    tokens_per_second=base_config.tokens_per_second / 2,
                    bucket_size=max(1, base_config.bucket_size // 2),
                    max_concurrent=max(1, base_config.max_concurrent // 2),
                    window_size=base_config.window_size,
                    max_requests_per_window=base_config.max_requests_per_window // 2,
                )
                self._user_limiters[user_id][provider] = LLMRateLimiter(provider, user_config)
            
            return self._user_limiters[user_id][provider]
        
        # å…¨å±€é™æµ
        if provider not in self._limiters:
            self._limiters[provider] = LLMRateLimiter(provider)
        
        return self._limiters[provider]
    
    def get_all_stats(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰é™æµå™¨çš„ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "global": {provider: limiter.stats for provider, limiter in self._limiters.items()},
            "users": {
                user_id: {provider: limiter.stats for provider, limiter in user_limiters.items()}
                for user_id, user_limiters in self._user_limiters.items()
            }
        }
        return stats


# å…¨å±€å®ä¾‹
rate_limit_manager = GlobalRateLimitManager()


# ==================== è£…é¥°å™¨ ====================

def rate_limited(provider: str = "openai", user_id: Optional[str] = None):
    """
    åŒæ­¥å‡½æ•°é™æµè£…é¥°å™¨
    
    Usage:
        @rate_limited("openai")
        def call_openai(prompt):
            return llm.invoke(prompt)
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            limiter = rate_limit_manager.get_limiter(provider, user_id)
            
            if not limiter.acquire_sync():
                raise RateLimitExceeded(f"LLM API é™æµ: {provider}")
            
            try:
                return func(*args, **kwargs)
            finally:
                limiter.release_sync()
        
        return wrapper
    return decorator


def rate_limited_async(provider: str = "openai", user_id: Optional[str] = None):
    """
    å¼‚æ­¥å‡½æ•°é™æµè£…é¥°å™¨
    
    Usage:
        @rate_limited_async("openai")
        async def call_openai(prompt):
            return await llm.ainvoke(prompt)
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            limiter = rate_limit_manager.get_limiter(provider, user_id)
            
            if not await limiter.acquire_async():
                raise RateLimitExceeded(f"LLM API é™æµ: {provider}")
            
            try:
                return await func(*args, **kwargs)
            finally:
                limiter.release_async()
        
        return wrapper
    return decorator


class RateLimitExceeded(Exception):
    """é™æµå¼‚å¸¸"""
    pass


# ==================== ä¾¿æ·å‡½æ•° ====================

async def with_rate_limit_async(
    provider: str,
    func: Callable,
    *args,
    user_id: Optional[str] = None,
    **kwargs
) -> Any:
    """
    ä½¿ç”¨é™æµæ‰§è¡Œå¼‚æ­¥å‡½æ•°
    
    Usage:
        result = await with_rate_limit_async("openai", llm.ainvoke, prompt)
    """
    limiter = rate_limit_manager.get_limiter(provider, user_id)
    
    if not await limiter.acquire_async():
        raise RateLimitExceeded(f"LLM API é™æµ: {provider}")
    
    try:
        return await func(*args, **kwargs)
    finally:
        limiter.release_async()


def with_rate_limit_sync(
    provider: str,
    func: Callable,
    *args,
    user_id: Optional[str] = None,
    **kwargs
) -> Any:
    """
    ä½¿ç”¨é™æµæ‰§è¡ŒåŒæ­¥å‡½æ•°
    
    Usage:
        result = with_rate_limit_sync("openai", llm.invoke, prompt)
    """
    limiter = rate_limit_manager.get_limiter(provider, user_id)
    
    if not limiter.acquire_sync():
        raise RateLimitExceeded(f"LLM API é™æµ: {provider}")
    
    try:
        return func(*args, **kwargs)
    finally:
        limiter.release_sync()
