"""
Arxivå­¦æœ¯æœç´¢å·¥å…·

æä¾›å­¦æœ¯è®ºæ–‡æœç´¢åŠŸèƒ½ï¼Œç”¨äºè·å–ç›¸å…³ç ”ç©¶å’ŒæŠ€æœ¯æ–‡çŒ®
"""

import json
import time
from datetime import datetime
from typing import Any, Dict, List, Optional, Union

from loguru import logger

try:
    import arxiv
except ImportError:
    logger.warning("Arxiv library not installed. Please install with: pip install arxiv")
    arxiv = None

from ..core.types import ToolConfig

# LangChain Tool integration
try:
    from langchain_core.tools import StructuredTool
    from pydantic import BaseModel, Field

    LANGCHAIN_AVAILABLE = True
except ImportError:
    logger.warning("LangChain not available, tool wrapping disabled")
    LANGCHAIN_AVAILABLE = False

# ğŸ†• v7.64: å¯¼å…¥ç²¾å‡†æœç´¢å’Œè´¨é‡æ§åˆ¶æ¨¡å—
try:
    from .quality_control import SearchQualityControl
    from .query_builder import DeliverableQueryBuilder
except ImportError:
    logger.warning("âš ï¸ v7.64 modules not available. search_for_deliverable() will use fallback mode.")
    DeliverableQueryBuilder = None
    SearchQualityControl = None


class ArxivSearchTool:
    """Arxivå­¦æœ¯æœç´¢å·¥å…·ç±»"""

    def __init__(self, config: Optional[ToolConfig] = None):
        """
        åˆå§‹åŒ–Arxivæœç´¢å·¥å…·

        Args:
            config: å·¥å…·é…ç½®
        """
        if arxiv is None:
            raise ImportError("Arxiv library not installed. Please install with: pip install arxiv")

        self.config = config or ToolConfig(name="arxiv_search")
        self.name = self.config.name  # LangChain compatibility

        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self.client = arxiv.Client(page_size=100, delay_seconds=3.0, num_retries=3)  # æ¯é¡µç»“æœæ•°  # è¯·æ±‚é—´å»¶è¿Ÿ  # é‡è¯•æ¬¡æ•°

        # é»˜è®¤æœç´¢å‚æ•°
        self.default_params = {
            "max_results": 10,
            "sort_by": arxiv.SortCriterion.Relevance,
            "sort_order": arxiv.SortOrder.Descending,
        }

        # ğŸ†• v7.64: åˆå§‹åŒ–ç²¾å‡†æœç´¢å’Œè´¨é‡æ§åˆ¶æ¨¡å—
        self.query_builder = DeliverableQueryBuilder() if DeliverableQueryBuilder else None
        self.qc = SearchQualityControl() if SearchQualityControl else None

    def search(
        self,
        query: str,
        max_results: Optional[int] = None,
        sort_by: Optional[arxiv.SortCriterion] = None,
        sort_order: Optional[arxiv.SortOrder] = None,
        categories: Optional[List[str]] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå­¦æœ¯è®ºæ–‡æœç´¢

        Args:
            query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
            max_results: æœ€å¤§ç»“æœæ•°é‡
            sort_by: æ’åºæ–¹å¼
            sort_order: æ’åºé¡ºåº
            categories: è®ºæ–‡åˆ†ç±»è¿‡æ»¤
            **kwargs: å…¶ä»–æœç´¢å‚æ•°

        Returns:
            æœç´¢ç»“æœå­—å…¸
        """
        try:
            start_time = time.time()

            # å‡†å¤‡æœç´¢å‚æ•°
            search_params = {
                "query": query,
                "max_results": max_results or self.default_params["max_results"],
                "sort_by": sort_by or self.default_params["sort_by"],
                "sort_order": sort_order or self.default_params["sort_order"],
            }

            # å¦‚æœæŒ‡å®šäº†åˆ†ç±»ï¼Œæ·»åŠ åˆ°æŸ¥è¯¢ä¸­
            if categories:
                category_filter = " OR ".join([f"cat:{cat}" for cat in categories])
                search_params["query"] = f"({query}) AND ({category_filter})"

            logger.info(f"ğŸ” [Arxiv] Starting search")
            logger.info(f"ğŸ“ [Arxiv] Query: {query}")
            logger.debug(
                f"âš™ï¸ [Arxiv] Search params: max_results={search_params['max_results']}, sort_by={search_params['sort_by']}"
            )
            if categories:
                logger.debug(f"ğŸ·ï¸ [Arxiv] Categories filter: {categories}")

            # åˆ›å»ºæœç´¢å¯¹è±¡
            logger.debug(f"ğŸ“š [Arxiv] Creating search object...")
            search = arxiv.Search(**search_params)

            # æ‰§è¡Œæœç´¢
            logger.debug(f"ğŸŒ [Arxiv] Calling Arxiv API...")
            api_start = time.time()
            results = list(self.client.results(search))
            api_time = time.time() - api_start

            logger.info(f"âœ… [Arxiv] API call completed in {api_time:.2f}s, found {len(results)} papers")
            if results:
                logger.debug(f"ğŸ“„ [Arxiv] First result: {results[0].title[:50]}...")
                logger.debug(f"ğŸ“Š [Arxiv] Categories in results: {set([r.primary_category for r in results[:5]])}")

            # å¤„ç†å“åº”
            logger.debug(f"âš™ï¸ [Arxiv] Processing response...")
            process_start = time.time()
            processed_response = self._process_search_response(results, query, time.time() - start_time)
            process_time = time.time() - process_start

            logger.debug(f"âš™ï¸ [Arxiv] Response processing took {process_time:.2f}s")
            logger.info(f"âœ… [Arxiv] Search completed in {processed_response['execution_time']:.2f}s")

            return processed_response

        except Exception as e:
            logger.error(f"âŒ [Arxiv] Search failed: {str(e)}", exc_info=True)
            logger.error(f"âŒ [Arxiv] Failed query: {query}")
            logger.error(f"âŒ [Arxiv] Categories: {categories if categories else 'None'}")
            return {"success": False, "error": str(e), "query": query, "results": [], "execution_time": 0}

    def search_by_id(self, paper_ids: List[str]) -> Dict[str, Any]:
        """
        æ ¹æ®è®ºæ–‡IDæœç´¢

        Args:
            paper_ids: è®ºæ–‡IDåˆ—è¡¨

        Returns:
            æœç´¢ç»“æœå­—å…¸
        """
        try:
            start_time = time.time()

            logger.info(f"Searching Arxiv by IDs: {paper_ids}")

            # åˆ›å»ºIDæœç´¢
            search = arxiv.Search(id_list=paper_ids)
            results = list(self.client.results(search))

            end_time = time.time()
            execution_time = end_time - start_time

            # å¤„ç†å“åº”
            processed_response = self._process_search_response(results, f"IDs: {paper_ids}", execution_time)

            logger.info(f"Arxiv ID search completed in {execution_time:.2f}s")

            return processed_response

        except Exception as e:
            logger.error(f"Arxiv ID search failed: {str(e)}")
            return {"success": False, "error": str(e), "query": f"IDs: {paper_ids}", "results": [], "execution_time": 0}

    def search_recent_papers(self, query: str, days_back: int = 30, max_results: int = 10) -> Dict[str, Any]:
        """
        æœç´¢æœ€è¿‘å‘å¸ƒçš„è®ºæ–‡

        Args:
            query: æœç´¢æŸ¥è¯¢
            days_back: å›æº¯å¤©æ•°
            max_results: æœ€å¤§ç»“æœæ•°

        Returns:
            æœç´¢ç»“æœå­—å…¸
        """
        return self.search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate,
            sort_order=arxiv.SortOrder.Descending,
        )

    def search_by_author(self, author_name: str, max_results: int = 10) -> Dict[str, Any]:
        """
        æ ¹æ®ä½œè€…æœç´¢è®ºæ–‡

        Args:
            author_name: ä½œè€…å§“å
            max_results: æœ€å¤§ç»“æœæ•°

        Returns:
            æœç´¢ç»“æœå­—å…¸
        """
        query = f"au:{author_name}"
        return self.search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate,
            sort_order=arxiv.SortOrder.Descending,
        )

    def search_by_category(self, categories: List[str], query: str = "", max_results: int = 10) -> Dict[str, Any]:
        """
        æ ¹æ®åˆ†ç±»æœç´¢è®ºæ–‡

        Args:
            categories: åˆ†ç±»åˆ—è¡¨ (å¦‚ ["cs.AI", "cs.LG"])
            query: é¢å¤–æŸ¥è¯¢æ¡ä»¶
            max_results: æœ€å¤§ç»“æœæ•°

        Returns:
            æœç´¢ç»“æœå­—å…¸
        """
        category_query = " OR ".join([f"cat:{cat}" for cat in categories])

        if query:
            full_query = f"({query}) AND ({category_query})"
        else:
            full_query = category_query

        return self.search(query=full_query, max_results=max_results)

    def _process_search_response(
        self, results: List[arxiv.Result], query: str, execution_time: float
    ) -> Dict[str, Any]:
        """
        å¤„ç†æœç´¢å“åº”ï¼Œæ ‡å‡†åŒ–æ ¼å¼

        Args:
            results: Arxivæœç´¢ç»“æœåˆ—è¡¨
            query: åŸå§‹æŸ¥è¯¢
            execution_time: æ‰§è¡Œæ—¶é—´

        Returns:
            å¤„ç†åçš„å“åº”
        """
        processed = {
            "success": True,
            "query": query,
            "results": [],
            "execution_time": execution_time,
            "total_results": len(results),
        }

        # å¤„ç†æ¯ä¸ªç»“æœ
        for result in results:
            processed_result = {
                "title": result.title,
                "authors": [author.name for author in result.authors],
                "summary": result.summary,
                "published": result.published.isoformat() if result.published else None,
                "updated": result.updated.isoformat() if result.updated else None,
                "entry_id": result.entry_id,
                "arxiv_id": result.get_short_id(),
                "categories": result.categories,
                "primary_category": result.primary_category,
                "pdf_url": result.pdf_url,
                "links": [{"href": link.href, "title": link.title} for link in result.links],
                "journal_ref": result.journal_ref,
                "doi": result.doi,
                "comment": result.comment,
            }
            processed["results"].append(processed_result)

        return processed

    def search_for_agent(
        self, query: str, agent_context: str = "", max_results: int = 5, focus_recent: bool = True
    ) -> Dict[str, Any]:
        """
        ä¸ºæ™ºèƒ½ä½“ä¼˜åŒ–çš„æœç´¢æ–¹æ³•

        Args:
            query: æœç´¢æŸ¥è¯¢
            agent_context: æ™ºèƒ½ä½“ä¸Šä¸‹æ–‡ä¿¡æ¯
            max_results: æœ€å¤§ç»“æœæ•°
            focus_recent: æ˜¯å¦å…³æ³¨æœ€æ–°è®ºæ–‡

        Returns:
            ä¼˜åŒ–åçš„æœç´¢ç»“æœ
        """
        # æ ¹æ®æ™ºèƒ½ä½“ä¸Šä¸‹æ–‡ä¼˜åŒ–æŸ¥è¯¢
        enhanced_query = query
        if agent_context:
            # å¯ä»¥æ ¹æ®ä¸åŒæ™ºèƒ½ä½“ç±»å‹æ·»åŠ ç‰¹å®šçš„åˆ†ç±»è¿‡æ»¤
            if "æŠ€æœ¯" in agent_context or "architecture" in agent_context.lower():
                enhanced_query = f"({query}) AND (cat:cs.SE OR cat:cs.AI OR cat:cs.LG)"
            elif "è®¾è®¡" in agent_context or "design" in agent_context.lower():
                enhanced_query = f"({query}) AND (cat:cs.HC OR cat:cs.AI)"
            elif "å•†ä¸š" in agent_context or "business" in agent_context.lower():
                enhanced_query = f"({query}) AND (cat:econ.* OR cat:cs.CY)"

        # æ‰§è¡Œæœç´¢
        sort_by = arxiv.SortCriterion.SubmittedDate if focus_recent else arxiv.SortCriterion.Relevance

        results = self.search(
            query=enhanced_query, max_results=max_results, sort_by=sort_by, sort_order=arxiv.SortOrder.Descending
        )

        # ä¸ºæ™ºèƒ½ä½“ä¼˜åŒ–ç»“æœæ ¼å¼
        if results.get("success"):
            # æå–å…³é”®ä¿¡æ¯
            summary = {
                "query": query,
                "key_papers": [],
                "research_trends": [],
                "relevant_authors": set(),
                "categories": set(),
            }

            for result in results.get("results", []):
                # å…³é”®è®ºæ–‡ä¿¡æ¯
                summary["key_papers"].append(
                    {
                        "title": result["title"],
                        "authors": result["authors"][:3],  # å‰3ä¸ªä½œè€…
                        "summary": result["summary"][:300] + "..."
                        if len(result["summary"]) > 300
                        else result["summary"],
                        "published": result["published"],
                        "arxiv_id": result["arxiv_id"],
                        "categories": result["categories"],
                    }
                )

                # æ”¶é›†ä½œè€…å’Œåˆ†ç±»ä¿¡æ¯
                summary["relevant_authors"].update(result["authors"][:2])  # å‰2ä¸ªä½œè€…
                summary["categories"].update(result["categories"])

            # è½¬æ¢é›†åˆä¸ºåˆ—è¡¨
            summary["relevant_authors"] = list(summary["relevant_authors"])[:10]  # æœ€å¤š10ä¸ªä½œè€…
            summary["categories"] = list(summary["categories"])

            results["agent_summary"] = summary

        return results

    def search_for_deliverable(
        self,
        deliverable: Dict[str, Any],
        project_type: str = "",
        max_results: int = 10,
        enable_qc: bool = True,
        focus_recent: bool = False,
    ) -> Dict[str, Any]:
        """
        ğŸ†• v7.64: é’ˆå¯¹äº¤ä»˜ç‰©çš„ç²¾å‡†å­¦æœ¯æœç´¢

        æ ¸å¿ƒæ”¹è¿›ï¼š
        1. ä»äº¤ä»˜ç‰©æ„å»ºå­¦æœ¯åŒ–æŸ¥è¯¢ï¼ˆå¼ºè°ƒæ–¹æ³•è®ºæœ¯è¯­ï¼‰
        2. åº”ç”¨è´¨é‡æ§åˆ¶ç®¡é“
        3. è‡ªåŠ¨æ·»åŠ å­¦ç§‘åˆ†ç±»è¿‡æ»¤ï¼ˆåŸºäºé¡¹ç›®ç±»å‹ï¼‰

        Args:
            deliverable: äº¤ä»˜ç‰©å­—å…¸
            project_type: é¡¹ç›®ç±»å‹
            max_results: æœ€å¤§è¿”å›ç»“æœæ•°
            enable_qc: æ˜¯å¦å¯ç”¨è´¨é‡æ§åˆ¶
            focus_recent: æ˜¯å¦å…³æ³¨æœ€æ–°è®ºæ–‡

        Returns:
            æœç´¢ç»“æœå­—å…¸
        """
        try:
            start_time = time.time()
            deliverable_name = deliverable.get("name", "Unknown")

            logger.info(f"ğŸ¯ [Arxiv Deliverable] Starting search for deliverable: {deliverable_name}")
            logger.debug(
                f"ğŸ“‹ [Arxiv Deliverable] Deliverable details: {json.dumps(deliverable, ensure_ascii=False, indent=2)}"
            )
            logger.debug(f"ğŸ·ï¸ [Arxiv Deliverable] Project type: {project_type}")
            logger.debug(f"âš™ï¸ [Arxiv Deliverable] Max results: {max_results}, QC: {enable_qc}, Recent: {focus_recent}")

            # Step 1: æ„å»ºå­¦æœ¯åŒ–æŸ¥è¯¢
            logger.debug(f"ğŸ“ [Arxiv Deliverable] Step 1: Building academic query...")
            if self.query_builder:
                query_start = time.time()
                queries = self.query_builder.build_multi_tool_queries(deliverable, project_type)
                precise_query = queries.get("arxiv", deliverable.get("name", ""))
                query_time = time.time() - query_start
                logger.info(f"ğŸ” [Arxiv Deliverable] Academic query built in {query_time:.2f}s: {precise_query}")
            else:
                precise_query = deliverable.get("name", "")
                logger.warning(f"âš ï¸ [Arxiv Deliverable] Query builder not available, using name: {precise_query}")

            # Step 2: æ‰§è¡Œæœç´¢
            sort_by = arxiv.SortCriterion.SubmittedDate if focus_recent else arxiv.SortCriterion.Relevance
            search_count = max_results * 2 if enable_qc else max_results

            logger.debug(
                f"ğŸ” [Arxiv Deliverable] Step 2: Executing search (requesting {search_count} results, sort_by={sort_by})..."
            )
            search_start = time.time()
            search_results = self.search(
                query=precise_query, max_results=search_count, sort_by=sort_by, sort_order=arxiv.SortOrder.Descending
            )
            search_time = time.time() - search_start
            logger.info(
                f"âœ… [Arxiv Deliverable] Search completed in {search_time:.2f}s, success={search_results.get('success', False)}"
            )

            if not search_results.get("success", False):
                logger.error(f"âŒ [Arxiv Deliverable] Search failed: {search_results.get('error', 'Unknown error')}")
                return search_results

            initial_count = len(search_results.get("results", []))
            logger.debug(f"ğŸ“Š [Arxiv Deliverable] Initial papers count: {initial_count}")

            # Step 3: å½’ä¸€åŒ–ç»“æœæ ¼å¼ï¼ˆArxivç»“æœç»“æ„ä¸åŒäºTavilyï¼‰
            logger.debug(f"ğŸ”„ [Arxiv Deliverable] Step 3: Normalizing result format...")
            normalize_start = time.time()
            normalized_results = []
            for result in search_results.get("results", []):
                normalized = {
                    "title": result.get("title", ""),
                    "content": result.get("summary", "")[:500],  # é™åˆ¶æ‘˜è¦é•¿åº¦
                    "snippet": result.get("summary", "")[:300],
                    "url": result.get("pdf_url", ""),
                    "relevance_score": 0.8,  # Arxivæ— ç›¸å…³æ€§åˆ†æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    "published_date": result.get("published", ""),
                    "authors": result.get("authors", []),
                    "arxiv_id": result.get("arxiv_id", ""),
                }
                normalized_results.append(normalized)
            normalize_time = time.time() - normalize_start
            logger.debug(
                f"âš™ï¸ [Arxiv Deliverable] Normalization took {normalize_time:.2f}s, {len(normalized_results)} results"
            )

            # Step 4: è´¨é‡æ§åˆ¶
            if enable_qc and self.qc:
                logger.debug(f"ğŸ”¬ [Arxiv Deliverable] Step 4: Running quality control...")
                qc_start = time.time()
                processed_results = self.qc.process_results(normalized_results, deliverable_context=deliverable)
                qc_time = time.time() - qc_start

                before_limit = len(processed_results)
                processed_results = processed_results[:max_results]
                after_limit = len(processed_results)

                search_results["results"] = processed_results
                search_results["quality_controlled"] = True
                logger.info(
                    f"âœ… [Arxiv Deliverable] QC completed in {qc_time:.2f}s: {initial_count} â†’ {before_limit} â†’ {after_limit} results"
                )
                logger.debug(
                    f"ğŸ“‰ [Arxiv Deliverable] QC pipeline: initial={initial_count}, after_qc={before_limit}, after_limit={after_limit}"
                )
            else:
                search_results["results"] = normalized_results[:max_results]
                search_results["quality_controlled"] = False
                logger.debug(f"â­ï¸ [Arxiv Deliverable] Quality control skipped")

            # Step 5: æ·»åŠ ç¼–å·
            logger.debug(f"ğŸ”¢ [Arxiv Deliverable] Step 5: Adding reference numbers...")
            for idx, result in enumerate(search_results.get("results", []), start=1):
                result["reference_number"] = idx
            logger.debug(f"âœ… [Arxiv Deliverable] Reference numbers added (1-{len(search_results.get('results', []))})")

            end_time = time.time()
            total_time = end_time - start_time
            search_results["execution_time"] = total_time
            search_results["deliverable_name"] = deliverable_name
            search_results["precise_query"] = precise_query

            logger.info(f"ğŸ‰ [Arxiv Deliverable] Search for deliverable completed in {total_time:.2f}s")
            logger.info(
                f"ğŸ“Š [Arxiv Deliverable] Final results: {len(search_results.get('results', []))} papers, QC={search_results.get('quality_controlled', False)}"
            )

            return search_results

        except Exception as e:
            logger.error(f"âŒ [Arxiv Deliverable] search_for_deliverable failed: {str(e)}", exc_info=True)
            logger.error(f"âŒ [Arxiv Deliverable] Failed deliverable: {deliverable.get('name', 'Unknown')}")
            logger.error(f"âŒ [Arxiv Deliverable] Full deliverable data: {json.dumps(deliverable, ensure_ascii=False)}")
            return {
                "success": False,
                "error": str(e),
                "deliverable_name": deliverable.get("name", ""),
                "results": [],
                "execution_time": 0,
            }

    def get_paper_details(self, arxiv_id: str) -> Dict[str, Any]:
        """
        è·å–ç‰¹å®šè®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯

        Args:
            arxiv_id: Arxivè®ºæ–‡ID

        Returns:
            è®ºæ–‡è¯¦ç»†ä¿¡æ¯
        """
        return self.search_by_id([arxiv_id])

    def is_available(self) -> bool:
        """
        æ£€æŸ¥å·¥å…·æ˜¯å¦å¯ç”¨

        Returns:
            æ˜¯å¦å¯ç”¨
        """
        try:
            # ç®€å•æµ‹è¯•æœç´¢
            test_search = arxiv.Search(query="test", max_results=1)
            test_results = list(self.client.results(test_search))
            return True
        except Exception as e:
            logger.error(f"Arxiv tool availability check failed: {str(e)}")
            return False

    def to_langchain_tool(self):
        """
        å°† ArxivSearchTool è½¬æ¢ä¸º LangChain StructuredTool

        Returns:
            StructuredTool instance compatible with bind_tools()
        """
        if not LANGCHAIN_AVAILABLE:
            logger.warning("LangChain not available, returning self")
            return self

        # å®šä¹‰è¾“å…¥schema
        class ArxivSearchInput(BaseModel):
            query: str = Field(description="å­¦æœ¯è®ºæ–‡æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²")

        def arxiv_search_func(query: str) -> str:
            """åœ¨Arxivå­¦æœ¯æ•°æ®åº“ä¸­æœç´¢ç›¸å…³è®ºæ–‡"""
            result = self.search(query)
            if not result.get("success", False):
                return f"æœç´¢å¤±è´¥: {result.get('error', 'Unknown error')}"

            results = result.get("results", [])
            if not results:
                return "æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡"

            # æ ¼å¼åŒ–è¾“å‡º
            output = f"Arxivå­¦æœ¯æœç´¢ç»“æœ (å…³é”®è¯: {query}):\n\n"
            for i, item in enumerate(results, 1):
                output += f"{i}. {item.get('title', 'æ— æ ‡é¢˜')}\n"
                output += f"   ä½œè€…: {', '.join(item.get('authors', []))[:100]}\n"
                output += f"   æ‘˜è¦: {item.get('summary', 'æ— æ‘˜è¦')[:200]}...\n"
                output += f"   å‘å¸ƒæ—¥æœŸ: {item.get('published_date', 'æœªçŸ¥')}\n"
                output += f"   PDF: {item.get('pdf_url', 'æ— ')}\n\n"

            return output

        # ğŸ†• v7.65: æ ¹æ®role_typeæä¾›ç®€åŒ–ç‰ˆ/å®Œæ•´ç‰ˆæè¿°
        role_type = getattr(self, "_current_role_type", None)

        if role_type in ["V4", "V6"]:  # ç ”ç©¶å‹è§’è‰²ä½¿ç”¨å®Œæ•´æè¿°
            description = """Arxivå­¦æœ¯æœç´¢ - è·å–è®¾è®¡ç†è®ºã€äººå› å·¥ç¨‹ã€æŠ€æœ¯æ–¹æ³•è®ºæ–‡çŒ®

é€‚ç”¨åœºæ™¯:
- éœ€è¦å­¦æœ¯ç†è®ºæ”¯æ’‘è®¾è®¡å†³ç­–
- äººå› å·¥ç¨‹ã€ç¯å¢ƒå¿ƒç†å­¦ç­‰ç§‘å­¦ä¾æ®
- æ–°å…´æŠ€æœ¯æ–¹æ³•è®ºï¼ˆAIè®¾è®¡ã€å‚æ•°åŒ–è®¾è®¡ç­‰ï¼‰
- é‡åŒ–è¯„ä¼°æ ‡å‡†å’Œæµ‹é‡æ–¹æ³•

ä¸é€‚ç”¨åœºæ™¯:
- å•†ä¸šæ¡ˆä¾‹å’Œé¡¹ç›®å®ä¾‹ï¼ˆä½¿ç”¨tavily/bochaï¼‰
- å¿«é€Ÿå®æ—¶ä¿¡æ¯ï¼ˆä½¿ç”¨tavilyï¼‰
- éå­¦æœ¯ç±»è®¾è®¡çµæ„Ÿï¼ˆä½¿ç”¨tavilyï¼‰

æŸ¥è¯¢ç¤ºä¾‹: "wayfinding design cognitive load methodology"""
        else:  # å…¶ä»–è§’è‰²ä½¿ç”¨ç®€åŒ–æè¿°
            description = "Arxivå­¦æœ¯æœç´¢ - æä¾›è®¾è®¡ç†è®ºã€äººå› å·¥ç¨‹ã€æŠ€æœ¯æ–¹æ³•è®ºæ–‡çŒ®ï¼ˆéœ€è¦ç§‘å­¦ä¾æ®æ—¶ä½¿ç”¨ï¼‰"

        tool = StructuredTool(
            name=self.name, description=description, func=arxiv_search_func, args_schema=ArxivSearchInput
        )

        return tool
