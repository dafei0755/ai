"""
Search Quality Control (v7.64)

æœç´¢ç»“æœè´¨é‡æ§åˆ¶ç®¡é“ï¼šè¿‡æ»¤ â†’ å»é‡ â†’ å¯ä¿¡åº¦è¯„ä¼° â†’ è´¨é‡è¯„åˆ† â†’ æ’åº
"""

from typing import List, Dict, Any, Optional, Set
from datetime import datetime, timedelta
import re
from loguru import logger
from urllib.parse import urlparse


class SearchQualityControl:
    """
    æœç´¢ç»“æœè´¨é‡æ§åˆ¶å™¨

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. ç›¸å…³æ€§è¿‡æ»¤ - ç§»é™¤ä½ç›¸å…³åº¦ç»“æœ
    2. å†…å®¹å®Œæ•´æ€§æ£€æŸ¥ - ç¡®ä¿å†…å®¹ä¸æ˜¯ç©ºå£³
    3. å»é‡å’Œèšç±» - ç§»é™¤é‡å¤å†…å®¹
    4. æ¥æºå¯ä¿¡åº¦è¯„ä¼° - è¯„ä¼°ä¿¡æ¯æ¥æºå¯é æ€§
    5. ç»¼åˆè´¨é‡è¯„åˆ† - è®¡ç®—å¤šç»´åº¦åŠ æƒåˆ†æ•°
    6. æ’åºå’Œç¼–å· - æŒ‰è´¨é‡åˆ†æ•°æ’åº
    """

    # ğŸ”’ å¯ä¿¡åŸŸåç™½åå•ï¼ˆåˆ†çº§ï¼‰
    TRUSTED_DOMAINS = {
        "high": [
            # å­¦æœ¯æœºæ„
            "arxiv.org", ".edu", ".ac.uk", ".edu.cn",
            # æ”¿åºœ/æ ‡å‡†ç»„ç»‡
            ".gov", ".gov.cn", "iso.org", "w3.org",
            # çŸ¥åè®¾è®¡/æŠ€æœ¯ç«™ç‚¹
            "nngroup.com", "smashingmagazine.com", "a11yproject.com",
            "designbetter.co", "ideo.com", "frogdesign.com"
        ],
        "medium": [
            # ä¸“ä¸šç¤¾åŒº
            "medium.com", "stackoverflow.com", "github.com",
            "dribbble.com", "behance.net", "awwwards.com",
            # è¡Œä¸šåª’ä½“
            "designmilk.com", "dezeen.com", "archdaily.com",
            "interiordesign.net", "architizer.com"
        ],
        "low": [
            # å•†ä¸šå†…å®¹å¹³å°ï¼ˆå¯èƒ½è´¨é‡ä¸ç¨³å®šï¼‰
            "zhihu.com", "jianshu.com", "csdn.net"
        ]
    }

    # âš ï¸ å†…å®¹å®Œæ•´æ€§é˜ˆå€¼
    MIN_CONTENT_LENGTH = 50  # æœ€å°å†…å®¹é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰
    MIN_RELEVANCE_THRESHOLD = 0.6  # æœ€å°ç›¸å…³æ€§åˆ†æ•°ï¼ˆ0-1ï¼‰

    # ğŸ“Š è´¨é‡è¯„åˆ†æƒé‡
    SCORE_WEIGHTS = {
        "relevance": 0.4,      # ç›¸å…³æ€§ 40%
        "timeliness": 0.2,     # æ—¶æ•ˆæ€§ 20%
        "credibility": 0.2,    # å¯ä¿¡åº¦ 20%
        "completeness": 0.2    # å®Œæ•´æ€§ 20%
    }

    def __init__(
        self,
        min_relevance: float = 0.6,
        min_content_length: int = 50,
        enable_deduplication: bool = True
    ):
        """
        åˆå§‹åŒ–è´¨é‡æ§åˆ¶å™¨

        Args:
            min_relevance: æœ€å°ç›¸å…³æ€§é˜ˆå€¼
            min_content_length: æœ€å°å†…å®¹é•¿åº¦
            enable_deduplication: æ˜¯å¦å¯ç”¨å»é‡
        """
        self.min_relevance = min_relevance
        self.min_content_length = min_content_length
        self.enable_deduplication = enable_deduplication

        logger.info(
            f"âœ… SearchQualityControl initialized: "
            f"min_relevance={min_relevance}, "
            f"min_content_length={min_content_length}, "
            f"dedup={enable_deduplication}"
        )

    def process_results(
        self,
        results: List[Dict[str, Any]],
        deliverable_context: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        å¤„ç†æœç´¢ç»“æœï¼ˆå®Œæ•´ç®¡é“ï¼‰

        Pipeline: Filter â†’ Deduplicate â†’ Assess â†’ Score â†’ Sort

        Args:
            results: åŸå§‹æœç´¢ç»“æœåˆ—è¡¨
            deliverable_context: äº¤ä»˜ç‰©ä¸Šä¸‹æ–‡ï¼ˆç”¨äºç›¸å…³æ€§åˆ¤æ–­ï¼‰

        Returns:
            å¤„ç†åçš„ç»“æœåˆ—è¡¨ï¼ˆå·²æ’åºï¼‰
        """
        if not results:
            return []

        logger.info(f"ğŸ”§ Processing {len(results)} search results")

        # Step 1: ç›¸å…³æ€§è¿‡æ»¤
        filtered = self._filter_by_relevance(results)
        logger.debug(f"ğŸ“Œ After relevance filter: {len(filtered)} results")

        # Step 2: å†…å®¹å®Œæ•´æ€§è¿‡æ»¤
        filtered = self._filter_by_completeness(filtered)
        logger.debug(f"ğŸ“Œ After completeness filter: {len(filtered)} results")

        # Step 3: å»é‡
        if self.enable_deduplication:
            unique = self._deduplicate(filtered)
            logger.debug(f"ğŸ“Œ After deduplication: {len(unique)} results")
        else:
            unique = filtered

        # Step 4: å¯ä¿¡åº¦è¯„ä¼° + è´¨é‡è¯„åˆ†
        for result in unique:
            # è¯„ä¼°æ¥æºå¯ä¿¡åº¦
            result["source_credibility"] = self.assess_credibility(
                result.get("url", "")
            )

            # è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°
            result["quality_score"] = self.calculate_composite_score(result)

        # Step 5: æ’åºï¼ˆæŒ‰è´¨é‡åˆ†æ•°é™åºï¼‰
        sorted_results = sorted(
            unique,
            key=lambda x: x.get("quality_score", 0),
            reverse=True
        )

        logger.info(
            f"âœ… Quality control completed: {len(sorted_results)} high-quality results"
        )

        return sorted_results

    def _filter_by_relevance(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æŒ‰ç›¸å…³æ€§åˆ†æ•°è¿‡æ»¤

        Args:
            results: æœç´¢ç»“æœåˆ—è¡¨

        Returns:
            è¿‡æ»¤åçš„ç»“æœåˆ—è¡¨
        """
        filtered = [
            r for r in results
            if r.get("relevance_score", 0) >= self.min_relevance or
               r.get("similarity_score", 0) >= self.min_relevance or
               r.get("score", 0) >= self.min_relevance
        ]

        removed_count = len(results) - len(filtered)
        if removed_count > 0:
            logger.debug(f"âš ï¸ Filtered out {removed_count} low-relevance results")

        return filtered

    def _filter_by_completeness(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æŒ‰å†…å®¹å®Œæ•´æ€§è¿‡æ»¤

        Args:
            results: æœç´¢ç»“æœåˆ—è¡¨

        Returns:
            è¿‡æ»¤åçš„ç»“æœåˆ—è¡¨
        """
        filtered = []
        for r in results:
            content = r.get("content", "") or r.get("snippet", "") or r.get("summary", "")
            if len(content) >= self.min_content_length:
                r["content_complete"] = True
                filtered.append(r)
            else:
                r["content_complete"] = False
                logger.debug(
                    f"âš ï¸ Filtered out incomplete result: '{r.get('title', 'N/A')}' "
                    f"(length={len(content)})"
                )

        return filtered

    def _deduplicate(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å»é‡å’Œèšç±»

        ç­–ç•¥ï¼š
        1. æŒ‰URLå»é‡ï¼ˆå®Œå…¨ç›¸åŒï¼‰
        2. æŒ‰æ ‡é¢˜ç›¸ä¼¼åº¦å»é‡ï¼ˆç¼–è¾‘è·ç¦»ï¼‰
        3. æŒ‰å†…å®¹ç›¸ä¼¼åº¦å»é‡ï¼ˆç®€åŒ–ç‰ˆï¼šå‰100å­—ç¬¦å¯¹æ¯”ï¼‰

        Args:
            results: æœç´¢ç»“æœåˆ—è¡¨

        Returns:
            å»é‡åçš„ç»“æœåˆ—è¡¨
        """
        unique_results = []
        seen_urls: Set[str] = set()
        seen_titles: Set[str] = set()
        seen_content_prefixes: Set[str] = set()

        for result in results:
            # 1. URLå»é‡
            url = result.get("url", "")
            if url and url in seen_urls:
                logger.debug(f"âš ï¸ Duplicate URL: {url}")
                continue

            # 2. æ ‡é¢˜å»é‡ï¼ˆå½’ä¸€åŒ–åå¯¹æ¯”ï¼‰
            title = result.get("title", "")
            normalized_title = self._normalize_text(title)
            if normalized_title and normalized_title in seen_titles:
                logger.debug(f"âš ï¸ Duplicate title: {title}")
                continue

            # 3. å†…å®¹å‰ç¼€å»é‡ï¼ˆç®€åŒ–ç‰ˆç›¸ä¼¼åº¦æ£€æµ‹ï¼‰
            content = result.get("content", "") or result.get("snippet", "")
            content_prefix = self._normalize_text(content[:100])
            if content_prefix and content_prefix in seen_content_prefixes:
                logger.debug(f"âš ï¸ Duplicate content prefix")
                continue

            # é€šè¿‡æ‰€æœ‰å»é‡æ£€æŸ¥ï¼Œæ·»åŠ åˆ°ç»“æœ
            unique_results.append(result)
            if url:
                seen_urls.add(url)
            if normalized_title:
                seen_titles.add(normalized_title)
            if content_prefix:
                seen_content_prefixes.add(content_prefix)

        removed_count = len(results) - len(unique_results)
        if removed_count > 0:
            logger.debug(f"âš ï¸ Removed {removed_count} duplicate results")

        return unique_results

    def _normalize_text(self, text: str) -> str:
        """
        å½’ä¸€åŒ–æ–‡æœ¬ï¼ˆç”¨äºç›¸ä¼¼åº¦å¯¹æ¯”ï¼‰

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            å½’ä¸€åŒ–åçš„æ–‡æœ¬
        """
        if not text:
            return ""

        # è½¬å°å†™ã€å»é™¤å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹
        normalized = text.lower().strip()
        normalized = re.sub(r'\s+', ' ', normalized)
        normalized = re.sub(r'[^\w\s]', '', normalized)

        return normalized

    def assess_credibility(self, url: str) -> str:
        """
        è¯„ä¼°æ¥æºå¯ä¿¡åº¦

        Args:
            url: æ¥æºURL

        Returns:
            å¯ä¿¡åº¦ç­‰çº§: "high" | "medium" | "low" | "unknown"
        """
        if not url:
            return "unknown"

        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()

            # æ£€æŸ¥é«˜å¯ä¿¡åº¦åŸŸå
            for trusted_domain in self.TRUSTED_DOMAINS["high"]:
                if domain.endswith(trusted_domain) or trusted_domain in domain:
                    return "high"

            # æ£€æŸ¥ä¸­ç­‰å¯ä¿¡åº¦åŸŸå
            for medium_domain in self.TRUSTED_DOMAINS["medium"]:
                if domain.endswith(medium_domain) or medium_domain in domain:
                    return "medium"

            # æ£€æŸ¥ä½å¯ä¿¡åº¦åŸŸå
            for low_domain in self.TRUSTED_DOMAINS["low"]:
                if domain.endswith(low_domain) or low_domain in domain:
                    return "low"

            # æœªçŸ¥åŸŸå
            return "unknown"

        except Exception as e:
            logger.warning(f"âš ï¸ Failed to parse URL '{url}': {e}")
            return "unknown"

    def calculate_composite_score(self, result: Dict[str, Any]) -> float:
        """
        è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°

        å…¬å¼:
        Quality Score = Relevance(40%) + Timeliness(20%) + Credibility(20%) + Completeness(20%)

        åˆ†æ•°èŒƒå›´: [30, 100]

        Args:
            result: æœç´¢ç»“æœå­—å…¸

        Returns:
            ç»¼åˆè´¨é‡åˆ†æ•°ï¼ˆ0-100ï¼‰
        """
        # 1. ç›¸å…³æ€§åˆ†æ•°ï¼ˆ0-100ï¼‰
        relevance = (
            result.get("relevance_score") or
            result.get("similarity_score") or
            result.get("score") or
            0.7  # é»˜è®¤ä¸­ç­‰ç›¸å…³
        )
        relevance_score = relevance * 100

        # 2. æ—¶æ•ˆæ€§åˆ†æ•°ï¼ˆ0-100ï¼‰
        timeliness_score = self._calculate_timeliness_score(result)

        # 3. å¯ä¿¡åº¦åˆ†æ•°ï¼ˆ0-100ï¼‰
        credibility = result.get("source_credibility", "unknown")
        credibility_map = {"high": 100, "medium": 70, "low": 50, "unknown": 60}
        credibility_score = credibility_map.get(credibility, 60)

        # 4. å®Œæ•´æ€§åˆ†æ•°ï¼ˆ0-100ï¼‰
        completeness_score = 100 if result.get("content_complete", True) else 50

        # åŠ æƒè®¡ç®—
        composite = (
            relevance_score * self.SCORE_WEIGHTS["relevance"] +
            timeliness_score * self.SCORE_WEIGHTS["timeliness"] +
            credibility_score * self.SCORE_WEIGHTS["credibility"] +
            completeness_score * self.SCORE_WEIGHTS["completeness"]
        )

        # ç¡®ä¿åœ¨åˆç†èŒƒå›´å†…ï¼ˆ30-100ï¼‰
        composite = max(30.0, min(100.0, composite))

        return round(composite, 2)

    def _calculate_timeliness_score(self, result: Dict[str, Any]) -> float:
        """
        è®¡ç®—æ—¶æ•ˆæ€§åˆ†æ•°

        ç­–ç•¥ï¼š
        - æœ€è¿‘1å¹´: 100åˆ†
        - 1-2å¹´: 90åˆ†
        - 2-3å¹´: 80åˆ†
        - 3-5å¹´: 70åˆ†
        - 5å¹´ä»¥ä¸Š: 60åˆ†
        - æ— æ—¥æœŸ: 70åˆ†ï¼ˆä¸­æ€§ï¼‰

        Args:
            result: æœç´¢ç»“æœå­—å…¸

        Returns:
            æ—¶æ•ˆæ€§åˆ†æ•°ï¼ˆ0-100ï¼‰
        """
        # å°è¯•ä»å¤šä¸ªå­—æ®µè·å–å‘å¸ƒæ—¥æœŸ
        date_str = (
            result.get("published_date") or
            result.get("published") or
            result.get("updated") or
            result.get("last_updated")
        )

        if not date_str:
            return 70.0  # æ— æ—¥æœŸï¼Œè¿”å›ä¸­æ€§åˆ†æ•°

        try:
            # è§£ææ—¥æœŸï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
            if isinstance(date_str, str):
                # ISOæ ¼å¼
                pub_date = datetime.fromisoformat(date_str.replace("Z", "+00:00"))
            else:
                pub_date = date_str

            # è®¡ç®—æ—¶é—´å·®
            now = datetime.now(pub_date.tzinfo) if pub_date.tzinfo else datetime.now()
            delta = now - pub_date
            years = delta.days / 365.25

            # åˆ†çº§è¯„åˆ†
            if years < 1:
                return 100.0
            elif years < 2:
                return 90.0
            elif years < 3:
                return 80.0
            elif years < 5:
                return 70.0
            else:
                return 60.0

        except Exception as e:
            logger.debug(f"âš ï¸ Failed to parse date '{date_str}': {e}")
            return 70.0  # è§£æå¤±è´¥ï¼Œè¿”å›ä¸­æ€§åˆ†æ•°


# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šå¿«é€Ÿä½¿ç”¨
# ============================================================================

def quick_quality_control(
    results: List[Dict[str, Any]],
    min_relevance: float = 0.6
) -> List[Dict[str, Any]]:
    """
    å¿«é€Ÿè´¨é‡æ§åˆ¶ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰

    Args:
        results: æœç´¢ç»“æœåˆ—è¡¨
        min_relevance: æœ€å°ç›¸å…³æ€§é˜ˆå€¼

    Returns:
        å¤„ç†åçš„ç»“æœåˆ—è¡¨
    """
    qc = SearchQualityControl(min_relevance=min_relevance)
    return qc.process_results(results)
