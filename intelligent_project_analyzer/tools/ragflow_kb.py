"""
RagflowçŸ¥è¯†åº“å·¥å…·

æä¾›çŸ¥è¯†åº“æŸ¥è¯¢åŠŸèƒ½ï¼Œç”¨äºè·å–å†…éƒ¨çŸ¥è¯†å’Œæ–‡æ¡£
"""

import json
import requests
from typing import Dict, List, Optional, Any, Union
import time
from loguru import logger

from ..core.types import ToolConfig

# ğŸ†• v7.64: å¯¼å…¥ç²¾å‡†æœç´¢å’Œè´¨é‡æ§åˆ¶æ¨¡å—
try:
    from .query_builder import DeliverableQueryBuilder
    from .quality_control import SearchQualityControl
except ImportError:
    logger.warning("âš ï¸ v7.64 modules not available. search_for_deliverable() will use fallback mode.")
    DeliverableQueryBuilder = None
    SearchQualityControl = None


class RagflowKBTool:
    """RagflowçŸ¥è¯†åº“å·¥å…·ç±»"""

    def __init__(self, api_endpoint: str = "", api_key: str = "", dataset_id: str = "", config: Optional[ToolConfig] = None):
        """
        åˆå§‹åŒ–RagflowçŸ¥è¯†åº“å·¥å…·

        Args:
            api_endpoint: Ragflow APIç«¯ç‚¹
            api_key: APIå¯†é’¥
            dataset_id: æ•°æ®é›†ID
            config: å·¥å…·é…ç½®
        """
        self.api_endpoint = (api_endpoint or "http://localhost:9380").rstrip('/')
        self.api_key = api_key or "placeholder_key"
        self.dataset_id = dataset_id or ""
        self.config = config or ToolConfig(name="ragflow_kb")  # âœ… ä¿®å¤ï¼šæ·»åŠ å¿…éœ€çš„nameå‚æ•°

        # å½“å‰ä¸ºå ä½ç¬¦çŠ¶æ€
        self.is_placeholder = not api_endpoint or api_key == "placeholder_key"

        if self.is_placeholder:
            logger.warning("Ragflow KB tool is in placeholder mode. Please configure API endpoint and key.")
        else:
            logger.info(f"Ragflow KB tool initialized with endpoint: {self.api_endpoint}")

        # é»˜è®¤æŸ¥è¯¢å‚æ•°ï¼ˆåŸºäºå®¢æˆ·é…ç½®ï¼‰
        self.default_params = {
            "top_k": 1,
            "similarity_threshold": 0.6,
            "vector_similarity_weight": 0.5,  # å¯¹åº”å®¢æˆ·çš„keyword_weight
            "page_size": 10,
            "rerank_id": "BAAI/bge-reranker-v2-m3"
        }

        # ğŸ†• v7.64: åˆå§‹åŒ–ç²¾å‡†æœç´¢å’Œè´¨é‡æ§åˆ¶æ¨¡å—
        self.query_builder = DeliverableQueryBuilder() if DeliverableQueryBuilder else None
        self.qc = SearchQualityControl() if SearchQualityControl else None
    
    def search_knowledge(
        self,
        query: str,
        knowledge_base_id: Optional[str] = None,
        max_results: Optional[int] = None,
        similarity_threshold: Optional[float] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³å†…å®¹

        Args:
            query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
            knowledge_base_id: çŸ¥è¯†åº“IDï¼ˆæ•°æ®é›†IDï¼‰
            max_results: æœ€å¤§ç»“æœæ•°é‡
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            **kwargs: å…¶ä»–æœç´¢å‚æ•°

        Returns:
            æœç´¢ç»“æœå­—å…¸
        """
        if self.is_placeholder:
            return self._placeholder_search_response(query)

        try:
            start_time = time.time()

            # ä½¿ç”¨æä¾›çš„dataset_idæˆ–é»˜è®¤å€¼
            dataset_id = knowledge_base_id or self.dataset_id
            if not dataset_id:
                logger.warning("No dataset_id provided, using placeholder response")
                return self._placeholder_search_response(query)

            # æ„å»ºè¯·æ±‚å‚æ•°ï¼ˆåŸºäºRAGFlowå®˜æ–¹APIæ–‡æ¡£ï¼‰
            payload = {
                "question": query,
                "dataset_ids": [dataset_id],
                "page": kwargs.get("page", 1),
                "page_size": max_results or kwargs.get("page_size", self.default_params["page_size"]),
                "similarity_threshold": similarity_threshold or kwargs.get("similarity_threshold", self.default_params["similarity_threshold"]),
                "vector_similarity_weight": kwargs.get("vector_similarity_weight", self.default_params["vector_similarity_weight"]),
                "top_k": kwargs.get("top_k", self.default_params["top_k"]),
                "rerank_id": kwargs.get("rerank_id", self.default_params["rerank_id"]),
                "keyword": kwargs.get("keyword", False),
                "highlight": kwargs.get("highlight", False)
            }

            logger.info(f"Executing Ragflow KB search: {query}")
            logger.debug(f"Search payload: {payload}")

            # è°ƒç”¨RAGFlowæ£€ç´¢API
            response = self._make_api_request("/api/v1/retrieval", payload)

            end_time = time.time()
            execution_time = end_time - start_time

            # å¤„ç†å“åº”
            processed_response = self._process_search_response(response, execution_time, query)

            logger.info(f"Ragflow KB search completed in {execution_time:.2f}s, found {processed_response.get('total_results', 0)} results")

            return processed_response

        except Exception as e:
            logger.error(f"Ragflow KB search failed: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "query": query,
                "results": [],
                "execution_time": 0
            }
    
    def get_document(self, document_id: str) -> Dict[str, Any]:
        """
        è·å–ç‰¹å®šæ–‡æ¡£çš„è¯¦ç»†å†…å®¹
        
        Args:
            document_id: æ–‡æ¡£ID
            
        Returns:
            æ–‡æ¡£è¯¦ç»†ä¿¡æ¯
        """
        if self.is_placeholder:
            return self._placeholder_document_response(document_id)
        
        try:
            logger.info(f"Retrieving document: {document_id}")
            
            # TODO: å®é™…çš„APIè°ƒç”¨
            # response = self._make_api_request(f"/documents/{document_id}")
            
            logger.info("Document retrieved successfully")
            
            # ä¸´æ—¶è¿”å›å ä½ç¬¦å“åº”
            return self._placeholder_document_response(document_id)
            
        except Exception as e:
            logger.error(f"Document retrieval failed: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "document_id": document_id
            }
    
    def list_knowledge_bases(self) -> Dict[str, Any]:
        """
        åˆ—å‡ºå¯ç”¨çš„çŸ¥è¯†åº“
        
        Returns:
            çŸ¥è¯†åº“åˆ—è¡¨
        """
        if self.is_placeholder:
            return self._placeholder_kb_list_response()
        
        try:
            logger.info("Listing knowledge bases")
            
            # TODO: å®é™…çš„APIè°ƒç”¨
            # response = self._make_api_request("/knowledge_bases")
            
            logger.info("Knowledge bases listed successfully")
            
            # ä¸´æ—¶è¿”å›å ä½ç¬¦å“åº”
            return self._placeholder_kb_list_response()
            
        except Exception as e:
            logger.error(f"Knowledge base listing failed: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "knowledge_bases": []
            }
    
    def search_for_agent(
        self,
        query: str,
        agent_context: str = "",
        max_results: int = 5,
        similarity_threshold: float = 0.6
    ) -> Dict[str, Any]:
        """
        ä¸ºæ™ºèƒ½ä½“ä¼˜åŒ–çš„æœç´¢æ–¹æ³•
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            agent_context: æ™ºèƒ½ä½“ä¸Šä¸‹æ–‡ä¿¡æ¯
            max_results: æœ€å¤§ç»“æœæ•°
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.6ï¼Œé‡è¯•æ—¶å¯é™ä½åˆ°0.3-0.4ï¼‰
            
        Returns:
            ä¼˜åŒ–åçš„æœç´¢ç»“æœ
        """
        # æ ¹æ®æ™ºèƒ½ä½“ä¸Šä¸‹æ–‡ä¼˜åŒ–æŸ¥è¯¢
        enhanced_query = query
        if agent_context:
            enhanced_query = f"{query} context:{agent_context}"
        
        # æ‰§è¡Œæœç´¢
        results = self.search_knowledge(
            query=enhanced_query,
            max_results=max_results,
            similarity_threshold=similarity_threshold
        )
        
        # ä¸ºæ™ºèƒ½ä½“ä¼˜åŒ–ç»“æœæ ¼å¼
        if results.get("success"):
            # æå–å…³é”®ä¿¡æ¯
            summary = {
                "query": query,
                "relevant_documents": [],
                "key_insights": [],
                "knowledge_sources": []
            }
            
            for result in results.get("results", []):
                summary["relevant_documents"].append({
                    "title": result.get("title", ""),
                    "content": result.get("content", "")[:200] + "..." if len(result.get("content", "")) > 200 else result.get("content", ""),
                    "relevance_score": result.get("similarity_score", 0),
                    "source": result.get("source", "")
                })
                
                summary["knowledge_sources"].append({
                    "title": result.get("title", ""),
                    "source": result.get("source", ""),
                    "last_updated": result.get("last_updated", "")
                })
            
            results["agent_summary"] = summary
        
        return results
    
    def _placeholder_search_response(self, query: str) -> Dict[str, Any]:
        """
        å ä½ç¬¦æœç´¢å“åº”
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            
        Returns:
            å ä½ç¬¦å“åº”
        """
        return {
            "success": True,
            "query": query,
            "results": [
                {
                    "title": "ç¤ºä¾‹çŸ¥è¯†æ–‡æ¡£1",
                    "content": f"è¿™æ˜¯å…³äº'{query}'çš„ç¤ºä¾‹çŸ¥è¯†å†…å®¹ã€‚å½“å‰RagflowçŸ¥è¯†åº“å·¥å…·å¤„äºå ä½ç¬¦æ¨¡å¼ï¼Œç­‰å¾…å®é™…APIé…ç½®ã€‚",
                    "similarity_score": 0.85,
                    "source": "internal_kb",
                    "document_id": "doc_001",
                    "last_updated": "2024-01-01T00:00:00Z",
                    "metadata": {
                        "category": "general",
                        "tags": ["example", "placeholder"]
                    }
                },
                {
                    "title": "ç¤ºä¾‹çŸ¥è¯†æ–‡æ¡£2", 
                    "content": f"è¿™æ˜¯å¦ä¸€ä¸ªå…³äº'{query}'çš„ç¤ºä¾‹çŸ¥è¯†å†…å®¹ã€‚å®é™…éƒ¨ç½²æ—¶å°†è¿æ¥åˆ°çœŸå®çš„RagflowçŸ¥è¯†åº“ã€‚",
                    "similarity_score": 0.78,
                    "source": "internal_kb",
                    "document_id": "doc_002",
                    "last_updated": "2024-01-01T00:00:00Z",
                    "metadata": {
                        "category": "technical",
                        "tags": ["example", "placeholder"]
                    }
                }
            ],
            "execution_time": 0.1,
            "total_results": 2,
            "is_placeholder": True
        }
    
    def _placeholder_document_response(self, document_id: str) -> Dict[str, Any]:
        """
        å ä½ç¬¦æ–‡æ¡£å“åº”
        
        Args:
            document_id: æ–‡æ¡£ID
            
        Returns:
            å ä½ç¬¦å“åº”
        """
        return {
            "success": True,
            "document_id": document_id,
            "title": f"ç¤ºä¾‹æ–‡æ¡£ {document_id}",
            "content": f"è¿™æ˜¯æ–‡æ¡£ {document_id} çš„å®Œæ•´å†…å®¹ã€‚å½“å‰ä¸ºå ä½ç¬¦æ¨¡å¼ã€‚",
            "metadata": {
                "category": "example",
                "tags": ["placeholder"],
                "created_at": "2024-01-01T00:00:00Z",
                "updated_at": "2024-01-01T00:00:00Z",
                "author": "system"
            },
            "is_placeholder": True
        }
    
    def _placeholder_kb_list_response(self) -> Dict[str, Any]:
        """
        å ä½ç¬¦çŸ¥è¯†åº“åˆ—è¡¨å“åº”
        
        Returns:
            å ä½ç¬¦å“åº”
        """
        return {
            "success": True,
            "knowledge_bases": [
                {
                    "id": "kb_001",
                    "name": "æŠ€æœ¯æ–‡æ¡£åº“",
                    "description": "åŒ…å«æŠ€æœ¯è§„èŒƒå’Œæœ€ä½³å®è·µ",
                    "document_count": 150,
                    "last_updated": "2024-01-01T00:00:00Z"
                },
                {
                    "id": "kb_002", 
                    "name": "ä¸šåŠ¡çŸ¥è¯†åº“",
                    "description": "åŒ…å«ä¸šåŠ¡æµç¨‹å’Œè§„åˆ™",
                    "document_count": 89,
                    "last_updated": "2024-01-01T00:00:00Z"
                }
            ],
            "total_count": 2,
            "is_placeholder": True
        }

    def search_for_deliverable(
        self,
        deliverable: Dict[str, Any],
        project_type: str = "",
        max_results: int = 10,
        enable_qc: bool = True,
        similarity_threshold: float = 0.6
    ) -> Dict[str, Any]:
        """
        ğŸ†• v7.64: é’ˆå¯¹äº¤ä»˜ç‰©çš„ç²¾å‡†çŸ¥è¯†åº“æœç´¢

        æ ¸å¿ƒæ”¹è¿›ï¼š
        1. ä»äº¤ä»˜ç‰©æ„å»ºä¸­æ–‡ä¼˜åŒ–æŸ¥è¯¢ï¼ˆå†…éƒ¨çŸ¥è¯†åº“ä»¥ä¸­æ–‡ä¸ºä¸»ï¼‰
        2. åº”ç”¨è´¨é‡æ§åˆ¶ç®¡é“
        3. æŒ‰è´¨é‡åˆ†æ•°æ’åº

        Args:
            deliverable: äº¤ä»˜ç‰©å­—å…¸
            project_type: é¡¹ç›®ç±»å‹
            max_results: æœ€å¤§è¿”å›ç»“æœæ•°
            enable_qc: æ˜¯å¦å¯ç”¨è´¨é‡æ§åˆ¶
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            æœç´¢ç»“æœå­—å…¸
        """
        try:
            start_time = time.time()

            # Step 1: æ„å»ºæŸ¥è¯¢ï¼ˆå†…éƒ¨çŸ¥è¯†åº“ä¼˜å…ˆä½¿ç”¨ä¸­æ–‡åŸæ–‡ï¼‰
            if self.query_builder:
                queries = self.query_builder.build_multi_tool_queries(
                    deliverable,
                    project_type
                )
                precise_query = queries.get("ragflow", deliverable.get("name", ""))
                logger.info(f"ğŸ” [v7.64 RAGFlow] Precise query: {precise_query}")
            else:
                # Fallback: ä½¿ç”¨äº¤ä»˜ç‰©åç§°+æè¿°å‰50å­—
                name = deliverable.get("name", "")
                desc = deliverable.get("description", "")
                precise_query = f"{name} {desc[:50]}" if desc else name
                logger.warning("âš ï¸ Query builder not available, using name+description")

            # Step 2: æ‰§è¡Œæœç´¢
            search_results = self.search_knowledge(
                query=precise_query,
                max_results=max_results * 2 if enable_qc else max_results,
                similarity_threshold=similarity_threshold
            )

            if not search_results.get("success", False):
                return search_results

            # Step 3: å½’ä¸€åŒ–ç»“æœæ ¼å¼ï¼ˆRAGFlowç»“æœç»“æ„ï¼‰
            normalized_results = []
            for result in search_results.get("results", []):
                normalized = {
                    "title": result.get("title", ""),
                    "content": result.get("content", "")[:500],
                    "snippet": result.get("content", "")[:300],
                    "url": None,  # å†…éƒ¨çŸ¥è¯†åº“é€šå¸¸æ— URL
                    "relevance_score": result.get("similarity_score", 0.7),
                    "vector_similarity": result.get("vector_similarity", 0),
                    "term_similarity": result.get("term_similarity", 0),
                    "document_id": result.get("document_id", ""),
                    "kb_id": result.get("kb_id", "")
                }
                normalized_results.append(normalized)

            # Step 4: è´¨é‡æ§åˆ¶
            if enable_qc and self.qc:
                processed_results = self.qc.process_results(
                    normalized_results,
                    deliverable_context=deliverable
                )
                processed_results = processed_results[:max_results]
                search_results["results"] = processed_results
                search_results["quality_controlled"] = True
                logger.info(
                    f"âœ… [v7.64 RAGFlow] Quality control: {len(processed_results)} results"
                )
            else:
                search_results["results"] = normalized_results[:max_results]
                search_results["quality_controlled"] = False

            # Step 5: æ·»åŠ ç¼–å·
            for idx, result in enumerate(search_results.get("results", []), start=1):
                result["reference_number"] = idx

            end_time = time.time()
            search_results["execution_time"] = end_time - start_time
            search_results["deliverable_name"] = deliverable.get("name", "")
            search_results["precise_query"] = precise_query

            return search_results

        except Exception as e:
            logger.error(f"âŒ [v7.64 RAGFlow] search_for_deliverable failed: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "deliverable_name": deliverable.get("name", ""),
                "results": [],
                "execution_time": 0
            }

    def is_available(self) -> bool:
        """
        æ£€æŸ¥å·¥å…·æ˜¯å¦å¯ç”¨
        
        Returns:
            æ˜¯å¦å¯ç”¨
        """
        if self.is_placeholder:
            logger.info("Ragflow KB tool is in placeholder mode")
            return True  # å ä½ç¬¦æ¨¡å¼ä¸‹è®¤ä¸ºå¯ç”¨
        
        try:
            # TODO: å®é™…çš„å¥åº·æ£€æŸ¥
            # response = self._make_api_request("/health")
            return True
        except Exception as e:
            logger.error(f"Ragflow KB tool availability check failed: {str(e)}")
            return False
    
    def configure_api(self, api_endpoint: str, api_key: str, dataset_id: str = ""):
        """
        é…ç½®APIè¿æ¥ä¿¡æ¯

        Args:
            api_endpoint: APIç«¯ç‚¹
            api_key: APIå¯†é’¥
            dataset_id: æ•°æ®é›†ID
        """
        self.api_endpoint = api_endpoint.rstrip('/')
        self.api_key = api_key
        if dataset_id:
            self.dataset_id = dataset_id
        self.is_placeholder = False
        logger.info(f"Ragflow KB tool configured with endpoint: {api_endpoint}")

    def _make_api_request(self, endpoint: str, data: Optional[Dict] = None) -> Dict[str, Any]:
        """
        æ‰§è¡ŒAPIè¯·æ±‚

        Args:
            endpoint: APIç«¯ç‚¹è·¯å¾„
            data: è¯·æ±‚æ•°æ®

        Returns:
            APIå“åº”
        """
        url = f"{self.api_endpoint}{endpoint}"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }

        try:
            logger.debug(f"Making API request to: {url}")

            response = requests.post(
                url,
                headers=headers,
                json=data,
                timeout=self.config.timeout if hasattr(self.config, 'timeout') else 30
            )

            response.raise_for_status()

            result = response.json()
            logger.debug(f"API response code: {result.get('code')}")

            return result

        except requests.exceptions.Timeout:
            logger.error(f"API request timeout: {url}")
            raise Exception("RAGFlow API request timeout")
        except requests.exceptions.RequestException as e:
            logger.error(f"API request failed: {str(e)}")
            raise Exception(f"RAGFlow API request failed: {str(e)}")
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse API response: {str(e)}")
            raise Exception("Invalid JSON response from RAGFlow API")

    def _process_search_response(self, response: Dict[str, Any], execution_time: float, query: str) -> Dict[str, Any]:
        """
        å¤„ç†æœç´¢å“åº”

        Args:
            response: APIå“åº”
            execution_time: æ‰§è¡Œæ—¶é—´
            query: æŸ¥è¯¢å­—ç¬¦ä¸²

        Returns:
            å¤„ç†åçš„å“åº”
        """
        # æ£€æŸ¥å“åº”çŠ¶æ€
        if response.get("code") != 0:
            error_msg = response.get("message", "Unknown error")
            logger.error(f"RAGFlow API error: {error_msg}")
            return {
                "success": False,
                "error": error_msg,
                "query": query,
                "results": [],
                "execution_time": execution_time
            }

        # æå–æ•°æ®
        data = response.get("data", {})
        chunks = data.get("chunks", [])
        total = data.get("total", 0)

        # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
        results = []
        for chunk in chunks:
            results.append({
                "title": chunk.get("document_keyword", ""),
                "content": chunk.get("content", ""),
                "similarity_score": chunk.get("similarity", 0.0),
                "vector_similarity": chunk.get("vector_similarity", 0.0),
                "term_similarity": chunk.get("term_similarity", 0.0),
                "source": "ragflow",
                "document_id": chunk.get("document_id", ""),
                "chunk_id": chunk.get("id", ""),
                "kb_id": chunk.get("kb_id", ""),
                "highlight": chunk.get("highlight", ""),
                "metadata": {
                    "image_id": chunk.get("image_id", ""),
                    "important_keywords": chunk.get("important_keywords", []),
                    "positions": chunk.get("positions", [])
                }
            })

        return {
            "success": True,
            "query": query,
            "results": results,
            "execution_time": execution_time,
            "total_results": total,
            "is_placeholder": False
        }
