"""
ç›‘æ§ç³»ç»Ÿ (v7.119+)

æ”¶é›†å’Œä¸ŠæŠ¥æ€§èƒ½æŒ‡æ ‡ã€æ…¢æŸ¥è¯¢ã€é”™è¯¯ç»Ÿè®¡ç­‰ç›‘æ§æ•°æ®
"""

import os
import time
from collections import defaultdict, deque
from datetime import datetime, timedelta
from threading import Lock
from typing import Any, Dict, List, Optional

from loguru import logger


class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(self):
        self._metrics = defaultdict(list)
        self._lock = Lock()
        self._enable_monitoring = os.getenv("ENABLE_MONITORING", "true").lower() == "true"

        # æ…¢æŸ¥è¯¢é˜ˆå€¼
        self.slow_query_threshold = float(os.getenv("SLOW_QUERY_THRESHOLD", "3.0"))

        # æ»‘åŠ¨çª—å£ç»Ÿè®¡ï¼ˆæœ€è¿‘1å°æ—¶ï¼‰
        self._recent_metrics = defaultdict(lambda: deque(maxlen=1000))

    def record_search(
        self, tool: str, operation: str, execution_time: float, success: bool, result_count: int = 0, **extra_metrics
    ):
        """
        è®°å½•æœç´¢æ“ä½œæŒ‡æ ‡

        Args:
            tool: å·¥å…·åç§° ("tavily", "arxiv", "ragflow", "bocha")
            operation: æ“ä½œç±»å‹ ("search", "search_for_deliverable")
            execution_time: æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
            success: æ˜¯å¦æˆåŠŸ
            result_count: ç»“æœæ•°é‡
            **extra_metrics: é¢å¤–æŒ‡æ ‡
        """
        if not self._enable_monitoring:
            return

        metric = {
            "tool": tool,
            "operation": operation,
            "execution_time": execution_time,
            "success": success,
            "result_count": result_count,
            "timestamp": datetime.now().isoformat(),
            **extra_metrics,
        }

        with self._lock:
            # æŒä¹…åŒ–æŒ‡æ ‡
            self._metrics[f"{tool}.{operation}"].append(metric)

            # æ»‘åŠ¨çª—å£æŒ‡æ ‡
            self._recent_metrics[f"{tool}.{operation}"].append(metric)

        # æ£€æŸ¥æ…¢æŸ¥è¯¢
        if execution_time > self.slow_query_threshold:
            self._report_slow_query(tool, operation, execution_time, metric)

    def record_error(self, tool: str, operation: str, error_type: str, error_message: str, **context):
        """
        è®°å½•é”™è¯¯

        Args:
            tool: å·¥å…·åç§°
            operation: æ“ä½œç±»å‹
            error_type: é”™è¯¯ç±»å‹
            error_message: é”™è¯¯ä¿¡æ¯
            **context: é¢å¤–ä¸Šä¸‹æ–‡
        """
        if not self._enable_monitoring:
            return

        error_metric = {
            "tool": tool,
            "operation": operation,
            "error_type": error_type,
            "error_message": error_message,
            "timestamp": datetime.now().isoformat(),
            **context,
        }

        with self._lock:
            self._metrics[f"{tool}.errors"].append(error_metric)

    def get_statistics(self, tool: Optional[str] = None, window_minutes: int = 60) -> Dict[str, Any]:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯

        Args:
            tool: å·¥å…·åç§°ï¼ˆNoneè¡¨ç¤ºæ‰€æœ‰å·¥å…·ï¼‰
            window_minutes: ç»Ÿè®¡çª—å£ï¼ˆåˆ†é’Ÿï¼‰

        Returns:
            ç»Ÿè®¡æ•°æ®
        """
        cutoff_time = datetime.now() - timedelta(minutes=window_minutes)
        stats = {}

        with self._lock:
            for key, metrics in self._metrics.items():
                # è¿‡æ»¤æ—¶é—´çª—å£
                recent_metrics = [m for m in metrics if datetime.fromisoformat(m["timestamp"]) > cutoff_time]

                if not recent_metrics:
                    continue

                # æŒ‰å·¥å…·è¿‡æ»¤
                if tool and not key.startswith(tool):
                    continue

                # è®¡ç®—ç»Ÿè®¡
                execution_times = [m["execution_time"] for m in recent_metrics if "execution_time" in m]
                successes = [m for m in recent_metrics if m.get("success", False)]

                stats[key] = {
                    "total_requests": len(recent_metrics),
                    "successful_requests": len(successes),
                    "failed_requests": len(recent_metrics) - len(successes),
                    "success_rate": len(successes) / len(recent_metrics) if recent_metrics else 0,
                    "avg_execution_time": sum(execution_times) / len(execution_times) if execution_times else 0,
                    "min_execution_time": min(execution_times) if execution_times else 0,
                    "max_execution_time": max(execution_times) if execution_times else 0,
                    "p95_execution_time": self._calculate_percentile(execution_times, 0.95),
                    "p99_execution_time": self._calculate_percentile(execution_times, 0.99),
                }

        return stats

    def _calculate_percentile(self, values: List[float], percentile: float) -> float:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        if not values:
            return 0
        sorted_values = sorted(values)
        index = int(len(sorted_values) * percentile)
        return sorted_values[min(index, len(sorted_values) - 1)]

    def _report_slow_query(self, tool: str, operation: str, execution_time: float, metric: Dict):
        """ä¸ŠæŠ¥æ…¢æŸ¥è¯¢"""
        logger.warning(
            f"ğŸŒ Slow query detected",
            tool=tool,
            operation=operation,
            execution_time=execution_time,
            threshold=self.slow_query_threshold,
            query=metric.get("query", ""),
            result_count=metric.get("result_count", 0),
        )


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨ï¼ˆä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰"""

    def __init__(self, tool: str, operation: str, metrics_collector: Optional[MetricsCollector] = None, **context):
        """
        Args:
            tool: å·¥å…·åç§°
            operation: æ“ä½œåç§°
            metrics_collector: æŒ‡æ ‡æ”¶é›†å™¨å®ä¾‹
            **context: é¢å¤–ä¸Šä¸‹æ–‡
        """
        self.tool = tool
        self.operation = operation
        self.context = context
        self.metrics_collector = metrics_collector or global_metrics_collector
        self.start_time = None
        self.success = False
        self.result_count = 0
        self.extra_metrics = {}

    def __enter__(self):
        """è¿›å…¥ç›‘æ§ä¸Šä¸‹æ–‡"""
        self.start_time = time.time()
        logger.debug(f"ğŸ“Š [Monitor] Starting {self.tool}.{self.operation}")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """é€€å‡ºç›‘æ§ä¸Šä¸‹æ–‡"""
        execution_time = time.time() - self.start_time

        # åˆ¤æ–­æ˜¯å¦æˆåŠŸ
        self.success = exc_type is None

        # è®°å½•æŒ‡æ ‡
        self.metrics_collector.record_search(
            tool=self.tool,
            operation=self.operation,
            execution_time=execution_time,
            success=self.success,
            result_count=self.result_count,
            **{**self.context, **self.extra_metrics},
        )

        # è®°å½•é”™è¯¯
        if not self.success:
            self.metrics_collector.record_error(
                tool=self.tool,
                operation=self.operation,
                error_type=exc_type.__name__ if exc_type else "Unknown",
                error_message=str(exc_val) if exc_val else "",
                **self.context,
            )

        logger.debug(
            f"ğŸ“Š [Monitor] Completed {self.tool}.{self.operation}: "
            f"success={self.success}, time={execution_time:.2f}s, results={self.result_count}"
        )

    def set_result_count(self, count: int):
        """è®¾ç½®ç»“æœæ•°é‡"""
        self.result_count = count

    def add_metric(self, key: str, value: Any):
        """æ·»åŠ é¢å¤–æŒ‡æ ‡"""
        self.extra_metrics[key] = value


class HealthCheck:
    """å¥åº·æ£€æŸ¥"""

    def __init__(self, metrics_collector: Optional[MetricsCollector] = None):
        self.metrics_collector = metrics_collector or global_metrics_collector

    def check_health(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œå¥åº·æ£€æŸ¥

        Returns:
            å¥åº·çŠ¶æ€
        """
        stats = self.metrics_collector.get_statistics(window_minutes=5)

        # è®¡ç®—æ•´ä½“å¥åº·çŠ¶æ€
        overall_health = "healthy"
        warnings = []
        errors = []

        for tool_op, metrics in stats.items():
            # æ£€æŸ¥æˆåŠŸç‡
            if metrics["success_rate"] < 0.9:  # 90%æˆåŠŸç‡é˜ˆå€¼
                overall_health = "degraded"
                warnings.append(f"{tool_op}: low success rate ({metrics['success_rate']:.1%})")

            if metrics["success_rate"] < 0.5:  # 50%æˆåŠŸç‡é˜ˆå€¼
                overall_health = "unhealthy"
                errors.append(f"{tool_op}: critical success rate ({metrics['success_rate']:.1%})")

            # æ£€æŸ¥å“åº”æ—¶é—´
            if metrics["p95_execution_time"] > 10.0:  # 10ç§’P95é˜ˆå€¼
                warnings.append(f"{tool_op}: slow response (P95={metrics['p95_execution_time']:.1f}s)")

        return {
            "status": overall_health,
            "timestamp": datetime.now().isoformat(),
            "warnings": warnings,
            "errors": errors,
            "statistics": stats,
        }


# å…¨å±€æŒ‡æ ‡æ”¶é›†å™¨
global_metrics_collector = MetricsCollector()
