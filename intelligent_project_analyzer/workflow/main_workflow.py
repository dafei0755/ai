"""
ä¸»å·¥ä½œæµç¼–æ’å™¨

åŸºäºLangGraphå®ç°çš„å¤šæ™ºèƒ½ä½“åä½œå·¥ä½œæµ
"""

import uuid
from typing import Dict, List, Optional, Any, Literal, Union
from datetime import datetime
from pathlib import Path
from loguru import logger
import yaml

from langgraph.graph import StateGraph, START, END
from langgraph.store.memory import InMemoryStore
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Send, Command

from ..core.state import ProjectAnalysisState, AnalysisStage, StateManager
from ..core.types import AgentType, format_role_display_name
# æ˜¾å¼å¯¼å…¥æ™ºèƒ½ä½“ç±»ä»¥è§¦å‘ AgentFactory æ³¨å†Œ
from ..agents import AgentFactory, RequirementsAnalystAgent, ProjectDirectorAgent
from ..agents.feasibility_analyst import FeasibilityAnalystAgent  # ğŸ†• V1.5å¯è¡Œæ€§åˆ†æå¸ˆ
from ..agents.base import NullLLM
from ..interaction.interaction_nodes import (
    CalibrationQuestionnaireNode,
    RequirementsConfirmationNode,
    AnalysisReviewNode,
    # FinalReviewNode,  # å·²ç§»é™¤ï¼šå®¢æˆ·éœ€æ±‚ä¸­æ²¡æœ‰æœ€ç»ˆå®¡æ ¸é˜¶æ®µ
    UserQuestionNode
)
# ğŸ†• ç»Ÿä¸€å®¡æ ¸èŠ‚ç‚¹ï¼ˆåˆå¹¶è§’è‰²é€‰æ‹©å’Œä»»åŠ¡åˆ†æ´¾å®¡æ ¸ï¼‰
from ..interaction.role_task_unified_review import role_task_unified_review_node
# from ..interaction.role_selection_review import role_selection_review_node  # å·²åºŸå¼ƒ
# from ..interaction.task_assignment_review import task_assignment_review_node  # å·²åºŸå¼ƒ
from ..interaction.second_batch_strategy_review import SecondBatchStrategyReviewNode
from ..interaction.nodes.quality_preflight import QualityPreflightNode  # ğŸ†•
from ..agents.quality_monitor import QualityMonitor  # ğŸ†•
from ..interaction.nodes.manual_review import ManualReviewNode  # ğŸ†• äººå·¥å®¡æ ¸èŠ‚ç‚¹
from ..agents.dynamic_project_director import detect_and_handle_challenges_node  # ğŸ†• v3.5
from ..report.result_aggregator import ResultAggregatorAgent
from ..report.pdf_generator import PDFGeneratorAgent
from ..security import (  # ğŸ†• å†…å®¹å®‰å…¨ä¸é¢†åŸŸè¿‡æ»¤
    ReportGuardNode
)
# ğŸ†• v7.3 ç»Ÿä¸€è¾“å…¥éªŒè¯èŠ‚ç‚¹ï¼ˆåˆå¹¶ input_guard å’Œ domain_validatorï¼‰
from ..security.unified_input_validator_node import (
    UnifiedInputValidatorNode,
    InputRejectedNode
)
# åŠ¨æ€æœ¬ä½“è®ºæ³¨å…¥å·¥å…·
from ..utils.ontology_loader import OntologyLoader


class MainWorkflow:
    """ä¸»å·¥ä½œæµç¼–æ’å™¨"""
    
    def __init__(self, llm_model: Optional[Any] = None, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–ä¸»å·¥ä½œæµ
        
        Args:
            llm_model: LLMæ¨¡å‹å®ä¾‹
            config: é…ç½®å‚æ•°
        """
        if llm_model is None:
            llm_model = NullLLM("MainWorkflow")
        self.llm_model = llm_model
        self.config = config or {}
        if isinstance(self.llm_model, NullLLM):
            self.config.setdefault("llm_placeholder", True)
        # å¯ç”¨å®Œæˆåè¿½é—®äº¤äº’ï¼ˆå¯é€šè¿‡é…ç½®è¦†ç›–ï¼‰
        self.config.setdefault("post_completion_followup_enabled", True)
        
        # åˆå§‹åŒ–å­˜å‚¨å’Œæ£€æŸ¥ç‚¹
        self.store = InMemoryStore()
        self.checkpointer = MemorySaver()
        
        # åˆå§‹åŒ–æœ¬ä½“è®ºåŠ è½½å™¨
        self.ontology_loader = OntologyLoader(
            "d:/11-20/langgraph-design/intelligent_project_analyzer/knowledge_base/ontology.yaml"
        )

        # æ„å»ºå·¥ä½œæµå›¾
        self.graph = self._build_workflow_graph()
        
        logger.info("Main workflow initialized successfully")
    
    def _build_workflow_graph(self) -> StateGraph:
        """
        æ„å»ºå·¥ä½œæµå›¾ï¼ˆ2025-11-19é‡æ„ï¼šæ”¯æŒåŠ¨æ€Næ‰¹æ¬¡æ‰§è¡Œï¼‰

        é‡è¦å˜æ›´:
        - ç§»é™¤ç¡¬ç¼–ç çš„ first_batch_agent å’Œ second_batch_agent
        - æ·»åŠ åŠ¨æ€æ‰¹æ¬¡èŠ‚ç‚¹ï¼šbatch_executor, agent_executor, batch_aggregator, batch_router
        - å®ç°å¾ªç¯æ‰§è¡Œï¼šbatch_executor â†’ agent_executor â†’ batch_aggregator â†’ batch_router
        - æ”¯æŒ 1-N ä¸ªæ‰¹æ¬¡çš„åŠ¨æ€æ‰§è¡Œ
        """
        # åˆ›å»ºçŠ¶æ€å›¾
        workflow = StateGraph(ProjectAnalysisState)

        # ============================================================================
        # 0. ğŸ†• å®‰å…¨èŠ‚ç‚¹ï¼ˆç¬¬ä¸€é“é˜²çº¿ï¼‰
        # ============================================================================
        # ğŸ†• v7.3 ç»Ÿä¸€è¾“å…¥éªŒè¯èŠ‚ç‚¹ï¼ˆåˆå¹¶ input_guard å’Œ domain_validatorï¼‰
        workflow.add_node("unified_input_validator_initial", self._unified_input_validator_initial_node)  # åˆå§‹éªŒè¯
        workflow.add_node("unified_input_validator_secondary", self._unified_input_validator_secondary_node)  # äºŒæ¬¡éªŒè¯
        workflow.add_node("input_rejected", self._input_rejected_node)        # æ‹’ç»ç»ˆæ­¢
        workflow.add_node("report_guard", self._report_guard_node)            # æŠ¥å‘Šå®¡æ ¸

        # ============================================================================
        # 1. å‰ç½®æµç¨‹èŠ‚ç‚¹ï¼ˆéœ€æ±‚æ”¶é›†ä¸ç¡®è®¤ï¼‰
        # ============================================================================
        workflow.add_node("requirements_analyst", self._requirements_analyst_node)
        workflow.add_node("feasibility_analyst", self._feasibility_analyst_node)  # ğŸ†• V1.5å¯è¡Œæ€§åˆ†æå¸ˆ
        workflow.add_node("calibration_questionnaire", self._calibration_questionnaire_node)
        workflow.add_node("requirements_confirmation", self._requirements_confirmation_node)

        # ============================================================================
        # 2. è§’è‰²é€‰æ‹©ä¸ä»»åŠ¡åˆ†æ´¾èŠ‚ç‚¹
        # ============================================================================
        workflow.add_node("project_director", self._project_director_node)
        # ğŸ†• ç»Ÿä¸€å®¡æ ¸èŠ‚ç‚¹ï¼ˆåˆå¹¶è§’è‰²é€‰æ‹©å’Œä»»åŠ¡åˆ†æ´¾ï¼‰
        workflow.add_node("role_task_unified_review", self._role_task_unified_review_node)
        workflow.add_node("quality_preflight", self._quality_preflight_node)  # ğŸ†• è´¨é‡é¢„æ£€

        # ============================================================================
        # 3. ğŸ†• åŠ¨æ€æ‰¹æ¬¡æ‰§è¡ŒèŠ‚ç‚¹ï¼ˆæ ¸å¿ƒé‡æ„ï¼‰
        # ============================================================================
        workflow.add_node("batch_executor", self._batch_executor_node)           # æ‰¹æ¬¡æ‰§è¡Œå™¨
        workflow.add_node("agent_executor", self._execute_agent_node)            # æ™ºèƒ½ä½“æ‰§è¡Œå™¨
        workflow.add_node("batch_aggregator", self._intermediate_aggregator_node) # æ‰¹æ¬¡èšåˆå™¨
        workflow.add_node("batch_router", self._batch_router_node)               # æ‰¹æ¬¡è·¯ç”±å™¨
        workflow.add_node("batch_strategy_review", self._batch_strategy_review_node)  # æ‰¹æ¬¡ç­–ç•¥å®¡æ ¸
        workflow.add_node("detect_challenges", self._detect_challenges_node)     # ğŸ†• v3.5 æŒ‘æˆ˜æ£€æµ‹

        # ============================================================================
        # 4. å®¡æ ¸ä¸ç»“æœç”ŸæˆèŠ‚ç‚¹
        # ============================================================================
        workflow.add_node("analysis_review", self._analysis_review_node)
        workflow.add_node("manual_review", self._manual_review_node)  # ğŸ†• äººå·¥å®¡æ ¸èŠ‚ç‚¹
        workflow.add_node("result_aggregator", self._result_aggregator_node)
        workflow.add_node("pdf_generator", self._pdf_generator_node)
        workflow.add_node("user_question", self._user_question_node)

        # ============================================================================
        # è¾¹è¿æ¥ï¼šå‰ç½®æµç¨‹
        # ============================================================================
        # ğŸ†• v7.3 ç»Ÿä¸€è¾“å…¥éªŒè¯æµç¨‹
        workflow.add_edge(START, "unified_input_validator_initial")  # ç¬¬ä¸€é“é˜²çº¿ï¼šåˆå§‹éªŒè¯
        # unified_input_validator_initial ä½¿ç”¨ Command è·¯ç”±åˆ° requirements_analyst æˆ– input_rejected
        workflow.add_edge("input_rejected", END)  # æ‹’ç»åç»ˆæ­¢

        workflow.add_edge("requirements_analyst", "feasibility_analyst")  # ğŸ†• V1.5å¯è¡Œæ€§åˆ†æ
        workflow.add_edge("feasibility_analyst", "unified_input_validator_secondary")  # ç¬¬äºŒé“é˜²çº¿ï¼šäºŒæ¬¡éªŒè¯
        workflow.add_edge("unified_input_validator_secondary", "calibration_questionnaire")  # éªŒè¯åç»§ç»­
        # âœ… calibration_questionnaire ä½¿ç”¨ Command å®Œå…¨åŠ¨æ€è·¯ç”±ï¼ˆæ— é™æ€ edgeï¼‰
        # âœ… requirements_confirmation ä½¿ç”¨ Command åŠ¨æ€è·¯ç”±åˆ° requirements_analyst æˆ– project_director

        workflow.add_edge("project_director", "role_task_unified_review")  # ğŸ†• ç»Ÿä¸€å®¡æ ¸
        # âŒ ç§»é™¤é™æ€è¾¹ï¼Œè®© role_task_unified_review ä½¿ç”¨ Command åŠ¨æ€è·¯ç”±
        # workflow.add_edge("role_task_unified_review", "quality_preflight")  # ğŸ†• è´¨é‡é¢„æ£€
        workflow.add_edge("quality_preflight", "batch_executor")  # ğŸ†• é¢„æ£€åæ‰§è¡Œ
        # role_task_unified_review ä½¿ç”¨ Command è·¯ç”±åˆ° batch_executor æˆ– quality_preflight

        # ============================================================================
        # ğŸ†• åŠ¨æ€æ‰¹æ¬¡æ‰§è¡Œæµç¨‹ï¼ˆæ ¸å¿ƒå¾ªç¯ï¼‰
        # ============================================================================
        # æ‰¹æ¬¡æ‰§è¡Œå™¨ â†’ æ™ºèƒ½ä½“æ‰§è¡Œå™¨ï¼ˆå¹¶è¡Œï¼‰â†’ æ‰¹æ¬¡èšåˆå™¨ â†’ æ‰¹æ¬¡è·¯ç”±å™¨
        workflow.add_edge("agent_executor", "batch_aggregator")
        # workflow.add_edge("batch_aggregator", "detect_challenges")  # âŒ æš‚æ—¶ç¦ç”¨ï¼šå¯¼è‡´çŠ¶æ€å†²çª
        
        # ğŸ†• v3.5: æŒ‘æˆ˜æ£€æµ‹ç§»è‡³ result_aggregator ä¹‹å‰
        # batch_aggregator â†’ batch_router â†’ analysis_review â†’ detect_challenges â†’ result_aggregator
        # workflow.add_conditional_edges(
        #     "detect_challenges",
        #     self._route_after_challenge_detection,
        #     {
        #         "revisit_requirements": "requirements_analyst",  # åé¦ˆå¾ªç¯
        #         "continue_workflow": "batch_router"  # ç»§ç»­æ­£å¸¸æµç¨‹
        #     }
        # )

        # ğŸ†• batch_aggregator â†’ batch_router (æ¡ä»¶è¾¹ï¼šæ£€æŸ¥å®¡æ ¸çŠ¶æ€)
        workflow.add_conditional_edges(
            "batch_aggregator",
            self._route_from_batch_aggregator,
            ["batch_router", END]  # ğŸ”§ N1ä¿®å¤ï¼šç§»é™¤detect_challengesï¼Œé¿å…é‡å¤è°ƒç”¨result_aggregator
        )
        
        # æ‰¹æ¬¡è·¯ç”±å™¨æ ¹æ® current_batch å’Œ total_batches å†³å®šï¼š
        # - å¦‚æœè¿˜æœ‰ä¸‹ä¸€æ‰¹æ¬¡ â†’ batch_strategy_reviewï¼ˆå®¡æ ¸åç»§ç»­ï¼‰
        # - å¦‚æœæ‰€æœ‰æ‰¹æ¬¡å®Œæˆ â†’ analysis_review
        # è·¯ç”±é€»è¾‘åœ¨ _batch_router_node ä¸­é€šè¿‡ Command å®ç°

        # æ‰¹æ¬¡ç­–ç•¥å®¡æ ¸ â†’ æ‰¹æ¬¡æ‰§è¡Œå™¨ï¼ˆå½¢æˆå¾ªç¯ï¼‰
        workflow.add_edge("batch_strategy_review", "batch_executor")
        # batch_strategy_review ä½¿ç”¨ Command è·¯ç”±ï¼Œå¯ä»¥è·³è¿‡å®¡æ ¸ç›´æ¥åˆ° batch_executor

        # âœ… æ‰¹æ¬¡æ‰§è¡Œå™¨ â†’ æ™ºèƒ½ä½“æ‰§è¡Œå™¨ï¼ˆæ¡ä»¶è¾¹ + Send APIï¼‰
        # ä½¿ç”¨æ¡ä»¶è¾¹å‡½æ•°åŠ¨æ€åˆ›å»ºå¹¶è¡Œä»»åŠ¡
        workflow.add_conditional_edges(
            "batch_executor",              # æºèŠ‚ç‚¹
            self._create_batch_sends,      # æ¡ä»¶è¾¹å‡½æ•°ï¼šè¿”å› List[Send]
            ["agent_executor"]             # å¯èƒ½çš„ç›®æ ‡èŠ‚ç‚¹åˆ—è¡¨
        )

        # ============================================================================
        # å®¡æ ¸ä¸ç»“æœç”Ÿæˆæµç¨‹
        # ============================================================================
        # analysis_review ä½¿ç”¨ Command åŠ¨æ€è·¯ç”±åˆ°ï¼š
        # - detect_challengesï¼ˆæ‰¹å‡†åå…ˆæ£€æµ‹æŒ‘æˆ˜ï¼‰
        # - batch_executorï¼ˆé‡æ‰§è¡Œç‰¹å®šæ‰¹æ¬¡ï¼‰
        # - project_directorï¼ˆé‡æ–°è§„åˆ’ï¼‰
        
        # ğŸ†• v3.5: æŒ‘æˆ˜æ£€æµ‹åœ¨å®¡æ ¸é€šè¿‡åã€ç»“æœèšåˆå‰æ‰§è¡Œ
        workflow.add_conditional_edges(
            "detect_challenges",
            self._route_after_challenge_detection,
            {
                "revisit_requirements": "requirements_analyst",  # åé¦ˆå¾ªç¯
                "manual_review": "manual_review",  # ğŸ†• äººå·¥å®¡æ ¸ï¼ˆ>3ä¸ªmust_fixï¼‰
                "continue_workflow": "result_aggregator"  # ç»§ç»­åˆ°ç»“æœèšåˆ
            }
        )
        
        # ğŸ†• äººå·¥å®¡æ ¸èŠ‚ç‚¹ï¼šmanual_review ä½¿ç”¨ Command åŠ¨æ€è·¯ç”±åˆ°ï¼š
        # - batch_executorï¼ˆé‡æ–°æ‰§è¡Œç‰¹å®šä¸“å®¶ï¼‰
        # - detect_challengesï¼ˆç»§ç»­æŒ‘æˆ˜æ£€æµ‹ï¼‰
        # - ENDï¼ˆç»ˆæ­¢æµç¨‹ï¼‰

        workflow.add_edge("result_aggregator", "report_guard")  # ğŸ†• æŠ¥å‘Šå®¡æ ¸
        workflow.add_edge("report_guard", "pdf_generator")      # ğŸ†• å®¡æ ¸åç”ŸæˆPDF

        workflow.add_conditional_edges(
            "pdf_generator",
            self._route_after_pdf_generator,
            ["user_question", END]
        )

        workflow.add_conditional_edges(
            "user_question",
            self._route_after_user_question,
            ["project_director", "result_aggregator"]
        )

        # ============================================================================
        # ç¼–è¯‘å›¾
        # ============================================================================
        logger.info("ğŸ”§ Workflow graph built with dynamic batch execution support")
        logger.info("   Nodes: batch_executor, agent_executor, batch_aggregator, batch_router, batch_strategy_review")
        logger.info("   Supports: 1-N batches with dependency-based execution")

        return workflow.compile(
            checkpointer=self.checkpointer,
            store=self.store
        )
    
    # ============================================================================
    # ğŸ†• å®‰å…¨èŠ‚ç‚¹åŒ…è£…æ–¹æ³•
    # ============================================================================
    
    # ============================================================================
    # ğŸ†• v7.3 ç»Ÿä¸€è¾“å…¥éªŒè¯èŠ‚ç‚¹åŒ…è£…æ–¹æ³•
    # ============================================================================

    def _unified_input_validator_initial_node(self, state: ProjectAnalysisState) -> Command:
        """
        ç»Ÿä¸€è¾“å…¥éªŒè¯ - é˜¶æ®µ1: åˆå§‹éªŒè¯

        æ‰§è¡Œå†…å®¹å®‰å…¨æ£€æµ‹ã€é¢†åŸŸåˆ†ç±»æ£€æµ‹ã€ä»»åŠ¡å¤æ‚åº¦è¯„ä¼°
        """
        try:
            logger.info("Executing unified input validator - initial validation")
            result = UnifiedInputValidatorNode.execute_initial_validation(
                state,
                store=self.store,
                llm_model=self.llm_model
            )

            if not isinstance(result, Command):
                logger.error("UnifiedInputValidatorNode.execute_initial_validation did not return Command object")
                return Command(
                    update={"rejection_reason": "system_error"},
                    goto="input_rejected"
                )

            # æå–çŠ¶æ€æ›´æ–°
            update_payload = dict(result.update or {})
            update_payload["detail"] = "æ£€æµ‹è¾“å…¥å†…å®¹å®‰å…¨æ€§ä¸é¢†åŸŸé€‚é…æ€§"

            # è¯»å–å¤æ‚åº¦è¯„ä¼°ç»“æœ
            suggested_workflow = update_payload.get("suggested_workflow", "full_analysis")
            task_complexity = update_payload.get("task_complexity", "complex")

            # å¦‚æœåŸæœ¬è¦å» input_rejectedï¼Œä¿æŒä¸å˜
            if result.goto == "input_rejected":
                return Command(update=update_payload, goto="input_rejected")

            # ğŸ”„ ç»Ÿä¸€è·¯ç”±ï¼šæ‰€æœ‰ä»»åŠ¡éƒ½èµ°å®Œæ•´æµç¨‹
            # ğŸ”§ v7.1 ä¿®å¤ï¼šä¸å†è‡ªåŠ¨è·³è¿‡é—®å·ï¼Œè®©ç”¨æˆ·å†³å®š
            # åŸé€»è¾‘ï¼šmedium å¤æ‚åº¦è·³è¿‡é—®å·ï¼Œä½†ä½ç½®ä¿¡åº¦çš„é»˜è®¤ medium æ°æ°éœ€è¦é—®å·è¡¥å……ä¿¡æ¯
            # æ–°é€»è¾‘ï¼šæ‰€æœ‰ä»»åŠ¡éƒ½æ˜¾ç¤ºé—®å·ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©è·³è¿‡
            complexity_confidence = update_payload.get("complexity_confidence", 0.5)
            logger.info(f"ğŸ“‹ {task_complexity} ä»»åŠ¡æ£€æµ‹ï¼ˆç½®ä¿¡åº¦: {complexity_confidence:.2f}ï¼‰ï¼Œä½¿ç”¨å®Œæ•´æµç¨‹")
            logger.info(f"   suggested_workflow: {suggested_workflow}")
            # skip_calibration é»˜è®¤ä¸º Falseï¼Œç”±æ ¡å‡†é—®å·èŠ‚ç‚¹è‡ªèº«é€»è¾‘æˆ–ç”¨æˆ·é€‰æ‹©å†³å®š

            return Command(update=update_payload, goto="requirements_analyst")

        except Interrupt:
            # ğŸ”¥ ä¿®å¤ï¼šInterrupt æ˜¯æ­£å¸¸çš„äº¤äº’ä¸­æ–­ï¼Œä¸åº”è¯¥æ•è·ï¼Œåº”è¯¥å‘ä¸Šä¼ æ’­
            logger.info("Input guard node raised an interrupt (user interaction required)")
            raise
        except Exception as e:
            logger.error(f"Error in input guard node: {e}")
            import traceback
            traceback.print_exc()
            # å‡ºé”™æ—¶ä¿å®ˆæ‹’ç»
            return Command(
                update={
                    "rejection_reason": "system_error",
                    "rejection_message": "ç³»ç»Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                    "detail": "å®‰å…¨æ£€æµ‹å¼‚å¸¸"
                },
                goto="input_rejected"
            )
    
    def _input_rejected_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """è¾“å…¥æ‹’ç»èŠ‚ç‚¹åŒ…è£…"""
        try:
            logger.info("Executing input rejected node")
            return InputRejectedNode.execute(state, store=self.store)
        except Exception as e:
            logger.error(f"Error in input rejected node: {e}")
            return {
                "final_status": "rejected",
                "rejection_message": "ç³»ç»Ÿé”™è¯¯ï¼Œæµç¨‹ç»ˆæ­¢ã€‚"
            }
    
    def _unified_input_validator_secondary_node(self, state: ProjectAnalysisState) -> Union[Dict[str, Any], Command]:
        """
        ç»Ÿä¸€è¾“å…¥éªŒè¯ - é˜¶æ®µ2: äºŒæ¬¡éªŒè¯

        åœ¨éœ€æ±‚åˆ†æåï¼Œé‡æ–°éªŒè¯é¢†åŸŸä¸€è‡´æ€§ï¼Œæ£€æµ‹é¢†åŸŸæ¼‚ç§»
        """
        from langgraph.types import Interrupt
        try:
            logger.info("Executing unified input validator - secondary validation")
            result = UnifiedInputValidatorNode.execute_secondary_validation(
                state,
                store=self.store,
                llm_model=self.llm_model
            )

            # âœ… æ­£å¸¸æƒ…å†µï¼šè¿”å›å­—å…¸ï¼ˆç”±é™æ€ edge è·¯ç”±åˆ° calibration_questionnaireï¼‰
            # âœ… æ‹’ç»æƒ…å†µï¼šè¿”å› Command(goto="input_rejected")ï¼ˆç»ˆæ­¢å·¥ä½œæµï¼‰
            # âœ… è°ƒæ•´æƒ…å†µï¼šè¿”å› Command(goto="requirements_analyst")ï¼ˆé‡æ–°åˆ†æï¼‰
            if isinstance(result, Command):
                logger.warning("âš ï¸ Secondary validation returned Command (rejection or re-analysis)")
                return result

            # ğŸ”¥ ä¿ç•™ skip_unified_review æ ‡å¿—
            if state.get("skip_unified_review"):
                result["skip_unified_review"] = True
                logger.info("ğŸ” [DEBUG] secondary_validation ä¿ç•™ skip_unified_review=True")

            # æ·»åŠ  detail å­—æ®µ
            result["detail"] = "äºŒæ¬¡éªŒè¯é¢†åŸŸé€‚é…æ€§"

            logger.info("ğŸ”„ [DEBUG] Secondary validation completed, proceeding to calibration_questionnaire")
            return result

        except Interrupt:
            # ğŸ”¥ ä¿®å¤ï¼šInterrupt æ˜¯æ­£å¸¸çš„äº¤äº’ä¸­æ–­ï¼Œä¸åº”è¯¥æ•è·ï¼Œåº”è¯¥å‘ä¸Šä¼ æ’­
            logger.info("Secondary validation raised an interrupt (user interaction required)")
            raise
        except Exception as e:
            logger.error(f"Error in secondary validation node: {e}")
            import traceback
            traceback.print_exc()
            # å‡ºé”™æ—¶ä¿¡ä»»åˆå§‹åˆ¤æ–­
            logger.warning("Secondary validation failed, trusting initial judgment")
            return {
                "secondary_validation_skipped": True,
                "secondary_validation_reason": "error_occurred"
            }
    
    def _report_guard_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """æŠ¥å‘Šå®¡æ ¸èŠ‚ç‚¹åŒ…è£…"""
        try:
            logger.info("Executing report guard node")
            result = ReportGuardNode.execute(state, store=self.store, llm_model=self.llm_model)
            result["detail"] = "å®¡æ ¸æŠ¥å‘Šå†…å®¹"
            return result
        except Exception as e:
            logger.error(f"Error in report guard node: {e}")
            # å‡ºé”™æ—¶æ”¾è¡Œï¼ˆé¿å…è¯¯æ‹¦ï¼‰
            logger.warning("Report guard failed, allowing report to pass")
            return {"report_safety_status": "error_passthrough", "detail": "æŠ¥å‘Šå®¡æ ¸å¼‚å¸¸"}

    def _requirements_analyst_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        éœ€æ±‚åˆ†æå¸ˆèŠ‚ç‚¹

        æ³¨æ„: åªè¿”å›éœ€è¦æ›´æ–°çš„å­—æ®µ,ä¸è¿”å›å®Œæ•´çŠ¶æ€
        è¿™æ ·å¯ä»¥é¿å…å¹¶å‘æ›´æ–°å†²çª
        """
        try:
            logger.info("Executing requirements analyst node")
            
            # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥è¿›å…¥æ—¶çš„æ ‡å¿—çŠ¶æ€
            logger.info(f"ğŸ” [DEBUG] requirements_analyst è¿›å…¥æ—¶ calibration_processed: {state.get('calibration_processed')}")
            logger.info(f"ğŸ” [DEBUG] requirements_analyst è¿›å…¥æ—¶ calibration_skipped: {state.get('calibration_skipped')}")
            logger.info(f"ğŸ” [DEBUG] requirements_analyst è¿›å…¥æ—¶ state æ‰€æœ‰é”®: {list(state.keys())}")
            
            # ğŸ” æ·±åº¦è°ƒè¯•ï¼šæ£€æŸ¥ state æ˜¯å¦åŒ…å«è¿™äº›é”®
            if "calibration_processed" in state:
                logger.info(f"ğŸ” [DEBUG] 'calibration_processed' é”®å­˜åœ¨ï¼Œå€¼={state['calibration_processed']}")
            else:
                logger.info("ğŸ” [DEBUG] 'calibration_processed' é”®ä¸å­˜åœ¨")

            # åˆ›å»ºéœ€æ±‚åˆ†æå¸ˆæ™ºèƒ½ä½“
            agent = AgentFactory.create_agent(
                AgentType.REQUIREMENTS_ANALYST,
                llm_model=self.llm_model,
                config=self.config
            )

            # æ‰§è¡Œåˆ†æ
            result = agent.execute(state, {}, self.store)

            # ğŸ†• æå–é¡¹ç›®ç±»å‹ï¼ˆä» structured_data ä¸­ï¼‰
            project_type = result.structured_data.get("project_type") if result.structured_data else None

            # ğŸ†• v7.0: ä» primary_deliverables ä¸­æå–äº¤ä»˜ç‰©å…ƒæ•°æ®å’Œè´£ä»»è€…æ˜ å°„
            deliverable_owner_map = {}
            deliverable_metadata = {}

            if result.structured_data:
                primary_deliverables = result.structured_data.get("primary_deliverables", [])

                for deliverable in primary_deliverables:
                    if not isinstance(deliverable, dict):
                        continue

                    deliverable_id = deliverable.get("deliverable_id", "")
                    if not deliverable_id:
                        continue

                    # æå– owner ä¿¡æ¯
                    owner_suggestion = deliverable.get("deliverable_owner_suggestion", {})
                    primary_owner = owner_suggestion.get("primary_owner", "")
                    supporters = owner_suggestion.get("supporters", [])

                    # ğŸ”§ ä¿®å¤é—®é¢˜1: æ ‡è®°ä¸ºé¢„æµ‹å€¼ï¼Œåç»­ç”± project_director æ ¡æ­£
                    # å¡«å…… deliverable_owner_mapï¼ˆæ­¤æ—¶æ˜¯éœ€æ±‚åˆ†æå¸ˆçš„é¢„æµ‹ï¼‰
                    if primary_owner:
                        deliverable_owner_map[deliverable_id] = primary_owner

                    # å¡«å…… deliverable_metadataï¼ˆåŒ…å«å®Œæ•´ä¿¡æ¯ï¼‰
                    deliverable_metadata[deliverable_id] = {
                        "name": deliverable.get("description", deliverable_id),
                        "type": deliverable.get("type", "unknown"),
                        "priority": deliverable.get("priority", "MUST_HAVE"),
                        "owner": primary_owner,  # é¢„æµ‹å€¼ï¼Œå¾…æ ¡æ­£
                        "owner_predicted": True,  # ğŸ†• æ ‡è®°ä¸ºé¢„æµ‹å€¼
                        "supporters": supporters,
                        "acceptance_criteria": deliverable.get("acceptance_criteria", []),
                        "format_requirements": deliverable.get("format_requirements", {}),
                        "source_requirement": deliverable.get("source_requirement", "")
                    }

                if deliverable_owner_map:
                    logger.info(f"ğŸ¯ [v7.0] æå–åˆ° {len(deliverable_owner_map)} ä¸ªäº¤ä»˜ç‰©è´£ä»»è€…æ˜ å°„ï¼ˆé¢„æµ‹å€¼ï¼‰: {deliverable_owner_map}")

            # åªè¿”å›éœ€è¦æ›´æ–°çš„å­—æ®µ
            # âœ… ä¿®å¤: ä½¿ç”¨ agent_type.value ä¿æŒä¸€è‡´æ€§
            # ğŸ”§ ä¿®å¤: ä¿ç•™é‡è¦çš„æµç¨‹æ§åˆ¶æ ‡å¿—ï¼Œé¿å…å¾ªç¯
            update_dict = {
                "current_stage": AnalysisStage.REQUIREMENT_COLLECTION.value,
                "structured_requirements": result.structured_data,
                "project_type": project_type,  # ğŸ†• æ·»åŠ é¡¹ç›®ç±»å‹å­—æ®µ
                "deliverable_owner_map": deliverable_owner_map,  # ğŸ†• v7.0: äº¤ä»˜ç‰©è´£ä»»è€…æ˜ å°„
                "deliverable_metadata": deliverable_metadata,  # ğŸ†• v7.0: äº¤ä»˜ç‰©å®Œæ•´å…ƒæ•°æ®
                "agent_results": {
                    AgentType.REQUIREMENTS_ANALYST.value: result.to_dict()
                },
                "updated_at": datetime.now().isoformat()
            }
            
            # ğŸ”§ å…³é”®ä¿®å¤: ä»å®Œæ•´çŠ¶æ€ä¸­æå–å¹¶ä¿ç•™æµç¨‹æ§åˆ¶æ ‡å¿—
            # æ³¨æ„: Command.update çš„å­—æ®µåœ¨ç›®æ ‡èŠ‚ç‚¹æ‰§è¡Œæ—¶ä¸å¯è§,
            # æ‰€ä»¥è¿™é‡Œéœ€è¦ä» agent_results ä¸­è·å–åŸå§‹ state çš„æ ‡å¿—å€¼
            # ä½†å®é™…ä¸Š,æˆ‘ä»¬åº”è¯¥è®©è¿™äº›æ ‡å¿—"ç©¿é€"æ•´ä¸ªåˆ†æè¿‡ç¨‹
            full_state = dict(state)  # è·å–å®Œæ•´çŠ¶æ€å‰¯æœ¬
            
            # ğŸ›¡ï¸ å¢å¼ºä¿®å¤ï¼šæ£€æŸ¥æ ‡å¿— OR æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦å­˜åœ¨
            has_processed = (
                ("calibration_processed" in full_state and full_state["calibration_processed"]) or 
                ("calibration_answers" in full_state and full_state["calibration_answers"])
            )
            
            if has_processed:
                update_dict["calibration_processed"] = True
                logger.info("ğŸ” [DEBUG] ä¿ç•™/æ¢å¤ calibration_processed=True æ ‡å¿—")
                
            if "calibration_skipped" in full_state and full_state["calibration_skipped"]:
                update_dict["calibration_skipped"] = True
                logger.info("ğŸ” [DEBUG] ä¿ç•™ calibration_skipped=True æ ‡å¿—")
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¿ç•™ modification_confirmation_round è½®æ¬¡è®¡æ•°
            if "modification_confirmation_round" in full_state:
                update_dict["modification_confirmation_round"] = full_state["modification_confirmation_round"]
                logger.info(f"ğŸ” [DEBUG] ä¿ç•™ modification_confirmation_round={full_state['modification_confirmation_round']} æ ‡å¿—")
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¿ç•™ skip_unified_review æ ‡å¿—
            if "skip_unified_review" in full_state and full_state["skip_unified_review"]:
                update_dict["skip_unified_review"] = True
                logger.info("ğŸ” [DEBUG] ä¿ç•™ skip_unified_review=True æ ‡å¿—")
            
            logger.info(f"ğŸ” [DEBUG] requirements_analyst è¿”å›çš„å­—æ®µ: {list(update_dict.keys())}")
            return update_dict

        except Exception as e:
            logger.error(f"Requirements analyst node failed: {e}")
            import traceback
            traceback.print_exc()
            return {
                "error": str(e),
                "updated_at": datetime.now().isoformat()
            }

    def _feasibility_analyst_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        V1.5 å¯è¡Œæ€§åˆ†æå¸ˆèŠ‚ç‚¹ï¼ˆåå°å†³ç­–æ”¯æŒï¼‰

        èŒè´£:
        1. åŸºäºV1çš„structured_requirementsè¿›è¡Œå¯è¡Œæ€§åˆ†æ
        2. æ£€æµ‹é¢„ç®—/æ—¶é—´/ç©ºé—´å†²çª
        3. è®¡ç®—éœ€æ±‚ä¼˜å…ˆçº§
        4. ç”Ÿæˆå†³ç­–å»ºè®®
        5. å°†ç»“æœå­˜å‚¨åˆ°state.feasibility_assessmentï¼ˆä¸å±•ç¤ºåœ¨å‰ç«¯ï¼‰
        6. åç»­åœ¨project_directorä¸­ç”¨äºæŒ‡å¯¼ä¸“å®¶ä»»åŠ¡åˆ†æ´¾
        """
        try:
            logger.info("Executing V1.5 feasibility analyst node")

            # åˆ›å»ºV1.5å¯è¡Œæ€§åˆ†æå¸ˆæ™ºèƒ½ä½“
            feasibility_agent = FeasibilityAnalystAgent(
                llm_model=self.llm_model,
                config=self.config
            )

            # éªŒè¯è¾“å…¥ï¼šV1çš„structured_requirementså¿…é¡»å­˜åœ¨
            if not state.get("structured_requirements"):
                logger.warning("âš ï¸ V1.5è·³è¿‡ï¼šstructured_requirementsä¸å­˜åœ¨")
                return {"updated_at": datetime.now().isoformat()}

            # æ‰§è¡Œå¯è¡Œæ€§åˆ†æ
            result = feasibility_agent.execute(state, {}, self.store)

            # å­˜å‚¨åˆ†æç»“æœåˆ°stateï¼ˆä»…åå°å­˜å‚¨ï¼Œä¸å±•ç¤ºåˆ°å‰ç«¯ï¼‰
            update_dict = {
                "feasibility_assessment": result.structured_data,  # å®Œæ•´çš„å¯è¡Œæ€§åˆ†æç»“æœ
                "updated_at": datetime.now().isoformat()
            }

            # æ—¥å¿—è®°å½•å…³é”®å‘ç°ï¼ˆç”¨äºè°ƒè¯•å’Œç›‘æ§ï¼‰
            if result.structured_data:
                feasibility = result.structured_data.get("feasibility_assessment", {})
                overall = feasibility.get("overall_feasibility", "unknown")
                conflicts = result.structured_data.get("conflict_detection", {})

                logger.info(f"âœ… V1.5å¯è¡Œæ€§åˆ†æå®Œæˆ: overall_feasibility={overall}")

                # è®°å½•å†²çªæ£€æµ‹ç»“æœ
                budget_conflicts = conflicts.get("budget_conflicts", [])
                if budget_conflicts and budget_conflicts[0].get("detected"):
                    severity = budget_conflicts[0].get("severity", "unknown")
                    logger.info(f"âš ï¸ é¢„ç®—å†²çªæ£€æµ‹: severity={severity}")

                # è®°å½•ä¼˜å…ˆçº§çŸ©é˜µ
                priority_matrix = result.structured_data.get("priority_matrix", [])
                if priority_matrix:
                    top_req = priority_matrix[0]
                    logger.info(f"ğŸ¯ æœ€é«˜ä¼˜å…ˆçº§éœ€æ±‚: {top_req.get('requirement')} (score={top_req.get('priority_score')})")

            return update_dict

        except Exception as e:
            logger.error(f"V1.5 Feasibility analyst node failed: {e}")
            import traceback
            traceback.print_exc()
            return {
                "error": str(e),
                "updated_at": datetime.now().isoformat()
            }

    def _calibration_questionnaire_node(self, state: ProjectAnalysisState) -> Command:
        """
        æˆ˜ç•¥æ ¡å‡†é—®å·èŠ‚ç‚¹

        æ ¹æ®éœ€æ±‚åˆ†æå¸ˆæ–‡æ¡£v1.0çš„è¦æ±‚ï¼š
        åœ¨å®Œæˆéœ€æ±‚åˆ†æåï¼Œå¿…é¡»ç”Ÿæˆ"æˆ˜ç•¥æ ¡å‡†é—®å·"å¹¶ç­‰å¾…ç”¨æˆ·å›ç­”ã€‚

        æ³¨æ„: ä¸è¦æ•è·Interruptå¼‚å¸¸!
        Interruptæ˜¯LangGraphçš„æ­£å¸¸æ§åˆ¶æµ,å¿…é¡»è®©å®ƒä¼ æ’­åˆ°æ¡†æ¶å±‚
        """
        logger.info("Executing calibration questionnaire node")
        return CalibrationQuestionnaireNode.execute(state, self.store)

    def _requirements_confirmation_node(self, state: ProjectAnalysisState) -> Command:
        """
        éœ€æ±‚ç¡®è®¤èŠ‚ç‚¹

        æ³¨æ„: ä¸è¦æ•è·Interruptå¼‚å¸¸!
        Interruptæ˜¯LangGraphçš„æ­£å¸¸æ§åˆ¶æµ,å¿…é¡»è®©å®ƒä¼ æ’­åˆ°æ¡†æ¶å±‚
        """
        logger.info("Executing requirements confirmation node")
        return RequirementsConfirmationNode.execute(state, self.store)
    
    def _find_matching_role(self, predicted_role: str, active_agents: List[str]) -> Optional[str]:
        """
        ğŸ”§ ä¿®å¤é—®é¢˜1: æŸ¥æ‰¾é¢„æµ‹è§’è‰²åœ¨å®é™…é€‰å®šè§’è‰²ä¸­çš„åŒ¹é…é¡¹

        Args:
            predicted_role: éœ€æ±‚åˆ†æå¸ˆé¢„æµ‹çš„è§’è‰²IDï¼ˆå¦‚ "V2_è®¾è®¡ç­–ç•¥ä¸“å®¶_3-2"ï¼‰
            active_agents: é¡¹ç›®æ€»ç›‘å®é™…é€‰å®šçš„è§’è‰²åˆ—è¡¨ï¼ˆå¦‚ ["V2_è®¾è®¡æ€»ç›‘_2-2", ...]ï¼‰

        Returns:
            åŒ¹é…çš„å®é™…è§’è‰²IDï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None

        åŒ¹é…è§„åˆ™ï¼š
        1. ç²¾ç¡®åŒ¹é…ï¼ˆpredicted_role in active_agentsï¼‰
        2. å‰ç¼€åŒ¹é…ï¼ˆæå– V2_è®¾è®¡ å‰ç¼€ï¼ŒæŸ¥æ‰¾ä»¥æ­¤å¼€å¤´çš„è§’è‰²ï¼‰
        3. å±‚çº§åŒ¹é…ï¼ˆæå– V2 å±‚çº§ï¼ŒæŸ¥æ‰¾åŒå±‚çº§è§’è‰²ï¼‰
        """
        # è§„åˆ™1: ç²¾ç¡®åŒ¹é…
        if predicted_role in active_agents:
            return predicted_role

        # è§„åˆ™2: å‰ç¼€åŒ¹é…ï¼ˆV2_è®¾è®¡ï¼‰
        parts = predicted_role.split("_")
        if len(parts) >= 2:
            prefix = f"{parts[0]}_{parts[1]}"  # V2_è®¾è®¡

            for agent_id in active_agents:
                if agent_id.startswith(prefix):
                    logger.debug(f"ğŸ” [åŒ¹é…] å‰ç¼€åŒ¹é…: {predicted_role} â†’ {agent_id}")
                    return agent_id

        # è§„åˆ™3: å±‚çº§åŒ¹é…ï¼ˆV2ï¼‰
        if len(parts) >= 1:
            level_prefix = parts[0]  # V2

            for agent_id in active_agents:
                if agent_id.startswith(level_prefix):
                    logger.debug(f"ğŸ” [åŒ¹é…] å±‚çº§åŒ¹é…: {predicted_role} â†’ {agent_id}")
                    return agent_id

        logger.warning(f"âš ï¸ [åŒ¹é…å¤±è´¥] æœªæ‰¾åˆ° {predicted_role} çš„åŒ¹é…è§’è‰²")
        return None

    def _project_director_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        é¡¹ç›®æ€»ç›‘èŠ‚ç‚¹ - è¿›è¡Œæˆ˜ç•¥åˆ†æå¹¶å‡†å¤‡å¹¶è¡Œä»»åŠ¡ï¼ˆä»… Dynamic Modeï¼‰

        æ³¨æ„:
        - ProjectDirectorè¿”å›Commandå¯¹è±¡,ä½†æˆ‘ä»¬éœ€è¦æå–çŠ¶æ€æ›´æ–°
        - ä»…æ”¯æŒåŠ¨æ€æ¨¡å¼: ä½¿ç”¨ active_agents å’Œ execution_mode
        """
        try:
            logger.info("Executing project director node (Dynamic Mode)")

            # åˆ›å»ºé¡¹ç›®æ€»ç›‘æ™ºèƒ½ä½“
            agent = AgentFactory.create_agent(
                AgentType.PROJECT_DIRECTOR,
                llm_model=self.llm_model,
                config=self.config
            )

            # æ‰§è¡Œåˆ†æ - è¿”å›Commandå¯¹è±¡
            command = agent.execute(state, {}, self.store)

            # æå–çŠ¶æ€æ›´æ–°
            if isinstance(command, Command):
                state_update = command.update or {}
            else:
                logger.error(f"Unexpected return type from ProjectDirector: {type(command)}")
                state_update = {"error": "ProjectDirector returned unexpected type"}

            # ğŸ”¥ ä¿ç•™å…³é”®çŠ¶æ€æ ‡å¿—ï¼ˆé¿å…è¢«è¦†ç›–ï¼‰
            if state.get("skip_unified_review"):
                state_update["skip_unified_review"] = True
                logger.info("ğŸ”„ ä¿ç•™ skip_unified_review æ ‡å¿—")
            else:
                logger.warning("âš ï¸ [DEBUG] project_director è¿›å…¥æ—¶ skip_unified_review ä¸å­˜åœ¨æˆ–ä¸ºFalse")
                logger.info(f"ğŸ” [DEBUG] project_director è¾“å…¥ state keys: {list(state.keys())}")

            # åŠ¨æ€æ¨¡å¼: ä½¿ç”¨ active_agents
            active_agents = state_update.get("active_agents", [])
            logger.info(f"ğŸ“Œ Dynamic mode: Project director selected {len(active_agents)} dynamic roles")
            logger.debug(f"Active agents: {active_agents}")

            # ğŸ”§ ä¿®å¤é—®é¢˜1: æ ¡æ­£äº¤ä»˜ç‰©è´£ä»»è€…æ˜ å°„
            deliverable_metadata = state.get("deliverable_metadata") or {}
            deliverable_owner_map = state.get("deliverable_owner_map") or {}

            if deliverable_metadata and active_agents:
                corrected_owner_map = {}
                corrected_metadata = {}

                for deliverable_id, metadata in deliverable_metadata.items():
                    predicted_owner = metadata.get("owner", "")

                    if metadata.get("owner_predicted") and predicted_owner:
                        # æŸ¥æ‰¾å®é™…é€‰å®šçš„åŒ¹é…è§’è‰²
                        actual_owner = self._find_matching_role(predicted_owner, active_agents)

                        if actual_owner and actual_owner != predicted_owner:
                            logger.info(f"ğŸ”§ [ä¿®å¤] äº¤ä»˜ç‰© {deliverable_id} è´£ä»»è€…æ ¡æ­£: {predicted_owner} â†’ {actual_owner}")
                            corrected_owner_map[deliverable_id] = actual_owner

                            # æ›´æ–°å…ƒæ•°æ®
                            corrected_metadata[deliverable_id] = {
                                **metadata,
                                "owner": actual_owner,
                                "owner_predicted": False,  # æ ‡è®°ä¸ºå·²æ ¡æ­£
                                "owner_original_prediction": predicted_owner  # ä¿ç•™åŸå§‹é¢„æµ‹
                            }
                        else:
                            # æœªæ‰¾åˆ°åŒ¹é…æˆ–å·²åŒ¹é…ï¼Œä¿æŒåŸå€¼
                            corrected_owner_map[deliverable_id] = predicted_owner
                            corrected_metadata[deliverable_id] = metadata
                    else:
                        # éé¢„æµ‹å€¼ï¼Œä¿æŒåŸå€¼
                        corrected_owner_map[deliverable_id] = predicted_owner
                        corrected_metadata[deliverable_id] = metadata

                # æ›´æ–°çŠ¶æ€
                if corrected_owner_map:
                    state_update["deliverable_owner_map"] = corrected_owner_map
                    state_update["deliverable_metadata"] = corrected_metadata
                    logger.info(f"âœ… [ä¿®å¤] å·²æ ¡æ­£ {len(corrected_owner_map)} ä¸ªäº¤ä»˜ç‰©è´£ä»»è€…æ˜ å°„")

            # ç¡®ä¿ execution_mode è¢«è®¾ç½®ä¸º dynamic
            state_update["execution_mode"] = "dynamic"

            logger.info(f"Project director completed, prepared {len(active_agents)} agents for execution")

            # åªè¿”å›çŠ¶æ€æ›´æ–°,ä¸è¿”å›Command
            # è·¯ç”±ç”±æ¡ä»¶è¾¹å¤„ç†
            state_update.setdefault("detail", f"è§„åˆ’ä¸“å®¶å›¢é˜Ÿå¹¶åˆ›å»º {len(active_agents)} åä¸“å®¶æ‰¹æ¬¡")
            return state_update

        except Exception as e:
            logger.error(f"Project director node failed: {e}")
            import traceback
            traceback.print_exc()
            
            # ğŸ”¥ è¿”å›æ˜ç¡®çš„é”™è¯¯çŠ¶æ€ï¼Œé˜²æ­¢ä¸‹æ¸¸èŠ‚ç‚¹å´©æºƒ
            return {
                "error": str(e),
                "strategic_analysis": None,  # æ˜ç¡®æ ‡è®°ä¸º None
                "active_agents": [],
                "execution_mode": "dynamic",
                "errors": [{"node": "project_director", "error": str(e), "type": "ProjectDirectorFailure"}]
            }

    def _role_task_unified_review_node(self, state: ProjectAnalysisState):
        """
        ğŸ†• è§’è‰²ä»»åŠ¡ç»Ÿä¸€å®¡æ ¸èŠ‚ç‚¹

        åˆå¹¶è§’è‰²é€‰æ‹©å®¡æ ¸å’Œä»»åŠ¡åˆ†æ´¾å®¡æ ¸ï¼Œå‡å°‘äººæœºäº¤äº’æ¬¡æ•°

        æ³¨æ„: ä¸è¦æ•è·Interruptå¼‚å¸¸!
        Interruptæ˜¯LangGraphçš„æ­£å¸¸æ§åˆ¶æµ,å¿…é¡»è®©å®ƒä¼ æ’­åˆ°æ¡†æ¶å±‚
        
        è¿”å›: Commandå¯¹è±¡ï¼ˆç”¨äºåŠ¨æ€è·¯ç”±åˆ° batch_executor æˆ– project_directorï¼‰
        """
        try:
            logger.info("ğŸ” Executing unified role & task review node")
            return role_task_unified_review_node(state)  # è¿”å›Command
        except Exception as e:
            # åªæ•è·éInterruptå¼‚å¸¸
            if "Interrupt" not in str(type(e)):
                logger.error(f"âŒ Unified review node failed: {e}")
                import traceback
                traceback.print_exc()
                return {"error": str(e)}
            else:
                # é‡æ–°æŠ›å‡ºInterruptå¼‚å¸¸
                raise

    # ============================================================================
    # ğŸ—‘ï¸ å·²åºŸå¼ƒçš„ç‹¬ç«‹å®¡æ ¸èŠ‚ç‚¹ï¼ˆä¿ç•™æ³¨é‡Šä½œä¸ºå†å²å‚è€ƒï¼‰
    # ============================================================================
    # def _role_selection_review_node(self, state: ProjectAnalysisState):
    #     """è§’è‰²é€‰æ‹©å®¡æ ¸èŠ‚ç‚¹ - å·²åˆå¹¶åˆ° role_task_unified_review"""
    #     pass
    
    # def _task_assignment_review_node(self, state: ProjectAnalysisState):
    #     """ä»»åŠ¡åˆ†æ´¾å®¡æ ¸èŠ‚ç‚¹ - å·²åˆå¹¶åˆ° role_task_unified_review"""
    #     pass
    

    async def _quality_preflight_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        è´¨é‡é¢„æ£€èŠ‚ç‚¹ - å‰ç½®é¢„é˜²ç¬¬1å±‚ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰

        ğŸš€ P1ä¼˜åŒ–ï¼šä½¿ç”¨å¼‚æ­¥è°ƒç”¨ï¼Œé…åˆ QualityPreflightNode çš„å¼‚æ­¥å®ç°

        åœ¨ä¸“å®¶æ‰§è¡Œå‰è¿›è¡Œ:
        1. é£é™©é¢„åˆ¤
        2. è´¨é‡æ£€æŸ¥æ¸…å•ç”Ÿæˆ
        3. èƒ½åŠ›åŒ¹é…åº¦éªŒè¯
        """
        try:
            logger.info("ğŸ” Executing quality preflight node (async)")
            node = QualityPreflightNode(self.llm_model)
            result = await node(state)  # ğŸš€ P1ä¼˜åŒ–ï¼šä½¿ç”¨ await è°ƒç”¨å¼‚æ­¥æ–¹æ³•
            result["detail"] = "è´¨é‡é¢„æ£€ä¸é£é™©è¯„ä¼°"
            return result
        except Exception as e:
            logger.error(f"âŒ Quality preflight node failed: {e}")
            import traceback
            traceback.print_exc()
            return {"preflight_completed": False, "error": str(e), "detail": "è´¨é‡é¢„æ£€å¤±è´¥"}

    def _continue_to_first_batch_agents(self, state: ProjectAnalysisState) -> List[Send]:
        """
        è·¯ç”±å‡½æ•° - åˆ›å»ºç¬¬ä¸€æ‰¹å¹¶è¡Œä»»åŠ¡(V3-V4-V5)ï¼ˆä»… Dynamic Modeï¼‰

        ç¬¬ä¸€æ‰¹ä¸“å®¶:
        - V3: æŠ€æœ¯æ¶æ„å¸ˆ
        - V4: ç”¨æˆ·ä½“éªŒè®¾è®¡å¸ˆ
        - V5: å•†ä¸šåˆ†æå¸ˆ

        è¿™äº›ä¸“å®¶çš„åˆ†æç»“æœå°†è¢«V2å’ŒV6ä½¿ç”¨

        æ”¯æŒé’ˆå¯¹æ€§é‡æ‰§è¡Œï¼š
        - å¦‚æœ state ä¸­æœ‰ specific_agents_to_runï¼Œåªæ‰§è¡ŒæŒ‡å®šçš„ä¸“å®¶
        """
        try:
            logger.info("Creating first batch parallel tasks (V3-V4-V5) using Send API (Dynamic Mode)")

            # æ£€æŸ¥æ˜¯å¦æœ‰æŒ‡å®šéœ€è¦é‡æ–°æ‰§è¡Œçš„ä¸“å®¶
            specific_agents = state.get("specific_agents_to_run", [])

            # åŠ¨æ€æ¨¡å¼: ä» active_agents ä¸­ç­›é€‰ç¬¬ä¸€æ‰¹ä¸“å®¶ (V3, V4, V5)
            active_agents = state.get("active_agents", [])
            logger.info(f"ğŸ¯ Dynamic mode: filtering first batch from {len(active_agents)} active agents")

            # ç¬¬ä¸€æ‰¹ä¸“å®¶: ä»¥ V3_, V4_, V5_ å¼€å¤´çš„è§’è‰²
            first_batch_roles = [
                role_id for role_id in active_agents
                if role_id.startswith("V3_") or role_id.startswith("V4_") or role_id.startswith("V5_")
            ]

            logger.info(f"Preparing to execute {len(first_batch_roles)} dynamic agents in first batch")
            logger.debug(f"First batch roles: {first_batch_roles}")

            # åˆ›å»ºSendå¯¹è±¡åˆ—è¡¨
            send_list = []
            for role_id in first_batch_roles:
                # åˆ›å»ºä¸€ä¸ªåŒ…å«role_idçš„æ–°çŠ¶æ€
                agent_state = dict(state)  # å¤åˆ¶å®Œæ•´çŠ¶æ€
                agent_state["role_id"] = role_id  # æ·»åŠ role_id (åŠ¨æ€æ¨¡å¼)
                agent_state["current_stage"] = AnalysisStage.PARALLEL_ANALYSIS.value
                agent_state["execution_batch"] = "first"  # æ ‡è®°ä¸ºç¬¬ä¸€æ‰¹
                agent_state["is_rerun"] = bool(specific_agents)  # æ ‡è®°æ˜¯å¦ä¸ºé‡æ–°æ‰§è¡Œ

                # ğŸ”§ ä¿®å¤ (2025-11-19): ä½¿ç”¨æ­£ç¡®çš„èŠ‚ç‚¹å agent_executor
                send_list.append(Send("agent_executor", agent_state))
                logger.debug(f"Created Send for first batch dynamic agent: {role_id}")

            return send_list

        except Exception as e:
            logger.error(f"Failed to create first batch parallel tasks: {e}")
            import traceback
            traceback.print_exc()
            return {
                "error": str(e),
                "strategic_analysis": None,  # æ˜ç¡®æ ‡è®°ä¸º None
                "active_agents": [],
                "detail": "é¡¹ç›®æ€»ç›‘åˆ†æå¤±è´¥"
            }

    def _continue_to_second_batch_agents(self, state: ProjectAnalysisState) -> List[Send]:
        """
        è·¯ç”±å‡½æ•° - åˆ›å»ºç¬¬äºŒæ‰¹å¹¶è¡Œä»»åŠ¡(V2-V6)ï¼ˆä»… Dynamic Modeï¼‰

        ç¬¬äºŒæ‰¹ä¸“å®¶:
        - V2: è®¾è®¡ç ”ç©¶åˆ†æå¸ˆ (ä¾èµ–V3-V4-V5çš„è¾“å‡º)
        - V6: å®æ–½è§„åˆ’å¸ˆ (ä¾èµ–V3-V4-V5çš„è¾“å‡º)

        è¿™äº›ä¸“å®¶å¯ä»¥è®¿é—® state["agent_results"] ä¸­ V3-V4-V5 çš„åˆ†æç»“æœ

        æ”¯æŒé’ˆå¯¹æ€§é‡æ‰§è¡Œ:
        - å¦‚æœ state ä¸­æœ‰ specific_agents_to_runï¼Œåªæ‰§è¡ŒæŒ‡å®šçš„ä¸“å®¶
        """
        try:
            logger.info("Creating second batch parallel tasks (V2-V6) using Send API (Dynamic Mode)")

            # æ£€æŸ¥æ˜¯å¦æœ‰æŒ‡å®šéœ€è¦é‡æ–°æ‰§è¡Œçš„ä¸“å®¶
            specific_agents = state.get("specific_agents_to_run", [])

            # åŠ¨æ€æ¨¡å¼: ä» active_agents ä¸­ç­›é€‰ç¬¬äºŒæ‰¹ä¸“å®¶ (V2, V6)
            active_agents = state.get("active_agents", [])
            logger.info(f"ğŸ¯ Dynamic mode: filtering second batch from {len(active_agents)} active agents")

            # ç¬¬äºŒæ‰¹ä¸“å®¶: ä»¥ V2_, V6_ å¼€å¤´çš„è§’è‰²
            second_batch_roles = [
                role_id for role_id in active_agents
                if role_id.startswith("V2_") or role_id.startswith("V6_")
            ]

            logger.info(f"Preparing to execute {len(second_batch_roles)} dynamic agents in second batch")
            logger.debug(f"Second batch roles: {second_batch_roles}")

            # éªŒè¯ç¬¬ä¸€æ‰¹ç»“æœæ˜¯å¦å­˜åœ¨
            agent_results = state.get("agent_results", {})
            first_batch_roles = [
                role_id for role_id in active_agents
                if role_id.startswith("V3_") or role_id.startswith("V4_") or role_id.startswith("V5_")
            ]

            first_batch_completed = all(
                role_id in agent_results
                for role_id in first_batch_roles
            )

            if not first_batch_completed:
                logger.warning(f"âš ï¸ First batch agents not all completed, but proceeding with second batch")
                logger.debug(f"Expected: {first_batch_roles}, Got: {list(agent_results.keys())}")

            # åˆ›å»ºSendå¯¹è±¡åˆ—è¡¨
            send_list = []
            for role_id in second_batch_roles:
                # åˆ›å»ºä¸€ä¸ªåŒ…å«role_idçš„æ–°çŠ¶æ€
                agent_state = dict(state)  # å¤åˆ¶å®Œæ•´çŠ¶æ€
                agent_state["role_id"] = role_id  # æ·»åŠ role_id (åŠ¨æ€æ¨¡å¼)
                agent_state["current_stage"] = AnalysisStage.PARALLEL_ANALYSIS.value
                agent_state["execution_batch"] = "second"  # æ ‡è®°ä¸ºç¬¬äºŒæ‰¹
                agent_state["is_rerun"] = bool(specific_agents)  # æ ‡è®°æ˜¯å¦ä¸ºé‡æ–°æ‰§è¡Œ

                # ğŸ”§ ä¿®å¤ (2025-11-19): ä½¿ç”¨æ­£ç¡®çš„èŠ‚ç‚¹å agent_executor
                send_list.append(Send("agent_executor", agent_state))
                logger.debug(f"Created Send for second batch dynamic agent: {role_id}")

            return send_list

        except Exception as e:
            logger.error(f"Failed to create second batch parallel tasks: {e}")
            import traceback
            traceback.print_exc()
            return []

    async def _execute_agent_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•ä¸ªæ™ºèƒ½ä½“èŠ‚ç‚¹ï¼ˆä»… Dynamic Modeï¼‰

        æ ¹æ®LangGraph Send API,è¿™ä¸ªèŠ‚ç‚¹æ¥æ”¶çš„stateæ˜¯é€šè¿‡Sendä¼ é€’çš„å®Œæ•´çŠ¶æ€
        çŠ¶æ€ä¸­åº”è¯¥åŒ…å«role_idå­—æ®µ,æŒ‡ç¤ºè¦æ‰§è¡Œå“ªä¸ªæ™ºèƒ½ä½“

        âœ¨ é›†æˆå®æ—¶è´¨é‡ç›‘æ§ï¼ˆç¬¬2å±‚é¢„é˜²ï¼‰:
        - æ‰§è¡Œå‰ï¼šæ³¨å…¥è´¨é‡çº¦æŸåˆ°prompt
        - æ‰§è¡Œåï¼šå¿«é€ŸéªŒè¯è¾“å‡ºè´¨é‡
        - å¦‚æœè´¨é‡ä¸è¾¾æ ‡ï¼šç»™äºˆä¸€æ¬¡é‡è¯•æœºä¼š

        é‡è¦:
        - åªè¿”å›éœ€è¦æ›´æ–°çš„å­—æ®µ,ä¸è¦è¿”å›å®Œæ•´çŠ¶æ€
        - agent_resultsä½¿ç”¨äº†Annotated[Dict, merge_agent_results]
        - å¤šä¸ªå¹¶å‘èŠ‚ç‚¹çš„agent_resultsä¼šè¢«è‡ªåŠ¨åˆå¹¶
        """
        try:
            # è·å– role_id
            role_id = state.get("role_id")

            if not role_id:
                logger.error("âŒ No role_id specified in state")
                logger.debug(f"State keys: {list(state.keys())}")
                return {"error": "No role_id specified"}

            # è·å–è´¨é‡æ£€æŸ¥æ¸…å•
            quality_checklists = state.get("quality_checklists", {})
            quality_checklist = quality_checklists.get(role_id, {})

            # è·å–é‡è¯•ä¿¡æ¯
            review_round = state.get("review_round", 0)
            review_feedback = state.get("review_feedback")
            retry_count = state.get(f"retry_count_{role_id}", 0)  # ğŸ†• é‡è¯•è®¡æ•°

            # åŠ¨æ€æ¨¡å¼: ä½¿ç”¨ TaskOrientedExpertFactory (v2.0)
            logger.info(f"ğŸ¯ Executing task-oriented agent: {role_id} (è½®æ¬¡{review_round}, é‡è¯•{retry_count})")

            # å¯¼å…¥ä»»åŠ¡å¯¼å‘ä¸“å®¶å·¥å‚
            from intelligent_project_analyzer.agents.task_oriented_expert_factory import TaskOrientedExpertFactory
            from intelligent_project_analyzer.core.role_manager import RoleManager

            # åˆå§‹åŒ–è§’è‰²ç®¡ç†å™¨
            role_manager = RoleManager()

            # è§£æè§’è‰²ID
            logger.info(f"ğŸ” [DEBUG] Parsing role_id: {role_id}")
            base_type, rid = role_manager.parse_full_role_id(role_id)
            logger.info(f"ğŸ” [DEBUG] Parsed: base_type={base_type}, rid={rid}")

            # âœ… å°è¯•ç›´æ¥è·å–é…ç½®ï¼Œå¦‚æœå¤±è´¥åˆ™ç”¨å‰ç¼€åŒ¹é…
            role_config = role_manager.get_role_config(base_type, rid)
            
            if not role_config:
                # ä½¿ç”¨å‰ç¼€åŒ¹é…ï¼ˆå¦‚ "V5"ï¼‰æŸ¥æ‰¾é…ç½®
                v_prefix = role_id.split("_")[0]  # æå– "V5"
                logger.info(f"ğŸ” [DEBUG] Direct match failed, trying prefix: {v_prefix}")
                
                # éå†æ‰€æœ‰è§’è‰²é…ç½®ï¼Œæ‰¾åˆ°åŒ¹é…å‰ç¼€çš„
                for config_key in role_manager.roles.keys():
                    if config_key.startswith(v_prefix):
                        role_config = role_manager.get_role_config(config_key, rid)
                        if role_config:
                            logger.info(f"âœ… Found config using prefix match: {config_key}")
                            break
            
            logger.info(f"ğŸ” [DEBUG] role_config found: {role_config is not None}")

            if not role_config:
                logger.error(f"âŒ Role config not found for {role_id}")
                return {"error": f"Role config not found for {role_id}"}

            # ğŸ†• P0-1ä¿®å¤ï¼šæ‰§è¡Œå‰æ€»æ˜¯æ³¨å…¥è´¨é‡çº¦æŸï¼ˆæ— è®ºæ˜¯å¦é‡è¯•ï¼‰
            if quality_checklist:
                risk_level = quality_checklist.get("risk_level", "low")
                # åªå¯¹mediumå’Œhighé£é™©ä»»åŠ¡æ³¨å…¥è´¨é‡çº¦æŸ
                if risk_level in ["medium", "high"]:
                    logger.info(f"ğŸ” æ³¨å…¥è´¨é‡çº¦æŸåˆ° {role_id} (é£é™©ç­‰çº§: {risk_level}, è½®æ¬¡: {review_round}, é‡è¯•: {retry_count})")
                    original_prompt = role_config.get("system_prompt", "")
                    enhanced_prompt = QualityMonitor.inject_quality_constraints(
                        original_prompt, quality_checklist
                    )
                    role_config["system_prompt"] = enhanced_prompt
                else:
                    logger.debug(f"â­ï¸ {role_id} é£é™©ç­‰çº§ä¸º {risk_level}ï¼Œè·³è¿‡è´¨é‡çº¦æŸæ³¨å…¥")

            # ğŸ†• åŠ¨æ€æœ¬ä½“è®ºæ³¨å…¥é€»è¾‘
            # 1. åˆ¤æ–­é¡¹ç›®ç±»å‹ï¼ˆå¦‚ personal_residentialï¼‰
            project_type = state.get("project_type")
            if project_type:
                ontology_fragment = self.ontology_loader.get_ontology_by_type(project_type)
            else:
                ontology_fragment = self.ontology_loader.get_meta_framework()
            # 2. æ³¨å…¥åˆ° system_prompt å ä½ç¬¦
            if role_config and "system_prompt" in role_config:
                prompt = role_config["system_prompt"]
                if "{{DYNAMIC_ONTOLOGY_INJECTION}}" in prompt:
                    injected = prompt.replace("{{DYNAMIC_ONTOLOGY_INJECTION}}", yaml.dump(ontology_fragment, allow_unicode=True, default_flow_style=False))
                    role_config["system_prompt"] = injected
                    logger.info(f"âœ… å·²åŠ¨æ€æ³¨å…¥æœ¬ä½“è®ºç‰‡æ®µåˆ° {role_id} çš„ system_prompt")
                else:
                    logger.warning(f"âš ï¸ {role_id} çš„ system_prompt æœªåŒ…å«åŠ¨æ€æ³¨å…¥å ä½ç¬¦")

            # åˆ›å»ºä»»åŠ¡å¯¼å‘ä¸“å®¶å·¥å‚å®ä¾‹
            expert_factory = TaskOrientedExpertFactory()

            # æ„å»ºä¸Šä¸‹æ–‡
            context = self._build_context_for_expert(state)
            
            # æ„å»ºè§’è‰²å¯¹è±¡ï¼ˆåŒ…å«TaskInstructionï¼‰
            # æ³¨æ„ï¼šProjectAnalysisStateæ˜¯TypedDictï¼Œä¸èƒ½ç›´æ¥å®ä¾‹åŒ–
            # æˆ‘ä»¬ç›´æ¥ä½¿ç”¨stateä½œä¸ºä¸Šä¸‹æ–‡
            
            # ğŸ”§ ä¿®å¤v4.0ï¼šä» strategic_analysis.selected_roles ä¸­è·å– role_object
            # æ”¯æŒä¸¤ç§åŒ¹é…æ–¹å¼ï¼šç²¾ç¡®åŒ¹é… å’Œ çŸ­æ ¼å¼åŒ¹é…
            strategic_analysis = state.get("strategic_analysis", {})
            selected_roles = strategic_analysis.get("selected_roles", [])
            role_object = None
            
            # æå–å½“å‰ role_id çš„çŸ­æ ¼å¼ (e.g., "V4_è®¾è®¡ç ”ç©¶å‘˜_4-1" -> "4-1")
            current_short_id = role_id.split('_')[-1] if '_' in role_id else role_id
            
            # ğŸ” è°ƒè¯•æ—¥å¿—
            logger.debug(f"ğŸ” [TaskInstructionæŸ¥æ‰¾] role_id={role_id}, current_short_id={current_short_id}")
            logger.debug(f"ğŸ” [TaskInstructionæŸ¥æ‰¾] selected_rolesæ•°é‡={len(selected_roles)}")
            if selected_roles:
                sample_ids = [r.get("role_id", "N/A") for r in selected_roles[:3]]
                logger.debug(f"ğŸ” [TaskInstructionæŸ¥æ‰¾] å‰3ä¸ªstored_role_id: {sample_ids}")
            
            for role in selected_roles:
                stored_role_id = role.get("role_id", "")
                # æå–å­˜å‚¨çš„ role_id çš„çŸ­æ ¼å¼
                stored_short_id = stored_role_id.split('_')[-1] if '_' in stored_role_id else stored_role_id
                
                # æ”¯æŒå¤šç§åŒ¹é…æ–¹å¼ï¼š
                # 1. ç²¾ç¡®åŒ¹é… (e.g., "V4_è®¾è®¡ç ”ç©¶å‘˜_4-1" == "V4_è®¾è®¡ç ”ç©¶å‘˜_4-1")
                # 2. çŸ­æ ¼å¼åŒ¹é… (e.g., "4-1" == "4-1")
                # 3. å­˜å‚¨çš„æ˜¯çŸ­æ ¼å¼ï¼ŒæŸ¥è¯¢çš„æ˜¯å®Œæ•´æ ¼å¼
                if (stored_role_id == role_id or 
                    stored_short_id == current_short_id):
                    role_object = role
                    logger.info(f"âœ… æ‰¾åˆ° {role_id} çš„TaskInstruction (stored_role_id={stored_role_id})")
                    break
            
            if not role_object:
                # æ„å»ºé»˜è®¤role_objectï¼ˆå‘åå…¼å®¹ï¼‰
                role_object = {
                    "role_id": role_id,
                    "role_name": role_config.get("name", role_id),
                    "dynamic_role_name": role_config.get("name", role_id),
                    "task_instruction": {
                        "objective": f"åŸºäº{role_config.get('name', 'ä¸“ä¸šé¢†åŸŸ')}è¿›è¡Œæ·±åº¦åˆ†æ",
                        "deliverables": [
                            {
                                "name": "ä¸“ä¸šåˆ†ææŠ¥å‘Š",
                                "description": "åŸºäºä¸“ä¸šé¢†åŸŸæä¾›æ·±åº¦åˆ†æå’Œå»ºè®®",
                                "format": "analysis",
                                "priority": "high",
                                "success_criteria": ["åˆ†ææ·±å…¥ä¸”ä¸“ä¸š", "å»ºè®®å…·æœ‰å¯æ“ä½œæ€§"]
                            }
                        ],
                        "success_criteria": ["è¾“å‡ºç¬¦åˆä¸“ä¸šæ ‡å‡†", "å»ºè®®å…·æœ‰å®ç”¨ä»·å€¼"],
                        "constraints": [],
                        "context_requirements": []
                    }
                }
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°{role_id}çš„TaskInstructionï¼Œä½¿ç”¨é»˜è®¤ç»“æ„")

            # æ‰§è¡Œä»»åŠ¡å¯¼å‘ä¸“å®¶åˆ†æ
            try:
                expert_result = await expert_factory.execute_expert(
                    role_object=role_object,
                    context=context,
                    state=state  # ç›´æ¥ä¼ é€’state
                )
                
                # æ„å»ºå…¼å®¹çš„ç»“æœæ ¼å¼
                result_content = expert_result.get("analysis", "")
                structured_output = expert_result.get("structured_output")
                
                # è®°å½•æ‰§è¡Œä¿¡æ¯
                logger.info(f"âœ… ä»»åŠ¡å¯¼å‘ä¸“å®¶ {role_id} æ‰§è¡Œå®Œæˆ")
                if structured_output:
                    logger.info(f"ğŸ“Š ç»“æ„åŒ–è¾“å‡ºéªŒè¯æˆåŠŸ")
                else:
                    logger.warning(f"âš ï¸ ç»“æ„åŒ–è¾“å‡ºéªŒè¯å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹è¾“å‡º")
                    
            except Exception as e:
                logger.error(f"âŒ æ‰§è¡Œä»»åŠ¡å¯¼å‘ä¸“å®¶å¤±è´¥: {str(e)}")
                result_content = f"æ‰§è¡Œå¤±è´¥: {str(e)}"
                expert_result = {
                    "expert_id": role_id,
                    "expert_name": role_config.get("name", role_id),
                    "analysis": result_content,
                    "structured_output": None,
                    "error": True
                }

            # ğŸ†• æ‰§è¡Œåï¼šå¿«é€ŸéªŒè¯
            quality_checklist = state.get("quality_checklist", {})
            if quality_checklist and result_content:
                logger.info(f"ğŸ” å¿«é€ŸéªŒè¯ {role_id} çš„è¾“å‡ºè´¨é‡")
                validation_result = QualityMonitor.quick_validation(
                    result_content, quality_checklist, role_id
                )
                
                # åˆ¤æ–­æ˜¯å¦éœ€è¦é‡è¯•
                should_retry = QualityMonitor.should_retry(validation_result)
                
                if should_retry and retry_count == 0:
                    logger.warning(f"âš ï¸ {role_id} è´¨é‡ä¸è¾¾æ ‡ï¼Œè§¦å‘é‡è¯•")
                    
                    try:
                        # ä½¿ç”¨ç›¸åŒçš„role_objectè¿›è¡Œé‡è¯•
                        state[f"retry_count_{role_id}"] = 1
                        expert_result_retry = await expert_factory.execute_expert(
                            role_object=role_object,
                            context=context,
                            state=state  # ç›´æ¥ä½¿ç”¨state
                        )
                        
                        # æ›´æ–°ç»“æœ
                        result_content = expert_result_retry.get("analysis", "")
                        expert_result = expert_result_retry
                        logger.info(f"âœ… {role_id} é‡è¯•å®Œæˆ")
                        
                    except Exception as retry_error:
                        logger.error(f"âŒ {role_id} é‡è¯•å¤±è´¥: {str(retry_error)}")
                
                # å°†éªŒè¯ç»“æœé™„åŠ åˆ°è¾“å‡º
                if 'quality_validation' not in expert_result:
                    expert_result["quality_validation"] = validation_result

            # ğŸ”§ ä¿®å¤ï¼šå°†æˆåŠŸè¿”å›é€»è¾‘ç§»å‡ºquality_checklistæ¡ä»¶å—
            logger.info(f"âœ… Dynamic agent {role_id} completed successfully")
            logger.debug(f"Result length: {len(result_content)} characters")

            # ğŸ”§ å­—æ®µåä¿®å¤ï¼šä½¿ç”¨ structured_outputï¼ˆè€Œé parsed_resultï¼‰
            structured_output = expert_result.get("structured_output", {})
            if structured_output:
                structured_data = structured_output
                # ç¡®ä¿contentå­—æ®µå­˜åœ¨
                if "content" not in structured_data:
                    structured_data["content"] = result_content
                
                # ğŸ”¥ Debug: æ£€æŸ¥challenge_flagsæ˜¯å¦å­˜åœ¨
                if "challenge_flags" in structured_data:
                    challenge_count = len(structured_data["challenge_flags"])
                    logger.info(f"ğŸ”¥ [DEBUG] {role_id} çš„ structured_data åŒ…å« {challenge_count} ä¸ª challenge_flags")
                else:
                    logger.debug(f"â„¹ï¸ [DEBUG] {role_id} çš„ structured_data ä¸åŒ…å« challenge_flags")
            else:
                structured_data = {"content": result_content}
                logger.debug(f"â„¹ï¸ [DEBUG] {role_id} æ—  structured_outputï¼Œä½¿ç”¨é»˜è®¤ structured_data")

            # è¿”å›ç»“æœ - ä½¿ç”¨ role_id ä½œä¸º key
            role_name = role_config.get("name", "æœªçŸ¥è§’è‰²")
            dynamic_role_name = role_object.get("dynamic_role_name", role_name)

            # ğŸ”§ ä¿®å¤ï¼ˆ2025-11-30ï¼‰ï¼šdetail æ˜¾ç¤ºå®Œæ•´çš„ role_idï¼Œä¾¿äºå‰ç«¯åŒæ­¥æ˜¾ç¤º
            # æ ¼å¼ï¼šä¸“å®¶ã€è§’è‰²IDã€‘æ‰§è¡Œä¸­
            detail_message = f"ä¸“å®¶ã€{role_id}ã€‘å®Œæˆåˆ†æ"

            # åªæœ‰åœ¨result_contentçœŸæ­£ä¸ºç©ºæ—¶æ‰è¿”å›é”™è¯¯
            if not result_content:
                logger.warning(f"âš ï¸ Dynamic agent {role_id} returned no results")
                return {"error": f"No results from {role_id}"}

            # ğŸš€ P4ä¼˜åŒ–ï¼šå•ä¸ªä¸“å®¶å®Œæˆå³æ¨é€ç»“æœï¼ˆæ¸è¿›å¼å±•ç¤ºï¼‰
            # è·å–session_idç”¨äºWebSocketæ¨é€
            session_id = state.get("session_id")
            if session_id:
                try:
                    # å¯¼å…¥broadcastå‡½æ•°
                    from intelligent_project_analyzer.api.server import broadcast_to_websockets

                    # æ¨é€ä¸“å®¶ç»“æœ
                    import asyncio
                    asyncio.create_task(broadcast_to_websockets(session_id, {
                        "type": "agent_result",
                        "role_id": role_id,
                        "role_name": role_name,
                        "dynamic_role_name": dynamic_role_name,
                        "analysis": result_content,
                        "structured_data": structured_data,
                        "timestamp": datetime.now().isoformat()
                    }))
                    logger.info(f"ğŸ“¤ [Progressive] å·²æ¨é€ä¸“å®¶ç»“æœ: {role_id} ({dynamic_role_name})")
                except Exception as broadcast_error:
                    logger.warning(f"âš ï¸ WebSocketæ¨é€å¤±è´¥: {broadcast_error}")

            return {
                "agent_results": {
                    role_id: {
                        "role_id": role_id,
                        "role_name": role_name,
                        "analysis": result_content,
                        "confidence": 0.8,  # é»˜è®¤ç½®ä¿¡åº¦
                        "structured_data": structured_data  # ğŸ†• ä½¿ç”¨å®Œæ•´çš„parsed_result
                    }
                },
                "detail": detail_message
            }

        except Exception as e:
            logger.error(f"Agent execution failed: {e}")
            import traceback
            traceback.print_exc()
            return {"error": str(e)}

    def _batch_executor_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        æ‰¹æ¬¡æ‰§è¡Œå™¨èŠ‚ç‚¹ - éªŒè¯å¹¶å‡†å¤‡æ‰§è¡Œå½“å‰æ‰¹æ¬¡

        âœ… ä¿®å¤ï¼šèŠ‚ç‚¹å‡½æ•°åªè¿”å›çŠ¶æ€æ›´æ–°ï¼ˆDictï¼‰ï¼ŒSend å¯¹è±¡çš„åˆ›å»ºäº¤ç»™æ¡ä»¶è¾¹å‡½æ•°
        âœ… æ–°å¢ï¼šæ”¯æŒ specific_agents_to_runï¼ˆå®¡æ ¸è§¦å‘çš„é‡æ–°æ‰§è¡Œï¼‰

        è¿™æ˜¯åŠ¨æ€æ‰¹æ¬¡æ‰§è¡Œçš„æ ¸å¿ƒèŠ‚ç‚¹ï¼Œæ›¿ä»£äº†ç¡¬ç¼–ç çš„ first_batch_agent å’Œ second_batch_agentã€‚

        å·¥ä½œåŸç†:
        1. æ£€æŸ¥æ˜¯å¦æœ‰ specific_agents_to_runï¼ˆå®¡æ ¸è§¦å‘çš„é‡æ–°æ‰§è¡Œï¼‰
        2. å¦‚æœæœ‰ï¼Œåˆ›å»ºä¸€ä¸ªä¸´æ—¶æ‰¹æ¬¡åªåŒ…å«æŒ‡å®šçš„ä¸“å®¶
        3. å¦åˆ™ï¼Œä» state["execution_batches"] è¯»å–æ­£å¸¸æ‰¹æ¬¡
        4. è¿”å›ç©ºå­—å…¸ï¼ˆçŠ¶æ€æ— éœ€æ›´æ–°ï¼‰
        5. åç»­ç”±æ¡ä»¶è¾¹å‡½æ•° _create_batch_sends åˆ›å»º Send ä»»åŠ¡

        çŠ¶æ€è¾“å…¥:
        - specific_agents_to_run: List[str] - å®¡æ ¸æŒ‡å®šéœ€è¦é‡æ–°æ‰§è¡Œçš„ä¸“å®¶ï¼ˆä¼˜å…ˆï¼‰
        - execution_batches: List[List[str]] - æ‰€æœ‰æ‰¹æ¬¡çš„è§’è‰²åˆ—è¡¨
        - current_batch: int - å½“å‰æ‰¹æ¬¡ç¼–å·ï¼ˆ1-basedï¼‰
        - agent_results: Dict - å·²å®Œæˆä¸“å®¶çš„ç»“æœ

        è¿”å›:
        - Dict[str, Any] - çŠ¶æ€æ›´æ–°ï¼ˆé€šå¸¸ä¸ºç©ºå­—å…¸ï¼‰
        """
        try:
            # âœ… ä¼˜å…ˆæ£€æŸ¥æ˜¯å¦æœ‰å®¡æ ¸è§¦å‘çš„é‡æ–°æ‰§è¡Œ
            specific_agents = state.get("specific_agents_to_run", [])
            
            if specific_agents:
                logger.info(f"ğŸ”„ å®¡æ ¸è§¦å‘çš„é‡æ–°æ‰§è¡Œ: {len(specific_agents)} ä¸ªä¸“å®¶")
                logger.info(f"   ä¸“å®¶åˆ—è¡¨: {specific_agents}")
                
                # åˆ›å»ºä¸´æ—¶æ‰¹æ¬¡
                return {
                    "execution_batches": [specific_agents],
                    "current_batch": 1,
                    "total_batches": 1,
                    "is_rerun": True,
                    "skip_role_review": True,
                    "skip_task_review": True
                }
            
            # æ­£å¸¸æ‰¹æ¬¡æ‰§è¡Œ
            batches = state.get("execution_batches", [])
            current_batch = state.get("current_batch", 1)
            total_batches = len(batches)

            logger.info(f"ğŸ¯ Batch executor: preparing batch {current_batch}/{total_batches}")

            # éªŒè¯æ‰¹æ¬¡ç¼–å·
            if current_batch < 1 or current_batch > total_batches:
                logger.error(f"âŒ Invalid batch number: {current_batch} (total: {total_batches})")
                return {"errors": [f"Invalid batch number: {current_batch}"]}

            if not batches:
                logger.error("âŒ No execution_batches found in state")
                return {"errors": ["No execution batches defined"]}

            # è·å–å½“å‰æ‰¹æ¬¡çš„è§’è‰²åˆ—è¡¨ï¼ˆ0-indexedï¼‰
            batch_roles = batches[current_batch - 1]
            display_roles = [format_role_display_name(r) for r in batch_roles]
            logger.info(f"ğŸ“‹ Batch {current_batch} contains {len(batch_roles)} agents: {display_roles}")

            # âœ… åªè¿”å›çŠ¶æ€æ›´æ–°ï¼Œä¸åˆ›å»º Send å¯¹è±¡
            return {"detail": f"æ‰¹æ¬¡ {current_batch}/{total_batches}: {len(batch_roles)} ä½ä¸“å®¶"}

        except Exception as e:
            logger.error(f"âŒ Batch executor failed: {e}")
            import traceback
            traceback.print_exc()
            return {"errors": [str(e)]}

    def _create_batch_sends(self, state: ProjectAnalysisState) -> List[Send]:
        """
        æ‰¹æ¬¡ Send åˆ›å»ºå™¨ - æ¡ä»¶è¾¹å‡½æ•°ï¼Œè¿”å›å¹¶è¡Œä»»åŠ¡åˆ—è¡¨

        âœ… æ–°å¢ï¼šä¸“é—¨è´Ÿè´£åˆ›å»º Send å¯¹è±¡çš„æ¡ä»¶è¾¹å‡½æ•°ï¼Œé…åˆ _batch_executor_node ä½¿ç”¨
        âœ… ä¿®å¤ï¼šæ”¯æŒå®¡æ ¸è§¦å‘çš„é‡æ–°æ‰§è¡Œï¼ˆspecific_agents_to_runï¼‰

        è¿™ä¸ªå‡½æ•°ä½œä¸ºæ¡ä»¶è¾¹å‡½æ•°ï¼Œæ ¹æ®å½“å‰æ‰¹æ¬¡åˆ›å»ºå¯¹åº”çš„ Send ä»»åŠ¡åˆ—è¡¨ã€‚
        LangGraph ä¼šè‡ªåŠ¨å¹¶è¡Œæ‰§è¡Œè¿™äº›ä»»åŠ¡ã€‚

        å·¥ä½œåŸç†:
        1. ä» state["execution_batches"] è¯»å–æ‰€æœ‰æ‰¹æ¬¡
        2. æ ¹æ® state["current_batch"] ç¡®å®šå½“å‰è¦æ‰§è¡Œçš„æ‰¹æ¬¡
        3. ä¸ºæ‰¹æ¬¡ä¸­çš„æ¯ä¸ªè§’è‰²åˆ›å»º Send å¯¹è±¡
        4. è¿”å› Send åˆ—è¡¨ï¼ŒLangGraph è‡ªåŠ¨å¹¶è¡Œè°ƒåº¦

        çŠ¶æ€è¾“å…¥:
        - execution_batches: List[List[str]] - æ‰€æœ‰æ‰¹æ¬¡çš„è§’è‰²åˆ—è¡¨
        - current_batch: int - å½“å‰æ‰¹æ¬¡ç¼–å·ï¼ˆ1-basedï¼‰
        - is_rerun: bool - æ˜¯å¦ä¸ºå®¡æ ¸è§¦å‘çš„é‡æ–°æ‰§è¡Œ
        - review_feedback: Dict - å®¡æ ¸åé¦ˆï¼ˆå¦‚æœæ˜¯é‡æ–°æ‰§è¡Œï¼‰

        è¿”å›:
        - List[Send] - å¹¶è¡Œä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ª Send æ‰§è¡Œä¸€ä¸ªæ™ºèƒ½ä½“

        æ³¨æ„:
        - è¿™ä¸ªå‡½æ•°å¿…é¡»æ˜¯æ¡ä»¶è¾¹å‡½æ•°ï¼Œä¸èƒ½æ˜¯èŠ‚ç‚¹å‡½æ•°
        - é€šè¿‡ add_conditional_edges é…ç½®åˆ°å›¾ä¸­
        """
        batches = state.get("execution_batches") or []
        current_batch = state.get("current_batch", 1)
        is_rerun = state.get("is_rerun", False)
        review_feedback = state.get("review_feedback", {})

        # è¾¹ç•Œæ£€æŸ¥
        if not batches:
            logger.warning(f"âš ï¸ No batches found, batches is None or empty")
            return []
        
        if current_batch < 1 or current_batch > len(batches):
            logger.warning(
                f"âš ï¸ No valid batch to execute: current={current_batch}, total={len(batches)}"
            )
            return []

        # è·å–å½“å‰æ‰¹æ¬¡çš„è§’è‰²åˆ—è¡¨ï¼ˆ0-indexedï¼‰
        batch_roles = batches[current_batch - 1]
        
        if is_rerun:
            display_roles = [format_role_display_name(r) for r in batch_roles]
            logger.info(f"ğŸ”„ åˆ›å»ºé‡æ–°æ‰§è¡Œä»»åŠ¡: {len(batch_roles)} ä¸ªä¸“å®¶ {display_roles}")
        else:
            display_roles = [format_role_display_name(r) for r in batch_roles]
            logger.info(f"ğŸ“‹ åˆ›å»ºæ‰¹æ¬¡ {current_batch} ä»»åŠ¡: {len(batch_roles)} ä¸ªä¸“å®¶ {display_roles}")

        # åˆ›å»º Send å¯¹è±¡åˆ—è¡¨
        send_list = []
        for role_id in batch_roles:
            # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“åˆ›å»ºç‹¬ç«‹çš„çŠ¶æ€å‰¯æœ¬
            agent_state = dict(state)
            agent_state["role_id"] = role_id
            agent_state["execution_batch"] = f"batch_{current_batch}"
            agent_state["current_stage"] = AnalysisStage.PARALLEL_ANALYSIS.value
            agent_state["is_rerun"] = is_rerun
            
            # âœ… å¦‚æœæ˜¯é‡æ–°æ‰§è¡Œï¼Œæ·»åŠ é’ˆå¯¹è¯¥ä¸“å®¶çš„å®¡æ ¸åé¦ˆ
            if is_rerun and review_feedback:
                feedback_by_agent = review_feedback.get("feedback_by_agent", {})
                agent_feedback = feedback_by_agent.get(role_id, {})
                if agent_feedback:
                    agent_state["review_feedback_for_agent"] = agent_feedback
                    logger.info(f"   {role_id}: é™„åŠ å®¡æ ¸åé¦ˆ ({len(agent_feedback.get('issues', []))} ä¸ªé—®é¢˜)")

            send_list.append(Send("agent_executor", agent_state))
            logger.debug(f"  â¤ Created Send for: {role_id}")

        logger.info(f"âœ… Created {len(send_list)} parallel tasks for batch {current_batch}")
        return send_list

    def _intermediate_aggregator_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        æ‰¹æ¬¡èšåˆèŠ‚ç‚¹ - éªŒè¯å½“å‰æ‰¹æ¬¡çš„æ‰§è¡Œç»“æœï¼ˆé€šç”¨ç‰ˆï¼Œ2025-11-18é‡æ„ï¼‰

        åŠŸèƒ½:
        1. éªŒè¯å½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰ä¸“å®¶æ˜¯å¦æˆåŠŸå®Œæˆ
        2. è®°å½•è¯¦ç»†æ—¥å¿—
        3. æ ‡è®°å½“å‰æ‰¹æ¬¡ä¸ºå·²å®Œæˆ
        4. ä¸ºä¸‹ä¸€æ‰¹æ¬¡å‡†å¤‡ä¾èµ–æ•°æ®
        5. è¿”å›çŠ¶æ€æ›´æ–°ï¼Œè§¦å‘ä¸‹ä¸€æ‰¹æ¬¡æˆ–è¿›å…¥å®¡æ ¸

        é‡æ„è¯´æ˜ï¼š
        - ä»å›ºå®šçš„V3/V4/V5æ£€æŸ¥æ”¹ä¸ºåŸºäºexecution_batchesçš„åŠ¨æ€æ£€æŸ¥
        - æ”¯æŒä»»æ„æ•°é‡çš„æ‰¹æ¬¡ï¼ˆ1-Næ‰¹ï¼‰
        - é€šè¿‡current_batchè·Ÿè¸ªå½“å‰æ‰¹æ¬¡
        
        ğŸ”§ P1ä¿®å¤ï¼ˆ2025-11-25ï¼‰ï¼š
        - ç­‰å¾…å½“å‰æ‰¹æ¬¡æ‰€æœ‰agentå®Œæˆåå†æ‰§è¡Œèšåˆ
        - é€šè¿‡æ£€æŸ¥agent_resultsç¡®ä¿ä¸ä¼šè¿‡æ—©è§¦å‘
        
        ğŸ”§ N2ä¼˜åŒ–ï¼ˆ2025-11-25ï¼‰ï¼š
        - LangGraph Send APIä¼šåœ¨æ¯ä¸ªå¹¶è¡Œä»»åŠ¡å®Œæˆæ—¶è§¦å‘æ­¤èŠ‚ç‚¹ï¼ˆé¢„æœŸè¡Œä¸ºï¼‰
        - æœ¬èŠ‚ç‚¹æ£€æŸ¥æ‰€æœ‰ä»»åŠ¡æ˜¯å¦å®Œæˆï¼Œæœªå®Œæˆæ—¶è¿”å›ç©ºå­—å…¸ç­‰å¾…
        - è¿™æ˜¯LangGraphå¹¶è¡Œæ‰§è¡Œçš„æ ‡å‡†"è½®è¯¢æ¨¡å¼"
        """
        try:
            agent_results = state.get("agent_results", {})
            batches = state.get("execution_batches", [])
            current_batch = state.get("current_batch", 1)
            total_batches = state.get("total_batches", len(batches))

            logger.info(f"ğŸ“Š å½“å‰æ‰¹æ¬¡: {current_batch}/{total_batches}")

            if not batches:
                logger.warning("æœªæ‰¾åˆ° execution_batchesï¼Œæ— æ³•éªŒè¯æ‰¹æ¬¡å®Œæˆæƒ…å†µ")
                # é™çº§åˆ°æ—§é€»è¾‘ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰
                return self._intermediate_aggregator_node_legacy(state)

            # è·å–å½“å‰æ‰¹æ¬¡çš„è§’è‰²åˆ—è¡¨
            if current_batch < 1 or current_batch > len(batches):
                logger.error(f"å½“å‰æ‰¹æ¬¡ç¼–å· {current_batch} è¶…å‡ºèŒƒå›´")
                return {"error": f"Invalid batch number: {current_batch}"}

            current_batch_roles = batches[current_batch - 1]

            # ğŸ”§ N2ä¼˜åŒ–ï¼šå¿«é€Ÿæ£€æŸ¥æ˜¯å¦æ‰€æœ‰agentå·²å®Œæˆï¼ˆå‡å°‘æ— æ•ˆæ—¥å¿—ï¼‰
            pending_agents = [role_id for role_id in current_batch_roles if role_id not in agent_results]
            if pending_agents:
                # âš¡ LangGraphå¹¶è¡Œæ¨¡å¼ï¼šéƒ¨åˆ†ä»»åŠ¡å®Œæˆæ—¶ä¼šè§¦å‘æ­¤èŠ‚ç‚¹ï¼Œè¿™æ˜¯é¢„æœŸè¡Œä¸º
                # è¿”å›ç©ºå­—å…¸ï¼Œç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆåå†æ¬¡è°ƒç”¨

                # ğŸš€ P2ä¼˜åŒ–ï¼šæ·»åŠ è½®è¯¢è®¡æ•°å™¨ï¼Œä¼˜åŒ–æ—¥å¿—çº§åˆ«
                poll_count_key = f"_aggregator_poll_count_batch_{current_batch}"
                poll_count = state.get(poll_count_key, 0) + 1

                # åªåœ¨ç¬¬ä¸€æ¬¡è½®è¯¢æ—¶è®°å½• info çº§åˆ«æ—¥å¿—ï¼Œåç»­ä½¿ç”¨ debug çº§åˆ«
                if poll_count == 1:
                    logger.info(f"â³ [Polling] æ‰¹æ¬¡ {current_batch} å¼€å§‹ç­‰å¾…: {len(pending_agents)}/{len(current_batch_roles)} æœªå®Œæˆ")
                else:
                    logger.debug(f"â³ [Polling #{poll_count}] æ‰¹æ¬¡ {current_batch} ç­‰å¾…ä¸­: {len(pending_agents)}/{len(current_batch_roles)} æœªå®Œæˆ")

                # è¿”å›æ›´æ–°çš„è½®è¯¢è®¡æ•°
                return {poll_count_key: poll_count}
            
            # âœ… æ‰€æœ‰agentå·²å®Œæˆï¼Œå¼€å§‹è¯¦ç»†èšåˆ
            # ğŸš€ P2ä¼˜åŒ–ï¼šè®°å½•æ€»è½®è¯¢æ¬¡æ•°
            poll_count_key = f"_aggregator_poll_count_batch_{current_batch}"
            total_polls = state.get(poll_count_key, 0)
            if total_polls > 0:
                logger.info(f"âœ… [Aggregator] æ‰¹æ¬¡ {current_batch}/{total_batches} æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ˆç»è¿‡ {total_polls} æ¬¡è½®è¯¢ï¼‰ï¼Œå¼€å§‹èšåˆ")
            else:
                logger.info(f"âœ… [Aggregator] æ‰¹æ¬¡ {current_batch}/{total_batches} æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œå¼€å§‹èšåˆ")

            # æ£€æŸ¥å®Œæˆæƒ…å†µ
            completed_agents = []
            failed_agents = []

            for role_id in current_batch_roles:
                if role_id in agent_results:
                    result = agent_results[role_id]
                    confidence = result.get("confidence", 0)
                    has_error = "error" in result

                    if has_error:
                        failed_agents.append(role_id)
                        logger.warning(f"{role_id} completed with error: {result.get('error')}")
                    else:
                        completed_agents.append(role_id)
                        logger.info(f"âœ… {role_id} completed successfully (confidence: {confidence:.2%})")
                else:
                    failed_agents.append(role_id)
                    logger.warning(f"âš ï¸ {role_id} not found in results")

            # è®°å½•æ±‡æ€»ä¿¡æ¯
            logger.info(f"æ‰¹æ¬¡ {current_batch} æ±‡æ€»: {len(completed_agents)}/{len(current_batch_roles)} æˆåŠŸå®Œæˆ")
            if completed_agents:
                logger.info(f"âœ… å·²å®Œæˆ: {', '.join(completed_agents)}")
            if failed_agents:
                logger.warning(f"âŒ å¤±è´¥/ç¼ºå¤±: {', '.join(failed_agents)}")

            # å‡†å¤‡ä¾èµ–æ•°æ®æ‘˜è¦
            dependency_summary = {
                "batch_number": current_batch,
                "batch_completed": len(completed_agents) == len(current_batch_roles),
                "completed_count": len(completed_agents),
                "total_count": len(current_batch_roles),
                "completed_agents": completed_agents,
                "failed_agents": failed_agents,
                "timestamp": datetime.now().isoformat()
            }

            # æ›´æ–°å·²å®Œæˆæ‰¹æ¬¡åˆ—è¡¨
            completed_batches = state.get("completed_batches", [])
            if current_batch not in completed_batches:
                completed_batches_updated = completed_batches + [current_batch]
            else:
                completed_batches_updated = completed_batches

            logger.info(f"âœ… æ‰¹æ¬¡ {current_batch} èšåˆå®Œæˆ")
            logger.info(f"ğŸ”„ å·²å®Œæˆæ‰¹æ¬¡: {completed_batches_updated}")

            # è¿”å›çŠ¶æ€æ›´æ–°ï¼ˆä¸‹ä¸€æ­¥ç”±second_batch_strategy_reviewå†³å®šè·¯ç”±ï¼‰

            # ğŸ”§ ä¿®å¤ï¼ˆ2025-11-30ï¼‰ï¼šæ„é€ æ›´å…·ä½“çš„detailæ¶ˆæ¯ï¼Œæ˜¾ç¤ºåˆšå®Œæˆçš„ä¸“å®¶
            if completed_agents:
                # å¦‚æœåªæœ‰1ä¸ªä¸“å®¶ï¼Œæ˜¾ç¤ºå…·ä½“åç§°ï¼›å¦‚æœå¤šä¸ªï¼Œæ˜¾ç¤ºæ•°é‡å’Œåˆ—è¡¨
                if len(completed_agents) == 1:
                    detail_message = f"ä¸“å®¶ã€{completed_agents[0]}ã€‘åˆ†æå®Œæˆ"
                else:
                    agent_list = "ã€".join([agent.split("_")[-1] if "_" in agent else agent for agent in completed_agents])
                    detail_message = f"æ‰¹æ¬¡ {current_batch} å®Œæˆï¼š{agent_list} ç­‰ {len(completed_agents)} ä½ä¸“å®¶"
            else:
                detail_message = f"æ‰¹æ¬¡ {current_batch} æ‰§è¡Œä¸­..."

            return {
                "dependency_summary": dependency_summary,
                "completed_batches": completed_batches_updated,
                "updated_at": datetime.now().isoformat(),
                "detail": detail_message
            }

        except Exception as e:
            logger.error(f"Intermediate aggregator node failed: {e}")
            import traceback
            traceback.print_exc()
            return {
                "error": str(e),
                "updated_at": datetime.now().isoformat()
            }

    def _intermediate_aggregator_node_legacy(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        æ—§ç‰ˆæ‰¹æ¬¡èšåˆé€»è¾‘ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰

        å½“stateä¸­æ²¡æœ‰execution_batchesæ—¶ä½¿ç”¨æ­¤æ–¹æ³•
        """
        logger.warning("ä½¿ç”¨æ—§ç‰ˆintermediate_aggregatoré€»è¾‘")

        agent_results = state.get("agent_results", {})
        active_agents = state.get("active_agents", [])

        # ç¡¬ç¼–ç æ£€æŸ¥V3/V4/V5
        first_batch_roles = [
            role_id for role_id in active_agents
            if role_id.startswith("V3_") or role_id.startswith("V4_") or role_id.startswith("V5_")
        ]

        completed_agents = []
        failed_agents = []

        for role_id in first_batch_roles:
            if role_id in agent_results:
                result = agent_results[role_id]
                has_error = "error" in result
                if has_error:
                    failed_agents.append(role_id)
                else:
                    completed_agents.append(role_id)
            else:
                failed_agents.append(role_id)

        dependency_summary = {
            "first_batch_completed": len(completed_agents) == len(first_batch_roles),
            "completed_count": len(completed_agents),
            "total_count": len(first_batch_roles),
            "completed_agents": completed_agents,
            "failed_agents": failed_agents,
            "timestamp": datetime.now().isoformat()
        }

        return {
            "dependency_summary": dependency_summary,
            "updated_at": datetime.now().isoformat()
        }

    # ============================================================================
    # ğŸ†• v3.5 Expert Collaboration: Challenge Detection Node
    # ============================================================================
    
    def _detect_challenges_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        ğŸ†• v3.5 æŒ‘æˆ˜æ£€æµ‹èŠ‚ç‚¹ - æ£€æµ‹ä¸“å®¶è¾“å‡ºä¸­çš„challenge_flagså¹¶å¤„ç†
        
        å·¥ä½œæµç¨‹:
        1. ä»stateä¸­æå–æ‰€æœ‰ä¸“å®¶çš„è¾“å‡º
        2. è°ƒç”¨detect_and_handle_challenges_node()æ£€æµ‹æŒ‘æˆ˜
        3. æ›´æ–°stateåŒ…å«æŒ‘æˆ˜æ£€æµ‹å’Œå¤„ç†ç»“æœ
        4. è®¾ç½®requires_feedback_loopæ ‡å¿—
        
        çŠ¶æ€è¾“å…¥:
        - batch_results: Dict - å„æ‰¹æ¬¡ä¸“å®¶çš„è¾“å‡º
        - (å¯é€‰) v2_output, v3_outputç­‰ç›´æ¥å­—æ®µ
        
        çŠ¶æ€è¾“å‡º:
        - challenge_detection: Dict - æŒ‘æˆ˜æ£€æµ‹ç»“æœ
        - challenge_handling: Dict - æŒ‘æˆ˜å¤„ç†ç»“æœ
        - has_active_challenges: bool - æ˜¯å¦æœ‰æ´»è·ƒæŒ‘æˆ˜
        - requires_feedback_loop: bool - æ˜¯å¦éœ€è¦åé¦ˆå¾ªç¯
        - feedback_loop_reason: str - åé¦ˆå¾ªç¯åŸå› ï¼ˆå¦‚æœ‰ï¼‰
        
        Returns:
            æ›´æ–°çš„çŠ¶æ€å­—å…¸ï¼ˆåªåŒ…å«æ–°å¢/ä¿®æ”¹çš„å­—æ®µï¼‰
        """
        logger.info("ğŸ” [v3.5] å¼€å§‹æ£€æµ‹ä¸“å®¶æŒ‘æˆ˜...")
        
        try:
            # è°ƒç”¨æ ¸å¿ƒæŒ‘æˆ˜æ£€æµ‹å‡½æ•°ï¼ˆç°åœ¨åªè¿”å›æ–°å¢å­—æ®µï¼‰
            updated_state = detect_and_handle_challenges_node(state)
            
            # è®°å½•æ£€æµ‹ç»“æœ
            if updated_state.get("has_active_challenges"):
                challenge_count = len(updated_state.get("challenge_detection", {}).get("challenges", []))
                logger.info(f"ğŸ”¥ [v3.5] æ£€æµ‹åˆ° {challenge_count} ä¸ªä¸“å®¶æŒ‘æˆ˜")
                
                if updated_state.get("requires_feedback_loop"):
                    logger.warning("âš ï¸ [v3.5] éœ€è¦å¯åŠ¨åé¦ˆå¾ªç¯å›è®¿éœ€æ±‚åˆ†æå¸ˆ")
            else:
                logger.info("âœ… [v3.5] æœªæ£€æµ‹åˆ°æŒ‘æˆ˜ï¼Œä¸“å®¶æ¥å—éœ€æ±‚åˆ†æå¸ˆçš„æ´å¯Ÿ")
            
            return updated_state
            
        except Exception as e:
            logger.error(f"âŒ [v3.5] æŒ‘æˆ˜æ£€æµ‹å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            # è¿”å›å®‰å…¨çš„é»˜è®¤å€¼
            return {
                "has_active_challenges": False,
                "requires_feedback_loop": False,
                "challenge_detection": {"has_challenges": False, "challenges": []},
                "challenge_handling": {"requires_revisit": False}
            }
            # å¤±è´¥æ—¶è¿”å›åŸçŠ¶æ€ï¼Œä¸å½±å“å·¥ä½œæµç»§ç»­
            return {
                **state,
                "has_active_challenges": False,
                "requires_feedback_loop": False,
                "challenge_detection_error": str(e)
            }
    
    def _route_after_challenge_detection(self, state: ProjectAnalysisState) -> str:
        """
        ğŸ†• v3.5 æŒ‘æˆ˜æ£€æµ‹åçš„è·¯ç”±å†³ç­–
        
        æ ¹æ®æŒ‘æˆ˜æ£€æµ‹ç»“æœå†³å®šä¸‹ä¸€æ­¥:
        - å¦‚æœrequires_manual_review=True â†’ "manual_review" (>3ä¸ªmust_fixé—®é¢˜)
        - å¦‚æœrequires_client_review=True â†’ "analysis_review" (äº¤ç”²æ–¹è£å†³)
        - å¦‚æœrequires_feedback_loop=True â†’ "revisit_requirements" (å›è®¿éœ€æ±‚åˆ†æå¸ˆ)
        - å¦åˆ™ â†’ "continue_workflow" (ç»§ç»­æ­£å¸¸æµç¨‹)
        
        ä¼˜å…ˆçº§: manual_review > escalate > revisit_ra > continue
        
        Args:
            state: å½“å‰å·¥ä½œæµçŠ¶æ€
            
        Returns:
            è·¯ç”±ç›®æ ‡: "manual_review" | "analysis_review" | "revisit_requirements" | "continue_workflow"
        """
        # ğŸ†• æœ€é«˜ä¼˜å…ˆçº§ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦äººå·¥å®¡æ ¸ï¼ˆ>3ä¸ªmust_fixï¼‰
        requires_manual_review = state.get("requires_manual_review", False)
        if requires_manual_review:
            issues_count = state.get("critical_issues_count", 0)
            logger.error(f"ğŸš¨ [Manual Review] å‘ç°{issues_count}ä¸ªä¸¥é‡è´¨é‡é—®é¢˜ï¼ˆè¶…è¿‡é˜ˆå€¼ï¼‰ï¼Œè§¦å‘äººå·¥å®¡æ ¸")
            return "manual_review"
        
        # ğŸ†• ä¼˜å…ˆæ£€æŸ¥æ˜¯å¦éœ€è¦ç”²æ–¹è£å†³ï¼ˆescalateé—­ç¯ï¼‰
        requires_client_review = state.get("requires_client_review", False)
        if requires_client_review:
            escalated_count = len(state.get("escalated_challenges", []))
            logger.warning(f"ğŸš¨ [v3.5 Escalate] {escalated_count}ä¸ªæŒ‘æˆ˜éœ€è¦ç”²æ–¹è£å†³ï¼Œè·¯ç”±åˆ°å®¡æ ¸èŠ‚ç‚¹")
            return "analysis_review"
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å›è®¿éœ€æ±‚åˆ†æå¸ˆï¼ˆrevisit_raé—­ç¯ï¼‰
        requires_feedback = state.get("requires_feedback_loop", False)
        if requires_feedback:
            reason = state.get("feedback_loop_reason", "ä¸“å®¶æŒ‘æˆ˜éœ€è¦æ¾„æ¸…")
            logger.info(f"ğŸ”„ [v3.5] å¯åŠ¨åé¦ˆå¾ªç¯: {reason}")
            return "revisit_requirements"
        
        # é»˜è®¤ç»§ç»­æ­£å¸¸æµç¨‹
        logger.info("â¡ï¸ [v3.5] ç»§ç»­æ­£å¸¸å·¥ä½œæµ")
        return "continue_workflow"

    def _batch_router_node(self, state: ProjectAnalysisState) -> Command:
        """
        æ‰¹æ¬¡è·¯ç”±å™¨èŠ‚ç‚¹ - å†³å®šä¸‹ä¸€æ­¥æ‰§è¡Œä»€ä¹ˆ

        è¿™æ˜¯åŠ¨æ€æ‰¹æ¬¡æ‰§è¡Œçš„å…³é”®è·¯ç”±èŠ‚ç‚¹ï¼Œæ›¿ä»£äº†ç¡¬ç¼–ç çš„è¾¹è¿æ¥ã€‚

        å·¥ä½œåŸç†:
        1. æ£€æŸ¥æ˜¯å¦ä¸ºå®¡æ ¸è§¦å‘çš„é‡æ–°æ‰§è¡Œ
        2. å¦‚æœæ˜¯é‡æ–°æ‰§è¡Œä¸”skip_second_review=True â†’ è·³è¿‡å®¡æ ¸ï¼Œç›´æ¥è¿›å…¥åç»­æµç¨‹
        3. å¦‚æœæ˜¯é‡æ–°æ‰§è¡Œä¸”éœ€è¦å®¡æ ¸ â†’ è¿”å›åˆ†æå®¡æ ¸
        4. æ£€æŸ¥å½“å‰æ‰¹æ¬¡å’Œæ€»æ‰¹æ¬¡æ•°
        5. å¦‚æœè¿˜æœ‰ä¸‹ä¸€æ‰¹æ¬¡ â†’ å¢åŠ æ‰¹æ¬¡å·å¹¶è·¯ç”±åˆ°æ‰¹æ¬¡ç­–ç•¥å®¡æ ¸
        6. å¦‚æœæ‰€æœ‰æ‰¹æ¬¡å®Œæˆ â†’ è·¯ç”±åˆ°åˆ†æå®¡æ ¸

        çŠ¶æ€è¾“å…¥:
        - is_rerun: bool - æ˜¯å¦ä¸ºå®¡æ ¸è§¦å‘çš„é‡æ–°æ‰§è¡Œ
        - skip_second_review: bool - æ•´æ”¹åæ˜¯å¦è·³è¿‡äºŒæ¬¡å®¡æ ¸
        - current_batch: int - å½“å‰å·²å®Œæˆçš„æ‰¹æ¬¡ç¼–å·
        - total_batches: int - æ€»æ‰¹æ¬¡æ•°
        - dependency_summary: Dict - å½“å‰æ‰¹æ¬¡çš„å®Œæˆæ‘˜è¦

        è¿”å›:
        - Command(goto="detect_challenges") - å¦‚æœæ˜¯æ•´æ”¹åè·³è¿‡å®¡æ ¸
        - Command(goto="analysis_review") - å¦‚æœéœ€è¦å®¡æ ¸
        - Command(goto="batch_strategy_review") - å¦‚æœè¿˜æœ‰ä¸‹ä¸€æ‰¹æ¬¡
        """
        try:
            is_rerun = state.get("is_rerun", False)
            skip_second_review = state.get("skip_second_review", False)
            current_batch = state.get("current_batch", 1)
            batches = state.get("execution_batches", [])
            total_batches = state.get("total_batches", len(batches))
            dependency_summary = state.get("dependency_summary", {})

            logger.info(f"ğŸ”€ Batch router: current={current_batch}, total={total_batches}, is_rerun={is_rerun}, skip_second_review={skip_second_review}")

            # âœ… ä¼˜å…ˆæ£€æŸ¥ï¼šå¦‚æœæ˜¯å®¡æ ¸è§¦å‘çš„é‡æ–°æ‰§è¡Œï¼ˆæ•´æ”¹ï¼‰
            if is_rerun:
                # ğŸ¯ å…³é”®æ”¹è¿›ï¼šæ£€æŸ¥æ˜¯å¦è·³è¿‡äºŒæ¬¡å®¡æ ¸
                if skip_second_review:
                    logger.info("âœ… ä¸“å®¶æ•´æ”¹å®Œæˆï¼Œè·³è¿‡äºŒæ¬¡å®¡æ ¸ï¼Œç›´æ¥è¿›å…¥æŒ‘æˆ˜æ£€æµ‹å’ŒæŠ¥å‘Šç”Ÿæˆ")
                    return Command(
                        update={
                            "is_rerun": False,  # æ¸…é™¤æ ‡è®°
                            "skip_second_review": False,  # æ¸…é™¤æ ‡è®°
                            "specific_agents_to_run": [],  # æ¸…ç©º
                            "analysis_approved": True,  # æ ‡è®°ä¸ºå®¡æ ¸é€šè¿‡ï¼ˆæ•´æ”¹è§†ä¸ºé€šè¿‡ï¼‰
                        },
                        goto="detect_challenges"  # ğŸ”‘ è·³è¿‡å®¡æ ¸ï¼Œç›´æ¥è¿›å…¥æŒ‘æˆ˜æ£€æµ‹
                    )
                else:
                    # éœ€è¦äºŒæ¬¡å®¡æ ¸ï¼ˆä¸€èˆ¬ä¸ä¼šèµ°åˆ°è¿™é‡Œï¼Œå› ä¸ºæˆ‘ä»¬è®¾ç½®äº†skip_second_review=Trueï¼‰
                    logger.info("ğŸ”„ é‡æ–°æ‰§è¡Œå®Œæˆï¼Œè¿”å›åˆ†æå®¡æ ¸è¿›è¡Œè¯„ä¼°")
                    return Command(
                        update={
                            "is_rerun": False,  # æ¸…é™¤æ ‡è®°
                            "specific_agents_to_run": []  # æ¸…ç©º
                        },
                        goto="analysis_review"
                    )

            # éªŒè¯æ‰¹æ¬¡å®Œæˆæƒ…å†µ
            batch_completed = dependency_summary.get("batch_completed", False)
            if not batch_completed:
                logger.warning(f"âš ï¸ Batch {current_batch} not fully completed, but routing to next step")
                completed_count = dependency_summary.get("completed_count", 0)
                total_count = dependency_summary.get("total_count", 0)
                logger.warning(f"   Completed: {completed_count}/{total_count}")

            # å†³ç­–è·¯ç”±
            if current_batch < total_batches:
                # è¿˜æœ‰ä¸‹ä¸€æ‰¹æ¬¡ï¼Œå¢åŠ æ‰¹æ¬¡å·
                next_batch = current_batch + 1
                logger.info(f"â¡ï¸  Routing to next batch: {next_batch}/{total_batches}")
                logger.info(f"   Next batch will contain: {len(batches[next_batch - 1])} agents")

                return Command(
                    update={"current_batch": next_batch},
                    goto="batch_strategy_review"
                )
            else:
                # æ‰€æœ‰æ‰¹æ¬¡å®Œæˆï¼Œæ£€æŸ¥æ˜¯å¦å·²å®¡æ ¸é€šè¿‡
                logger.info(f"âœ… All {total_batches} batches completed")
                
                # âœ… ä¿®å¤ï¼šå¦‚æœå·²å®¡æ ¸é€šè¿‡ï¼ˆç¬¬1è½®è§¦å‘detect_challengesï¼‰ï¼Œä¸å†é‡å¤è§¦å‘
                if state.get("analysis_approved", False):
                    logger.info("âœ… åˆ†æå·²å®¡æ ¸é€šè¿‡ï¼Œè·³è¿‡é‡å¤å®¡æ ¸")
                    return Command(goto=END)
                
                logger.info("â¡ï¸  Routing to analysis review")
                return Command(goto="analysis_review")

        except Exception as e:
            logger.error(f"âŒ Batch router failed: {e}")
            import traceback
            traceback.print_exc()
            # å‡ºé”™æ—¶é»˜è®¤è¿›å…¥å®¡æ ¸
            return Command(goto="analysis_review")

    def _batch_strategy_review_node(self, state: ProjectAnalysisState) -> Command:
        """
        æ‰¹æ¬¡ç­–ç•¥å®¡æ ¸èŠ‚ç‚¹ - æ”¯æŒå¤šç§æ‰§è¡Œæ¨¡å¼ (v3.6ä¼˜åŒ–)

        ğŸ”„ v3.6ä¼˜åŒ– (2025-11-29): æ”¯æŒç”¨æˆ·ç¡®è®¤æ¨¡å¼
        - é»˜è®¤ï¼šæ‰‹åŠ¨ç¡®è®¤æ¨¡å¼ï¼Œç”¨æˆ·å¯æŸ¥çœ‹å¹¶æ‰¹å‡†æ¯ä¸ªæ‰¹æ¬¡
        - å¯é€‰ï¼šè‡ªåŠ¨æ‰§è¡Œæ¨¡å¼ï¼ˆé€šè¿‡execution_modeé…ç½®ï¼‰

        æ”¯æŒçš„æ‰§è¡Œæ¨¡å¼:
        - "manual": æ¯æ‰¹æ¬¡éƒ½éœ€è¦ç”¨æˆ·ç¡®è®¤ï¼ˆé»˜è®¤ï¼‰
        - "automatic": å…¨è‡ªåŠ¨æ‰§è¡Œï¼ˆæ—§æ–¹æ¡ˆCè¡Œä¸ºï¼‰
        - "preview": æ˜¾ç¤ºè®¡åˆ’åè‡ªåŠ¨æ‰§è¡Œ

        è®¾è®¡ç†å¿µ:
        - æ‰¹æ¬¡ç­–ç•¥æ˜¯æŠ€æœ¯å®ç°ç»†èŠ‚ï¼Œä½†ç”¨æˆ·åº”è¯¥æœ‰é€‰æ‹©æƒ
        - æä¾›é€æ˜åº¦å’Œæ§åˆ¶æ„Ÿï¼ŒåŒæ—¶ä¿æŒçµæ´»æ€§
        - é»˜è®¤å®‰å…¨ï¼ˆéœ€ç¡®è®¤ï¼‰ï¼Œä½†æ”¯æŒé«˜æ•ˆæ¨¡å¼ï¼ˆè‡ªåŠ¨ï¼‰

        å·¥ä½œåŸç†:
        1. è¯»å–æ‰§è¡Œæ¨¡å¼é…ç½®
        2. æ ¹æ®æ¨¡å¼é€‰æ‹©æ˜¯å¦è§¦å‘ç”¨æˆ·ç¡®è®¤
        3. æ”¶é›†æ‰¹æ¬¡ä¿¡æ¯å¹¶æ˜¾ç¤ºç»™ç”¨æˆ·

        çŠ¶æ€è¾“å…¥:
        - current_batch: int - å³å°†æ‰§è¡Œçš„æ‰¹æ¬¡ç¼–å·
        - execution_batches: List[List[str]] - æ‰€æœ‰æ‰¹æ¬¡
        - execution_mode: str - æ‰§è¡Œæ¨¡å¼é…ç½®ï¼ˆå¯é€‰ï¼‰

        è¿”å›:
        - Command - æ ¹æ®æ¨¡å¼è¿”å›ä¸åŒçš„å‘½ä»¤
        """
        try:
            from langgraph.types import interrupt

            current_batch = state.get("current_batch", 1)
            batches = state.get("execution_batches", [])
            execution_mode = state.get("execution_mode", "manual")  # é»˜è®¤æ‰‹åŠ¨ç¡®è®¤

            # è·å–å½“å‰æ‰¹æ¬¡çš„ä¸“å®¶åˆ—è¡¨
            batch_agents = batches[current_batch - 1] if current_batch <= len(batches) else []

            # ğŸ”¥ v3.6: æ ¹æ®æ‰§è¡Œæ¨¡å¼å†³å®šæ˜¯å¦éœ€è¦ç”¨æˆ·ç¡®è®¤
            if execution_mode == "automatic":
                # æ–¹æ¡ˆCï¼šå…¨è‡ªåŠ¨æ‰§è¡Œï¼ˆæ—§è¡Œä¸ºï¼‰
                logger.info(f"âš¡ æ‰¹æ¬¡ {current_batch}/{len(batches)} è‡ªåŠ¨æ‰§è¡Œï¼ˆæ–¹æ¡ˆCï¼šå…¨è‡ªåŠ¨æ‰¹æ¬¡è°ƒåº¦ï¼‰")
                return Command(
                    update={
                        "batch_strategy_approved": True,
                        "auto_approved": True,
                        "auto_approval_reason": "execution_mode=automatic"
                    },
                    goto="batch_executor"
                )
            elif execution_mode == "preview":
                # æ–¹æ¡ˆDï¼šæ˜¾ç¤ºè®¡åˆ’åè‡ªåŠ¨æ‰§è¡Œ
                logger.info(f"ğŸ“‹ æ‰¹æ¬¡ {current_batch}/{len(batches)} æ˜¾ç¤ºè®¡åˆ’åè‡ªåŠ¨æ‰§è¡Œ")
                # TODO: å¯ä»¥åœ¨è¿™é‡Œå‘é€é€šçŸ¥ç»™å‰ç«¯
                return Command(
                    update={
                        "batch_strategy_approved": True,
                        "auto_approved": True,
                        "auto_approval_reason": "execution_mode=preview"
                    },
                    goto="batch_executor"
                )
            else:
                # æ–¹æ¡ˆAï¼šæ‰‹åŠ¨ç¡®è®¤æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
                logger.info(f"ğŸ‘¤ æ‰¹æ¬¡ {current_batch}/{len(batches)} ç­‰å¾…ç”¨æˆ·ç¡®è®¤")

                # æ„å»ºæ‰¹æ¬¡ä¿¡æ¯
                batch_info = {
                    "interaction_type": "batch_confirmation",  # ğŸ”¥ ä¿®å¤ï¼šä¸å…¶ä»– interrupt ä¿æŒä¸€è‡´ä½¿ç”¨ interaction_type
                    "current_batch": current_batch,
                    "total_batches": len(batches),
                    "agents_in_batch": batch_agents,
                    "execution_strategy": "parallel" if len(batch_agents) > 1 else "sequential",
                    "message": f"å‡†å¤‡æ‰§è¡Œæ‰¹æ¬¡ {current_batch}/{len(batches)}",
                    "details": {
                        "ä¸“å®¶åˆ—è¡¨": batch_agents,
                        "æ‰§è¡Œæ–¹å¼": "å¹¶è¡Œæ‰§è¡Œ" if len(batch_agents) > 1 else "é¡ºåºæ‰§è¡Œ",
                        "é¢„è®¡è€—æ—¶": f"{len(batch_agents) * 3}åˆ†é’Ÿ" if len(batch_agents) == 1 else f"{4}åˆ†é’Ÿ"
                    },
                    "options": {
                        "approve": "æ‰¹å‡†æ‰§è¡Œ",
                        "skip": "è·³è¿‡æ­¤æ‰¹æ¬¡",
                        "modify": "ä¿®æ”¹æ‰¹æ¬¡é…ç½®"
                    }
                }

                # è§¦å‘ä¸­æ–­ï¼Œç­‰å¾…ç”¨æˆ·ç¡®è®¤
                logger.info(f"ğŸ” [DEBUG] è§¦å‘æ‰¹æ¬¡ç¡®è®¤ä¸­æ–­: {batch_info}")
                user_response = interrupt(batch_info)
                logger.info(f"ğŸ“¥ æ”¶åˆ°ç”¨æˆ·å“åº”: {user_response}")

                # å¤„ç†ç”¨æˆ·å“åº”
                if user_response == "skip" or (isinstance(user_response, dict) and user_response.get("action") == "skip"):
                    logger.info("â­ï¸ ç”¨æˆ·é€‰æ‹©è·³è¿‡æ­¤æ‰¹æ¬¡")
                    # è·³è¿‡å½“å‰æ‰¹æ¬¡ï¼Œè¿›å…¥ä¸‹ä¸€æ‰¹æ¬¡
                    return Command(
                        update={
                            "current_batch": current_batch + 1,
                            "batch_strategy_approved": False,
                            "batch_skipped": True
                        },
                        goto="batch_router"  # è¿”å›è·¯ç”±å™¨æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šæ‰¹æ¬¡
                    )
                else:
                    # æ‰¹å‡†æ‰§è¡Œï¼ˆé»˜è®¤è¡Œä¸ºï¼‰
                    logger.info("âœ… ç”¨æˆ·æ‰¹å‡†æ‰§è¡Œæ­¤æ‰¹æ¬¡")
                    return Command(
                        update={
                            "batch_strategy_approved": True,
                            "auto_approved": False,
                            "user_confirmed": True
                        },
                        goto="batch_executor"
                    )

        except Exception as e:
            # åªæ•è·éInterruptå¼‚å¸¸
            if "Interrupt" not in str(type(e)):
                logger.error(f"âŒ Batch strategy review failed: {e}")
                import traceback
                traceback.print_exc()
                # å‡ºé”™æ—¶é»˜è®¤æ‰¹å‡†å¹¶ç»§ç»­
                return Command(goto="batch_executor")
            else:
                # é‡æ–°æŠ›å‡ºInterruptå¼‚å¸¸ï¼ˆLangGraphéœ€è¦ï¼‰
                raise

    def _analysis_review_node(self, state: ProjectAnalysisState) -> Command:
        """
        åˆ†æå®¡æ ¸èŠ‚ç‚¹ - é€’è¿›å¼å•è½®å®¡æ ¸ (v2.0)

        æ ¸å¿ƒæ”¹è¿›:
        1. ç§»é™¤å¤šè½®è¿­ä»£é€»è¾‘
        2. é€’è¿›å¼ä¸‰é˜¶æ®µï¼šçº¢â†’è“â†’è¯„å§”â†’ç”²æ–¹
        3. è¾“å‡ºæ”¹è¿›å»ºè®®ï¼ˆè€Œéé‡æ–°æ‰§è¡Œï¼‰
        4. è®°å½•final_rulingåˆ°state
        """
        logger.info("Executing progressive single-round analysis review node")
        return AnalysisReviewNode.execute(
            state=state,
            store=self.store,
            llm_model=self.llm_model,
            config=self.config
        )
    
    def _manual_review_node(self, state: ProjectAnalysisState) -> Command:
        """
        äººå·¥å®¡æ ¸èŠ‚ç‚¹ - å¤„ç†ä¸¥é‡è´¨é‡é—®é¢˜ (ğŸ†•)
        
        å½“å®¡æ ¸ç³»ç»Ÿå‘ç°>3ä¸ªmust_fixé—®é¢˜æ—¶è§¦å‘ï¼Œæš‚åœæµç¨‹ç­‰å¾…ç”¨æˆ·è£å†³ï¼š
        1. ç»§ç»­ï¼šæ¥å—é£é™©ç”ŸæˆæŠ¥å‘Š
        2. ç»ˆæ­¢ï¼šå…¨é¢æ•´æ”¹åå†ç”ŸæˆæŠ¥å‘Š
        3. é€‰æ‹©æ€§æ•´æ”¹ï¼šç”¨æˆ·é€‰æ‹©å…³é”®é—®é¢˜è¿›è¡Œæ•´æ”¹
        
        æ³¨æ„: ä¸è¦æ•è·Interruptå¼‚å¸¸!
        Interruptæ˜¯LangGraphçš„æ­£å¸¸æ§åˆ¶æµ,å¿…é¡»è®©å®ƒä¼ æ’­åˆ°æ¡†æ¶å±‚
        """
        logger.info("ğŸš¨ Executing manual review node for critical quality issues")
        return ManualReviewNode.execute(
            state=state,
            store=self.store
        )
    
    def _result_aggregator_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        ç»“æœèšåˆèŠ‚ç‚¹

        æ³¨æ„: åªè¿”å›éœ€è¦æ›´æ–°çš„å­—æ®µ,ä¸è¿”å›å®Œæ•´çŠ¶æ€
        è¿™æ ·å¯ä»¥é¿å…å¹¶å‘æ›´æ–°å†²çª
        
        ğŸ”§ ä¿®å¤: ä¸æ›´æ–°current_stage,é¿å…ä¸pdf_generatorå¹¶å‘å†²çª
        """
        try:
            logger.info("Executing result aggregator node")

            # åˆ›å»ºç»“æœèšåˆå™¨æ™ºèƒ½ä½“
            agent = ResultAggregatorAgent(
                llm_model=self.llm_model,
                config=self.config
            )

            # æ‰§è¡Œèšåˆ
            result = agent.execute(state, {}, self.store)

            # åªè¿”å›éœ€è¦æ›´æ–°çš„å­—æ®µ (ä¸æ›´æ–°current_stage)
            return {
                "final_report": result.structured_data,
                "agent_results": {
                    "RESULT_AGGREGATOR": result.to_dict()
                },
                "updated_at": datetime.now().isoformat(),
                "detail": "æ•´åˆä¸“å®¶æˆæœï¼Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šè‰ç¨¿"
            }

        except Exception as e:
            logger.error(f"Result aggregator node failed: {e}")
            return {
                "error": str(e),
                "updated_at": datetime.now().isoformat(),
                "detail": "ç»“æœèšåˆå¤±è´¥"
            }
    
    # def _final_review_node(self, state: ProjectAnalysisState) -> Command:
    #     """
    #     æœ€ç»ˆå®¡æ ¸èŠ‚ç‚¹ - å·²ç§»é™¤
    #
    #     æ ¹æ®å®¢æˆ·éœ€æ±‚,å·¥ä½œæµç¨‹åº”è¯¥æ˜¯:
    #     ç»“æœèšåˆ â†’ PDFç”Ÿæˆ â†’ ç»“æŸ
    #     ä¸éœ€è¦æœ€ç»ˆå®¡æ ¸é˜¶æ®µ
    #
    #     æ³¨æ„: ä¸è¦æ•è·Interruptå¼‚å¸¸!
    #     Interruptæ˜¯LangGraphçš„æ­£å¸¸æ§åˆ¶æµ,å¿…é¡»è®©å®ƒä¼ æ’­åˆ°æ¡†æ¶å±‚
    #     """
    #     logger.info("Executing final review node")
    #     return FinalReviewNode.execute(state, self.store)
    
    def _pdf_generator_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹ (ä½¿ç”¨çº¯æ–‡æœ¬ç”Ÿæˆå™¨è¿›è¡Œæµ‹è¯•)

        æ³¨æ„: åªè¿”å›éœ€è¦æ›´æ–°çš„å­—æ®µ,ä¸è¿”å›å®Œæ•´çŠ¶æ€
        è¿™æ ·å¯ä»¥é¿å…å¹¶å‘æ›´æ–°å†²çª
        """
        try:
            logger.info("Executing report generator node (text mode for testing)")

            # ä½¿ç”¨çº¯æ–‡æœ¬ç”Ÿæˆå™¨è¿›è¡Œæµ‹è¯•
            from ..report.text_generator import TextGeneratorAgent
            agent = TextGeneratorAgent(config=self.config)

            # æ‰§è¡ŒæŠ¥å‘Šç”Ÿæˆ
            result = agent.execute(state, {}, self.store)

            # åªè¿”å›éœ€è¦æ›´æ–°çš„å­—æ®µ
            return {
                "current_stage": AnalysisStage.COMPLETED.value,
                "pdf_path": result.structured_data.get("file_path"),
                "agent_results": {
                    "REPORT_GENERATOR": result.to_dict()
                },
                "updated_at": datetime.now().isoformat(),
                "detail": "ç”Ÿæˆæœ€ç»ˆäº¤ä»˜æ–‡æ¡£"
            }

        except Exception as e:
            logger.error(f"Report generator node failed: {e}")
            return {
                "error": str(e),
                "updated_at": datetime.now().isoformat(),
                "detail": "æŠ¥å‘Šç”Ÿæˆå¤±è´¥"
            }
    
    def _user_question_node(self, state: ProjectAnalysisState) -> Command:
        """
        ç”¨æˆ·è¿½é—®èŠ‚ç‚¹
        
        ğŸ”§ ä¿®å¤ (2025-11-27): ä¸è¦æ•è· Interrupt å¼‚å¸¸
        Interrupt æ˜¯ LangGraph çš„æ­£å¸¸æ§åˆ¶æµï¼Œå¿…é¡»è®©å®ƒä¼ æ’­åˆ°æ¡†æ¶å±‚
        """
        logger.info("Executing user question node")
        # âŒ ä¸è¦ç”¨ try-except æ•è·ï¼Interrupt å¿…é¡»ä¼ æ’­
        return UserQuestionNode.execute(state, self.store)
    
    # è·¯ç”±å‡½æ•°
    def _route_from_batch_aggregator(self, state: ProjectAnalysisState) -> str:
        """
        æ‰¹æ¬¡èšåˆå™¨åçš„è·¯ç”±å‡½æ•°

        ğŸ”§ ä¿®å¤ï¼ˆ2025-11-30ï¼‰ï¼šç§»é™¤ analysis_approved çš„ END è·¯å¾„
        - BugåŸå› ï¼šå½“ analysis_approved=True æ—¶ç›´æ¥ ENDï¼Œè·³è¿‡äº† result_aggregator å’Œ pdf_generator
        - ä¿®å¤ï¼šå§‹ç»ˆè·¯ç”±åˆ° batch_routerï¼Œè®© batch_router å†³å®šæ˜¯å¦è¿›å…¥ analysis_review
        - analysis_review â†’ detect_challenges â†’ result_aggregator æ˜¯å”¯ä¸€ç”ŸæˆæŠ¥å‘Šçš„è·¯å¾„

        é€»è¾‘:
        - å§‹ç»ˆ â†’ batch_router
          - batch_router æ£€æŸ¥æ‰¹æ¬¡å®Œæˆæƒ…å†µï¼š
            - å¦‚æœè¿˜æœ‰æ‰¹æ¬¡ï¼šç»§ç»­ä¸‹ä¸€æ‰¹
            - å¦‚æœæ‰€æœ‰æ‰¹æ¬¡å®Œæˆï¼šè¿›å…¥ analysis_review â†’ detect_challenges â†’ result_aggregator â†’ pdf_generator
        """
        logger.info("ğŸ”€ [batch_aggregator] è·¯ç”±åˆ° batch_router æ£€æŸ¥æ‰¹æ¬¡çŠ¶æ€")
        return "batch_router"
    
    def _route_after_requirements_confirmation(self, state: ProjectAnalysisState) -> Literal["project_director", "requirements_analyst"]:
        """éœ€æ±‚ç¡®è®¤åçš„è·¯ç”±"""
        if state.get("requirements_confirmed"):
            return "project_director"
        else:
            return "requirements_analyst"
    
    def _route_after_pdf_generator(self, state: ProjectAnalysisState) -> Union[str, Any]:
        """æŠ¥å‘Šç”Ÿæˆåçš„è·¯ç”±: ç›´æ¥ç»“æŸï¼Œç”±å‰ç«¯è´Ÿè´£ç»“æœå‘ˆç°å’Œè¿½é—®äº¤äº’"""
        # æ ‡è®°è¿½é—®åŠŸèƒ½å¯ç”¨ï¼Œå‰ç«¯ä¼šæ ¹æ®æ­¤æ ‡å¿—æ˜¾ç¤ºè¿½é—®å…¥å£
        state["post_completion_followup_available"] = self.config.get("post_completion_followup_enabled", True)
        
        logger.info("âœ… [pdf_generator] æŠ¥å‘Šå·²ç”Ÿæˆï¼Œæµç¨‹ç»“æŸï¼Œå‰ç«¯æ¥ç®¡ç»“æœå‘ˆç°")
        return END

    def _route_after_analysis_review(self, state: ProjectAnalysisState) -> Literal["result_aggregator", "project_director", "user_question"]:
        """åˆ†æå®¡æ ¸åçš„è·¯ç”±"""
        if state.get("analysis_approved"):
            return "result_aggregator"
        elif state.get("additional_questions"):
            return "user_question"
        else:
            return "project_director"
    
    # def _route_after_final_review(self, state: ProjectAnalysisState) -> Literal["pdf_generator", "result_aggregator"]:
    #     """æœ€ç»ˆå®¡æ ¸åçš„è·¯ç”± - å·²ç§»é™¤ï¼Œå› ä¸ºä¸å†æœ‰æœ€ç»ˆå®¡æ ¸é˜¶æ®µ"""
    #     if state.get("final_approved"):
    #         return "pdf_generator"
    #     else:
    #         return "result_aggregator"
    
    def _route_after_user_question(self, state: ProjectAnalysisState) -> Literal["project_director", END]:
        """
        ç”¨æˆ·è¿½é—®åçš„è·¯ç”±

        ğŸ”§ ä¿®å¤ (2025-11-27): è¿½é—®å®Œæˆååº”è¯¥ç»“æŸæµç¨‹ï¼Œè€Œä¸æ˜¯å›åˆ° result_aggregator
        é¿å…æ— é™å¾ªç¯ï¼špdf_generator â†’ user_question â†’ result_aggregator â†’ pdf_generator

        ğŸ”¥ ä¿®å¤ (2025-11-29): additional_questionsæ˜¯listç±»å‹ï¼Œä¸èƒ½è°ƒç”¨.strip()
        """
        additional_questions = state.get("additional_questions")

        # ğŸ”¥ ä¿®å¤ï¼šadditional_questions æ˜¯listï¼Œä¸æ˜¯string
        if additional_questions and len(additional_questions) > 0:
            logger.info(f"ğŸ“ ç”¨æˆ·æå‡ºè¿½é—®({len(additional_questions)}ä¸ª)ï¼Œè¿”å› project_director é‡æ–°åˆ†æ")
            return "project_director"
        else:
            logger.info("âœ… ç”¨æˆ·æœªè¿½é—®æˆ–è¿½é—®å®Œæˆï¼Œæµç¨‹ç»“æŸ")
            return END
    
    def run(self, user_input: str, session_id: Optional[str] = None) -> Dict[str, Any]:
        """
        è¿è¡Œå·¥ä½œæµ
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            session_id: ä¼šè¯ID
            
        Returns:
            å·¥ä½œæµæ‰§è¡Œç»“æœ
        """
        try:
            # ç”Ÿæˆä¼šè¯ID
            if not session_id:
                session_id = str(uuid.uuid4())
            
            # åˆå§‹åŒ–çŠ¶æ€
            initial_state = StateManager.create_initial_state(
                user_input=user_input,
                session_id=session_id
            )
            
            logger.info(f"Starting workflow execution for session {session_id}")
            
            # æ‰§è¡Œå·¥ä½œæµ
            config = {"configurable": {"thread_id": session_id}}
            final_state = self.graph.invoke(initial_state, config)
            
            logger.info(f"Workflow execution completed for session {session_id}")
            
            return {
                "session_id": session_id,
                "status": "completed",
                "final_state": final_state,
                "pdf_path": final_state.get("pdf_path"),
                "execution_time": final_state.get("execution_time")
            }
            
        except Exception as e:
            logger.error(f"Workflow execution failed: {e}")
            return {
                "session_id": session_id,
                "status": "failed",
                "error": str(e)
            }

    def _build_context_for_expert(self, state: ProjectAnalysisState) -> str:
        """
        ä¸ºä»»åŠ¡å¯¼å‘ä¸“å®¶æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
        
        Args:
            state: å½“å‰é¡¹ç›®çŠ¶æ€
            
        Returns:
            str: æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        context_parts = []
        
        # æ·»åŠ ç”¨æˆ·éœ€æ±‚
        task_description = state.get("task_description", "")
        if task_description:
            context_parts.append(f"## ç”¨æˆ·éœ€æ±‚\n{task_description}")
        
        # æ·»åŠ ç»“æ„åŒ–éœ€æ±‚
        structured_requirements = state.get("structured_requirements", {})
        if structured_requirements:
            context_parts.append("## ç»“æ„åŒ–éœ€æ±‚")
            for key, value in structured_requirements.items():
                if value:
                    context_parts.append(f"**{key}**: {value}")
        
        # æ·»åŠ å·²å®Œæˆçš„åˆ†æ
        expert_analyses = state.get("expert_analyses", {})
        if expert_analyses:
            context_parts.append("## å·²å®Œæˆçš„ä¸“å®¶åˆ†æ")
            for expert_id, analysis in expert_analyses.items():
                if isinstance(analysis, dict):
                    analysis_content = analysis.get("analysis", str(analysis))
                else:
                    analysis_content = str(analysis)
                
                # åªæ˜¾ç¤ºå‰500å­—ç¬¦ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
                preview = analysis_content[:500]
                if len(analysis_content) > 500:
                    preview += "..."
                    
                context_parts.append(f"### {expert_id}\n{preview}")
        
        # æ·»åŠ é¡¹ç›®çŠ¶æ€ä¿¡æ¯
        context_parts.append(f"\n## é¡¹ç›®çŠ¶æ€ä¿¡æ¯")
        context_parts.append(f"- å½“å‰é˜¶æ®µ: {state.get('current_phase', 'unknown')}")
        context_parts.append(f"- å·²å®Œæˆä¸“å®¶æ•°: {len(expert_analyses)}")
        
        # æ·»åŠ è´¨é‡æ£€æŸ¥æ¸…å•ï¼ˆå¦‚æœæœ‰ï¼‰
        quality_checklist = state.get("quality_checklist", {})
        if quality_checklist:
            context_parts.append("## è´¨é‡è¦æ±‚")
            for category, criteria in quality_checklist.items():
                if isinstance(criteria, list):
                    context_parts.append(f"**{category}**: {', '.join(criteria)}")
                else:
                    context_parts.append(f"**{category}**: {criteria}")
        
        return "\n\n".join(context_parts)
