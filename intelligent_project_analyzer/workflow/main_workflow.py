"""
ä¸»å·¥ä½œæµç¼–æŽ’å™¨

åŸºäºŽLangGraphå®žçŽ°çš„å¤šæ™ºèƒ½ä½“åä½œå·¥ä½œæµ
"""

import uuid
from typing import Dict, List, Optional, Any, Literal, Union
from datetime import datetime
from pathlib import Path
from loguru import logger
import yaml

from langgraph.graph import StateGraph, START, END
from langgraph.store.memory import InMemoryStore
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Send, Command

from ..core.state import ProjectAnalysisState, AnalysisStage, StateManager
from ..core.types import AgentType, format_role_display_name
# æ˜¾å¼å¯¼å…¥æ™ºèƒ½ä½“ç±»ä»¥è§¦å‘ AgentFactory æ³¨å†Œ
from ..agents import AgentFactory, RequirementsAnalystAgent, ProjectDirectorAgent
from ..agents.feasibility_analyst import FeasibilityAnalystAgent  # ðŸ†• V1.5å¯è¡Œæ€§åˆ†æžå¸ˆ
from ..agents.base import NullLLM
from ..interaction.interaction_nodes import (
    CalibrationQuestionnaireNode,
    RequirementsConfirmationNode,
    AnalysisReviewNode,
    # FinalReviewNode,  # å·²ç§»é™¤ï¼šå®¢æˆ·éœ€æ±‚ä¸­æ²¡æœ‰æœ€ç»ˆå®¡æ ¸é˜¶æ®µ
    UserQuestionNode
)
# ðŸ†• v7.87: ä¸‰æ­¥é€’è¿›å¼é—®å·èŠ‚ç‚¹
from ..interaction.nodes.progressive_questionnaire import (
    ProgressiveQuestionnaireNode,
    progressive_step1_core_task_node,
    progressive_step2_radar_node,
    progressive_step3_gap_filling_node
)
# ðŸ†• ç»Ÿä¸€å®¡æ ¸èŠ‚ç‚¹ï¼ˆåˆå¹¶è§’è‰²é€‰æ‹©å’Œä»»åŠ¡åˆ†æ´¾å®¡æ ¸ï¼‰
from ..interaction.role_task_unified_review import role_task_unified_review_node
# from ..interaction.role_selection_review import role_selection_review_node  # å·²åºŸå¼ƒ
# from ..interaction.task_assignment_review import task_assignment_review_node  # å·²åºŸå¼ƒ
from ..interaction.second_batch_strategy_review import SecondBatchStrategyReviewNode
from ..interaction.nodes.quality_preflight import QualityPreflightNode  # ðŸ†•
from ..agents.quality_monitor import QualityMonitor  # ðŸ†•
from ..interaction.nodes.manual_review import ManualReviewNode  # ðŸ†• äººå·¥å®¡æ ¸èŠ‚ç‚¹
from ..agents.dynamic_project_director import detect_and_handle_challenges_node  # ðŸ†• v3.5
from ..report.result_aggregator import ResultAggregatorAgent
from ..report.pdf_generator import PDFGeneratorAgent

# ðŸ†• v7.16: LangGraph Agent å‡çº§ç‰ˆæœ¬ï¼ˆé€šè¿‡çŽ¯å¢ƒå˜é‡æŽ§åˆ¶ï¼‰
import os
USE_V716_AGENTS = os.getenv("USE_V716_AGENTS", "false").lower() == "true"
USE_V717_REQUIREMENTS_ANALYST = os.getenv("USE_V717_REQUIREMENTS_ANALYST", "false").lower() == "true"
USE_V718_QUESTIONNAIRE_AGENT = os.getenv("USE_V718_QUESTIONNAIRE_AGENT", "false").lower() == "true"
# ðŸ†• v7.87: ä¸‰æ­¥é€’è¿›å¼é—®å·ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
USE_PROGRESSIVE_QUESTIONNAIRE = os.getenv("USE_PROGRESSIVE_QUESTIONNAIRE", "true").lower() == "true"
USE_MULTI_ROUND_QUESTIONNAIRE = os.getenv("USE_MULTI_ROUND_QUESTIONNAIRE", "false").lower() == "true"
if USE_V716_AGENTS:
    from ..agents.analysis_review_agent import AnalysisReviewAgent, AnalysisReviewNodeCompat
    from ..agents.result_aggregator_agent import ResultAggregatorAgentV2, ResultAggregatorAgentCompat
    from ..agents.challenge_detection_agent import ChallengeDetectionAgent, detect_and_handle_challenges_v2
    from ..agents.quality_preflight_agent import QualityPreflightAgent, QualityPreflightNodeCompat
    from ..agents.questionnaire_agent import QuestionnaireAgent, LLMQuestionGeneratorCompat
    logger.info("ðŸš€ [v7.16] å¯ç”¨ LangGraph Agent å‡çº§ç‰ˆæœ¬")
if USE_V717_REQUIREMENTS_ANALYST:
    from ..agents.requirements_analyst_agent import RequirementsAnalystAgentV2
    logger.info("ðŸš€ [v7.17] å¯ç”¨éœ€æ±‚åˆ†æžå¸ˆ StateGraph Agent")
if USE_V718_QUESTIONNAIRE_AGENT:
    from ..agents.questionnaire_agent import QuestionnaireAgent
    logger.info("ðŸš€ [v7.18] å¯ç”¨é—®å·ç”Ÿæˆ StateGraph Agent")
from ..security import (  # ðŸ†• å†…å®¹å®‰å…¨ä¸Žé¢†åŸŸè¿‡æ»¤
    ReportGuardNode
)
# ðŸ†• v7.3 ç»Ÿä¸€è¾“å…¥éªŒè¯èŠ‚ç‚¹ï¼ˆåˆå¹¶ input_guard å’Œ domain_validatorï¼‰
from ..security.unified_input_validator_node import (
    UnifiedInputValidatorNode,
    InputRejectedNode
)
# åŠ¨æ€æœ¬ä½“è®ºæ³¨å…¥å·¥å…·
from ..utils.ontology_loader import OntologyLoader


class MainWorkflow:
    """ä¸»å·¥ä½œæµç¼–æŽ’å™¨"""
    
    def __init__(self, llm_model: Optional[Any] = None, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–ä¸»å·¥ä½œæµ
        
        Args:
            llm_model: LLMæ¨¡åž‹å®žä¾‹
            config: é…ç½®å‚æ•°
        """
        if llm_model is None:
            llm_model = NullLLM("MainWorkflow")
        self.llm_model = llm_model
        self.config = config or {}
        if isinstance(self.llm_model, NullLLM):
            self.config.setdefault("llm_placeholder", True)
        # å¯ç”¨å®ŒæˆåŽè¿½é—®äº¤äº’ï¼ˆå¯é€šè¿‡é…ç½®è¦†ç›–ï¼‰
        self.config.setdefault("post_completion_followup_enabled", True)
        
        # åˆå§‹åŒ–å­˜å‚¨å’Œæ£€æŸ¥ç‚¹
        self.store = InMemoryStore()
        self.checkpointer = MemorySaver()
        
        # åˆå§‹åŒ–æœ¬ä½“è®ºåŠ è½½å™¨
        self.ontology_loader = OntologyLoader(
            "d:/11-20/langgraph-design/intelligent_project_analyzer/knowledge_base/ontology.yaml"
        )

        # æž„å»ºå·¥ä½œæµå›¾
        self.graph = self._build_workflow_graph()
        
        logger.info("Main workflow initialized successfully")
    
    def _build_workflow_graph(self) -> StateGraph:
        """
        æž„å»ºå·¥ä½œæµå›¾ï¼ˆ2025-11-19é‡æž„ï¼šæ”¯æŒåŠ¨æ€Næ‰¹æ¬¡æ‰§è¡Œï¼‰

        é‡è¦å˜æ›´:
        - ç§»é™¤ç¡¬ç¼–ç çš„ first_batch_agent å’Œ second_batch_agent
        - æ·»åŠ åŠ¨æ€æ‰¹æ¬¡èŠ‚ç‚¹ï¼šbatch_executor, agent_executor, batch_aggregator, batch_router
        - å®žçŽ°å¾ªçŽ¯æ‰§è¡Œï¼šbatch_executor â†’ agent_executor â†’ batch_aggregator â†’ batch_router
        - æ”¯æŒ 1-N ä¸ªæ‰¹æ¬¡çš„åŠ¨æ€æ‰§è¡Œ
        """
        # åˆ›å»ºçŠ¶æ€å›¾
        workflow = StateGraph(ProjectAnalysisState)

        # ============================================================================
        # 0. ðŸ†• å®‰å…¨èŠ‚ç‚¹ï¼ˆç¬¬ä¸€é“é˜²çº¿ï¼‰
        # ============================================================================
        # ðŸ†• v7.3 ç»Ÿä¸€è¾“å…¥éªŒè¯èŠ‚ç‚¹ï¼ˆåˆå¹¶ input_guard å’Œ domain_validatorï¼‰
        workflow.add_node("unified_input_validator_initial", self._unified_input_validator_initial_node)  # åˆå§‹éªŒè¯
        workflow.add_node("unified_input_validator_secondary", self._unified_input_validator_secondary_node)  # äºŒæ¬¡éªŒè¯
        workflow.add_node("input_rejected", self._input_rejected_node)        # æ‹’ç»ç»ˆæ­¢
        workflow.add_node("report_guard", self._report_guard_node)            # æŠ¥å‘Šå®¡æ ¸

        # ============================================================================
        # 1. å‰ç½®æµç¨‹èŠ‚ç‚¹ï¼ˆéœ€æ±‚æ”¶é›†ä¸Žç¡®è®¤ï¼‰
        # ============================================================================
        workflow.add_node("requirements_analyst", self._requirements_analyst_node)
        workflow.add_node("feasibility_analyst", self._feasibility_analyst_node)  # ðŸ†• V1.5å¯è¡Œæ€§åˆ†æžå¸ˆ

        # ðŸ†• v7.87: æ ¹æ®çŽ¯å¢ƒå˜é‡é€‰æ‹©é—®å·ç±»åž‹
        if USE_PROGRESSIVE_QUESTIONNAIRE:
            # ä¸‰æ­¥é€’è¿›å¼é—®å·ï¼ˆv7.80+ï¼‰
            workflow.add_node("progressive_step1_core_task", self._progressive_step1_node)
            workflow.add_node("progressive_step2_radar", self._progressive_step2_node)
            workflow.add_node("progressive_step3_gap_filling", self._progressive_step3_node)
            logger.info("ðŸŽ¯ [v7.87] å¯ç”¨ä¸‰æ­¥é€’è¿›å¼é—®å·ï¼ˆprogressive_questionnaireï¼‰")
        else:
            # æ—§ç‰ˆå•è½®é—®å·ï¼ˆå‘åŽå…¼å®¹ï¼‰
            workflow.add_node("calibration_questionnaire", self._calibration_questionnaire_node)
            logger.info("âš ï¸ [v7.87] ä½¿ç”¨æ—§ç‰ˆå•è½®é—®å·ï¼ˆcalibration_questionnaireï¼‰ï¼Œå»ºè®®è®¾ç½® USE_PROGRESSIVE_QUESTIONNAIRE=true")

        workflow.add_node("requirements_confirmation", self._requirements_confirmation_node)

        # ============================================================================
        # 2. è§’è‰²é€‰æ‹©ä¸Žä»»åŠ¡åˆ†æ´¾èŠ‚ç‚¹
        # ============================================================================
        workflow.add_node("project_director", self._project_director_node)
        # ðŸ†• v7.108 äº¤ä»˜ç‰©IDç”Ÿæˆå™¨ï¼ˆæ¦‚å¿µå›¾ç²¾å‡†å…³è”ï¼‰
        workflow.add_node("deliverable_id_generator", self._deliverable_id_generator_node)
        # ðŸ†• ç»Ÿä¸€å®¡æ ¸èŠ‚ç‚¹ï¼ˆåˆå¹¶è§’è‰²é€‰æ‹©å’Œä»»åŠ¡åˆ†æ´¾ï¼‰
        workflow.add_node("role_task_unified_review", self._role_task_unified_review_node)
        workflow.add_node("quality_preflight", self._quality_preflight_node)  # ðŸ†• è´¨é‡é¢„æ£€

        # ============================================================================
        # 3. ðŸ†• åŠ¨æ€æ‰¹æ¬¡æ‰§è¡ŒèŠ‚ç‚¹ï¼ˆæ ¸å¿ƒé‡æž„ï¼‰
        # ============================================================================
        workflow.add_node("batch_executor", self._batch_executor_node)           # æ‰¹æ¬¡æ‰§è¡Œå™¨
        workflow.add_node("agent_executor", self._execute_agent_node)            # æ™ºèƒ½ä½“æ‰§è¡Œå™¨
        workflow.add_node("batch_aggregator", self._intermediate_aggregator_node) # æ‰¹æ¬¡èšåˆå™¨
        workflow.add_node("batch_router", self._batch_router_node)               # æ‰¹æ¬¡è·¯ç”±å™¨
        workflow.add_node("batch_strategy_review", self._batch_strategy_review_node)  # æ‰¹æ¬¡ç­–ç•¥å®¡æ ¸
        workflow.add_node("detect_challenges", self._detect_challenges_node)     # ðŸ†• v3.5 æŒ‘æˆ˜æ£€æµ‹

        # ============================================================================
        # 4. å®¡æ ¸ä¸Žç»“æžœç”ŸæˆèŠ‚ç‚¹
        # ============================================================================
        workflow.add_node("analysis_review", self._analysis_review_node)
        workflow.add_node("manual_review", self._manual_review_node)  # ðŸ†• äººå·¥å®¡æ ¸èŠ‚ç‚¹
        workflow.add_node("result_aggregator", self._result_aggregator_node)
        workflow.add_node("pdf_generator", self._pdf_generator_node)
        workflow.add_node("user_question", self._user_question_node)

        # ============================================================================
        # è¾¹è¿žæŽ¥ï¼šå‰ç½®æµç¨‹
        # ============================================================================
        # ðŸ†• v7.3 ç»Ÿä¸€è¾“å…¥éªŒè¯æµç¨‹
        workflow.add_edge(START, "unified_input_validator_initial")  # ç¬¬ä¸€é“é˜²çº¿ï¼šåˆå§‹éªŒè¯
        # unified_input_validator_initial ä½¿ç”¨ Command è·¯ç”±åˆ° requirements_analyst æˆ– input_rejected
        workflow.add_edge("input_rejected", END)  # æ‹’ç»åŽç»ˆæ­¢

        workflow.add_edge("requirements_analyst", "feasibility_analyst")  # ðŸ†• V1.5å¯è¡Œæ€§åˆ†æž
        workflow.add_edge("feasibility_analyst", "unified_input_validator_secondary")  # ç¬¬äºŒé“é˜²çº¿ï¼šäºŒæ¬¡éªŒè¯

        # ðŸ†• v7.87: æ ¹æ®çŽ¯å¢ƒå˜é‡é…ç½®é—®å·æµç¨‹
        if USE_PROGRESSIVE_QUESTIONNAIRE:
            # ä¸‰æ­¥é€’è¿›å¼é—®å·æµç¨‹
            workflow.add_edge("unified_input_validator_secondary", "progressive_step1_core_task")
            # progressive_step1 ä½¿ç”¨ Command è·¯ç”±åˆ° progressive_step2_radar æˆ– requirements_analyst
            # progressive_step2 ä½¿ç”¨ Command è·¯ç”±åˆ° progressive_step3_gap_filling æˆ– progressive_step1
            # progressive_step3 ä½¿ç”¨ Command è·¯ç”±åˆ° requirements_confirmation æˆ– progressive_step2
            logger.info("ðŸŽ¯ [v7.87] é…ç½®ä¸‰æ­¥é€’è¿›å¼é—®å·æµç¨‹")
        elif USE_MULTI_ROUND_QUESTIONNAIRE:
            # å¤šè½®è¿­ä»£é—®å·å·²åœç”¨ï¼Œå›žé€€åˆ°ä¸‰æ­¥é€’è¿›å¼
            logger.warning("âš ï¸ [v7.87] å¤šè½®è¿­ä»£é—®å·å·²åœç”¨ï¼Œå›žé€€åˆ°ä¸‰æ­¥é€’è¿›å¼é—®å·")
            workflow.add_edge("unified_input_validator_secondary", "progressive_step1_core_task")
        else:
            # æ—§ç‰ˆå•è½®é—®å·
            workflow.add_edge("unified_input_validator_secondary", "calibration_questionnaire")
            # âœ… calibration_questionnaire ä½¿ç”¨ Command å®Œå…¨åŠ¨æ€è·¯ç”±ï¼ˆæ— é™æ€ edgeï¼‰

        # âœ… requirements_confirmation ä½¿ç”¨ Command åŠ¨æ€è·¯ç”±åˆ° requirements_analyst æˆ– project_director

        workflow.add_edge("project_director", "deliverable_id_generator")  # ðŸ†• v7.108 ç”Ÿæˆäº¤ä»˜ç‰©ID
        workflow.add_edge("deliverable_id_generator", "role_task_unified_review")  # ðŸ†• ç»Ÿä¸€å®¡æ ¸
        # âŒ ç§»é™¤é™æ€è¾¹ï¼Œè®© role_task_unified_review ä½¿ç”¨ Command åŠ¨æ€è·¯ç”±
        # workflow.add_edge("role_task_unified_review", "quality_preflight")  # ðŸ†• è´¨é‡é¢„æ£€
        workflow.add_edge("quality_preflight", "batch_executor")  # ðŸ†• é¢„æ£€åŽæ‰§è¡Œ
        # role_task_unified_review ä½¿ç”¨ Command è·¯ç”±åˆ° batch_executor æˆ– quality_preflight

        # ============================================================================
        # ðŸ†• åŠ¨æ€æ‰¹æ¬¡æ‰§è¡Œæµç¨‹ï¼ˆæ ¸å¿ƒå¾ªçŽ¯ï¼‰
        # ============================================================================
        # æ‰¹æ¬¡æ‰§è¡Œå™¨ â†’ æ™ºèƒ½ä½“æ‰§è¡Œå™¨ï¼ˆå¹¶è¡Œï¼‰â†’ æ‰¹æ¬¡èšåˆå™¨ â†’ æ‰¹æ¬¡è·¯ç”±å™¨
        workflow.add_edge("agent_executor", "batch_aggregator")
        # workflow.add_edge("batch_aggregator", "detect_challenges")  # âŒ æš‚æ—¶ç¦ç”¨ï¼šå¯¼è‡´çŠ¶æ€å†²çª
        
        # ðŸ†• v3.5: æŒ‘æˆ˜æ£€æµ‹ç§»è‡³ result_aggregator ä¹‹å‰
        # batch_aggregator â†’ batch_router â†’ analysis_review â†’ detect_challenges â†’ result_aggregator
        # workflow.add_conditional_edges(
        #     "detect_challenges",
        #     self._route_after_challenge_detection,
        #     {
        #         "revisit_requirements": "requirements_analyst",  # åé¦ˆå¾ªçŽ¯
        #         "continue_workflow": "batch_router"  # ç»§ç»­æ­£å¸¸æµç¨‹
        #     }
        # )

        # ðŸ†• batch_aggregator â†’ batch_router (æ¡ä»¶è¾¹ï¼šæ£€æŸ¥å®¡æ ¸çŠ¶æ€)
        workflow.add_conditional_edges(
            "batch_aggregator",
            self._route_from_batch_aggregator,
            ["batch_router", END]  # ðŸ”§ N1ä¿®å¤ï¼šç§»é™¤detect_challengesï¼Œé¿å…é‡å¤è°ƒç”¨result_aggregator
        )
        
        # æ‰¹æ¬¡è·¯ç”±å™¨æ ¹æ® current_batch å’Œ total_batches å†³å®šï¼š
        # - å¦‚æžœè¿˜æœ‰ä¸‹ä¸€æ‰¹æ¬¡ â†’ batch_strategy_reviewï¼ˆå®¡æ ¸åŽç»§ç»­ï¼‰
        # - å¦‚æžœæ‰€æœ‰æ‰¹æ¬¡å®Œæˆ â†’ analysis_review
        # è·¯ç”±é€»è¾‘åœ¨ _batch_router_node ä¸­é€šè¿‡ Command å®žçŽ°

        # æ‰¹æ¬¡ç­–ç•¥å®¡æ ¸ â†’ æ‰¹æ¬¡æ‰§è¡Œå™¨ï¼ˆå½¢æˆå¾ªçŽ¯ï¼‰
        workflow.add_edge("batch_strategy_review", "batch_executor")
        # batch_strategy_review ä½¿ç”¨ Command è·¯ç”±ï¼Œå¯ä»¥è·³è¿‡å®¡æ ¸ç›´æŽ¥åˆ° batch_executor

        # âœ… æ‰¹æ¬¡æ‰§è¡Œå™¨ â†’ æ™ºèƒ½ä½“æ‰§è¡Œå™¨ï¼ˆæ¡ä»¶è¾¹ + Send APIï¼‰
        # ä½¿ç”¨æ¡ä»¶è¾¹å‡½æ•°åŠ¨æ€åˆ›å»ºå¹¶è¡Œä»»åŠ¡
        workflow.add_conditional_edges(
            "batch_executor",              # æºèŠ‚ç‚¹
            self._create_batch_sends,      # æ¡ä»¶è¾¹å‡½æ•°ï¼šè¿”å›ž List[Send]
            ["agent_executor"]             # å¯èƒ½çš„ç›®æ ‡èŠ‚ç‚¹åˆ—è¡¨
        )

        # ============================================================================
        # å®¡æ ¸ä¸Žç»“æžœç”Ÿæˆæµç¨‹
        # ============================================================================
        # analysis_review ä½¿ç”¨ Command åŠ¨æ€è·¯ç”±åˆ°ï¼š
        # - detect_challengesï¼ˆæ‰¹å‡†åŽå…ˆæ£€æµ‹æŒ‘æˆ˜ï¼‰
        # - batch_executorï¼ˆé‡æ‰§è¡Œç‰¹å®šæ‰¹æ¬¡ï¼‰
        # - project_directorï¼ˆé‡æ–°è§„åˆ’ï¼‰
        
        # ðŸ†• v3.5: æŒ‘æˆ˜æ£€æµ‹åœ¨å®¡æ ¸é€šè¿‡åŽã€ç»“æžœèšåˆå‰æ‰§è¡Œ
        workflow.add_conditional_edges(
            "detect_challenges",
            self._route_after_challenge_detection,
            {
                "revisit_requirements": "requirements_analyst",  # åé¦ˆå¾ªçŽ¯
                "manual_review": "manual_review",  # ðŸ†• äººå·¥å®¡æ ¸ï¼ˆ>3ä¸ªmust_fixï¼‰
                "continue_workflow": "result_aggregator"  # ç»§ç»­åˆ°ç»“æžœèšåˆ
            }
        )
        
        # ðŸ†• äººå·¥å®¡æ ¸èŠ‚ç‚¹ï¼šmanual_review ä½¿ç”¨ Command åŠ¨æ€è·¯ç”±åˆ°ï¼š
        # - batch_executorï¼ˆé‡æ–°æ‰§è¡Œç‰¹å®šä¸“å®¶ï¼‰
        # - detect_challengesï¼ˆç»§ç»­æŒ‘æˆ˜æ£€æµ‹ï¼‰
        # - ENDï¼ˆç»ˆæ­¢æµç¨‹ï¼‰

        workflow.add_edge("result_aggregator", "report_guard")  # ðŸ†• æŠ¥å‘Šå®¡æ ¸
        workflow.add_edge("report_guard", "pdf_generator")      # ðŸ†• å®¡æ ¸åŽç”ŸæˆPDF

        workflow.add_conditional_edges(
            "pdf_generator",
            self._route_after_pdf_generator,
            ["user_question", END]
        )

        workflow.add_conditional_edges(
            "user_question",
            self._route_after_user_question,
            ["project_director", "result_aggregator"]
        )

        # ============================================================================
        # ç¼–è¯‘å›¾
        # ============================================================================
        logger.info("ðŸ”§ Workflow graph built with dynamic batch execution support")
        logger.info("   Nodes: batch_executor, agent_executor, batch_aggregator, batch_router, batch_strategy_review")
        logger.info("   Supports: 1-N batches with dependency-based execution")

        return workflow.compile(
            checkpointer=self.checkpointer,
            store=self.store
        )
    
    # ============================================================================
    # ðŸ†• å®‰å…¨èŠ‚ç‚¹åŒ…è£…æ–¹æ³•
    # ============================================================================
    
    # ============================================================================
    # ðŸ†• v7.3 ç»Ÿä¸€è¾“å…¥éªŒè¯èŠ‚ç‚¹åŒ…è£…æ–¹æ³•
    # ============================================================================

    def _unified_input_validator_initial_node(self, state: ProjectAnalysisState) -> Command:
        """
        ç»Ÿä¸€è¾“å…¥éªŒè¯ - é˜¶æ®µ1: åˆå§‹éªŒè¯

        æ‰§è¡Œå†…å®¹å®‰å…¨æ£€æµ‹ã€é¢†åŸŸåˆ†ç±»æ£€æµ‹ã€ä»»åŠ¡å¤æ‚åº¦è¯„ä¼°
        """
        try:
            logger.info("Executing unified input validator - initial validation")
            result = UnifiedInputValidatorNode.execute_initial_validation(
                state,
                store=self.store,
                llm_model=self.llm_model
            )

            if not isinstance(result, Command):
                logger.error("UnifiedInputValidatorNode.execute_initial_validation did not return Command object")
                return Command(
                    update={"rejection_reason": "system_error"},
                    goto="input_rejected"
                )

            # æå–çŠ¶æ€æ›´æ–°
            update_payload = dict(result.update or {})
            update_payload["detail"] = "æ£€æµ‹è¾“å…¥å†…å®¹å®‰å…¨æ€§ä¸Žé¢†åŸŸé€‚é…æ€§"

            # è¯»å–å¤æ‚åº¦è¯„ä¼°ç»“æžœ
            suggested_workflow = update_payload.get("suggested_workflow", "full_analysis")
            task_complexity = update_payload.get("task_complexity", "complex")

            # å¦‚æžœåŽŸæœ¬è¦åŽ» input_rejectedï¼Œä¿æŒä¸å˜
            if result.goto == "input_rejected":
                return Command(update=update_payload, goto="input_rejected")

            # ðŸ”„ ç»Ÿä¸€è·¯ç”±ï¼šæ‰€æœ‰ä»»åŠ¡éƒ½èµ°å®Œæ•´æµç¨‹
            # ðŸ”§ v7.1 ä¿®å¤ï¼šä¸å†è‡ªåŠ¨è·³è¿‡é—®å·ï¼Œè®©ç”¨æˆ·å†³å®š
            # åŽŸé€»è¾‘ï¼šmedium å¤æ‚åº¦è·³è¿‡é—®å·ï¼Œä½†ä½Žç½®ä¿¡åº¦çš„é»˜è®¤ medium æ°æ°éœ€è¦é—®å·è¡¥å……ä¿¡æ¯
            # æ–°é€»è¾‘ï¼šæ‰€æœ‰ä»»åŠ¡éƒ½æ˜¾ç¤ºé—®å·ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©è·³è¿‡
            complexity_confidence = update_payload.get("complexity_confidence", 0.5)
            logger.info(f"ðŸ“‹ {task_complexity} ä»»åŠ¡æ£€æµ‹ï¼ˆç½®ä¿¡åº¦: {complexity_confidence:.2f}ï¼‰ï¼Œä½¿ç”¨å®Œæ•´æµç¨‹")
            logger.info(f"   suggested_workflow: {suggested_workflow}")
            # skip_calibration é»˜è®¤ä¸º Falseï¼Œç”±æ ¡å‡†é—®å·èŠ‚ç‚¹è‡ªèº«é€»è¾‘æˆ–ç”¨æˆ·é€‰æ‹©å†³å®š

            return Command(update=update_payload, goto="requirements_analyst")

        except Interrupt:
            # ðŸ”¥ ä¿®å¤ï¼šInterrupt æ˜¯æ­£å¸¸çš„äº¤äº’ä¸­æ–­ï¼Œä¸åº”è¯¥æ•èŽ·ï¼Œåº”è¯¥å‘ä¸Šä¼ æ’­
            logger.info("Input guard node raised an interrupt (user interaction required)")
            raise
        except Exception as e:
            logger.error(f"Error in input guard node: {e}")
            import traceback
            traceback.print_exc()
            # å‡ºé”™æ—¶ä¿å®ˆæ‹’ç»
            return Command(
                update={
                    "rejection_reason": "system_error",
                    "rejection_message": "ç³»ç»Ÿé”™è¯¯ï¼Œè¯·ç¨åŽé‡è¯•ã€‚",
                    "detail": "å®‰å…¨æ£€æµ‹å¼‚å¸¸"
                },
                goto="input_rejected"
            )
    
    def _input_rejected_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """è¾“å…¥æ‹’ç»èŠ‚ç‚¹åŒ…è£…"""
        try:
            logger.info("Executing input rejected node")
            return InputRejectedNode.execute(state, store=self.store)
        except Exception as e:
            logger.error(f"Error in input rejected node: {e}")
            return {
                "final_status": "rejected",
                "rejection_message": "ç³»ç»Ÿé”™è¯¯ï¼Œæµç¨‹ç»ˆæ­¢ã€‚"
            }
    
    def _unified_input_validator_secondary_node(self, state: ProjectAnalysisState) -> Union[Dict[str, Any], Command]:
        """
        ç»Ÿä¸€è¾“å…¥éªŒè¯ - é˜¶æ®µ2: äºŒæ¬¡éªŒè¯

        åœ¨éœ€æ±‚åˆ†æžåŽï¼Œé‡æ–°éªŒè¯é¢†åŸŸä¸€è‡´æ€§ï¼Œæ£€æµ‹é¢†åŸŸæ¼‚ç§»
        """
        from langgraph.types import Interrupt
        try:
            logger.info("Executing unified input validator - secondary validation")
            result = UnifiedInputValidatorNode.execute_secondary_validation(
                state,
                store=self.store,
                llm_model=self.llm_model
            )

            # âœ… æ­£å¸¸æƒ…å†µï¼šè¿”å›žå­—å…¸ï¼ˆç”±é™æ€ edge è·¯ç”±åˆ° calibration_questionnaireï¼‰
            # âœ… æ‹’ç»æƒ…å†µï¼šè¿”å›ž Command(goto="input_rejected")ï¼ˆç»ˆæ­¢å·¥ä½œæµï¼‰
            # âœ… è°ƒæ•´æƒ…å†µï¼šè¿”å›ž Command(goto="requirements_analyst")ï¼ˆé‡æ–°åˆ†æžï¼‰
            if isinstance(result, Command):
                logger.warning("âš ï¸ Secondary validation returned Command (rejection or re-analysis)")
                return result

            # ðŸ”¥ ä¿ç•™ skip_unified_review æ ‡å¿—
            if state.get("skip_unified_review"):
                result["skip_unified_review"] = True
                logger.info("ðŸ” [DEBUG] secondary_validation ä¿ç•™ skip_unified_review=True")

            # æ·»åŠ  detail å­—æ®µ
            result["detail"] = "äºŒæ¬¡éªŒè¯é¢†åŸŸé€‚é…æ€§"

            logger.info("ðŸ”„ [DEBUG] Secondary validation completed, proceeding to calibration_questionnaire")
            return result

        except Interrupt:
            # ðŸ”¥ ä¿®å¤ï¼šInterrupt æ˜¯æ­£å¸¸çš„äº¤äº’ä¸­æ–­ï¼Œä¸åº”è¯¥æ•èŽ·ï¼Œåº”è¯¥å‘ä¸Šä¼ æ’­
            logger.info("Secondary validation raised an interrupt (user interaction required)")
            raise
        except Exception as e:
            logger.error(f"Error in secondary validation node: {e}")
            import traceback
            traceback.print_exc()
            # å‡ºé”™æ—¶ä¿¡ä»»åˆå§‹åˆ¤æ–­
            logger.warning("Secondary validation failed, trusting initial judgment")
            return {
                "secondary_validation_skipped": True,
                "secondary_validation_reason": "error_occurred"
            }
    
    def _report_guard_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """æŠ¥å‘Šå®¡æ ¸èŠ‚ç‚¹åŒ…è£…"""
        try:
            logger.info("Executing report guard node")
            result = ReportGuardNode.execute(state, store=self.store, llm_model=self.llm_model)
            result["detail"] = "å®¡æ ¸æŠ¥å‘Šå†…å®¹"
            return result
        except Exception as e:
            logger.error(f"Error in report guard node: {e}")
            # å‡ºé”™æ—¶æ”¾è¡Œï¼ˆé¿å…è¯¯æ‹¦ï¼‰
            logger.warning("Report guard failed, allowing report to pass")
            return {"report_safety_status": "error_passthrough", "detail": "æŠ¥å‘Šå®¡æ ¸å¼‚å¸¸"}

    def _requirements_analyst_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        éœ€æ±‚åˆ†æžå¸ˆèŠ‚ç‚¹

        æ³¨æ„: åªè¿”å›žéœ€è¦æ›´æ–°çš„å­—æ®µ,ä¸è¿”å›žå®Œæ•´çŠ¶æ€
        è¿™æ ·å¯ä»¥é¿å…å¹¶å‘æ›´æ–°å†²çª
        
        v7.17: æ”¯æŒ StateGraph Agent æ¨¡å¼ï¼ˆé€šè¿‡çŽ¯å¢ƒå˜é‡æŽ§åˆ¶ï¼‰
        """
        try:
            logger.info("Executing requirements analyst node")
            
            # ðŸ” è°ƒè¯•ï¼šæ£€æŸ¥è¿›å…¥æ—¶çš„æ ‡å¿—çŠ¶æ€
            logger.info(f"ðŸ” [DEBUG] requirements_analyst è¿›å…¥æ—¶ calibration_processed: {state.get('calibration_processed')}")
            logger.info(f"ðŸ” [DEBUG] requirements_analyst è¿›å…¥æ—¶ calibration_skipped: {state.get('calibration_skipped')}")
            logger.info(f"ðŸ” [DEBUG] requirements_analyst è¿›å…¥æ—¶ state æ‰€æœ‰é”®: {list(state.keys())}")
            
            # ðŸ” æ·±åº¦è°ƒè¯•ï¼šæ£€æŸ¥ state æ˜¯å¦åŒ…å«è¿™äº›é”®
            if "calibration_processed" in state:
                logger.info(f"ðŸ” [DEBUG] 'calibration_processed' é”®å­˜åœ¨ï¼Œå€¼={state['calibration_processed']}")
            else:
                logger.info("ðŸ” [DEBUG] 'calibration_processed' é”®ä¸å­˜åœ¨")

            # ðŸ†• v7.17: ä½¿ç”¨ StateGraph Agent æ¨¡å¼
            if USE_V717_REQUIREMENTS_ANALYST:
                logger.info("ðŸš€ [v7.17] ä½¿ç”¨ StateGraph Agent æ‰§è¡Œéœ€æ±‚åˆ†æž")
                agent = RequirementsAnalystAgentV2(llm_model=self.llm_model, config=self.config)
                result = agent.execute(
                    user_input=state.get("user_input", ""),
                    session_id=state.get("session_id", "unknown")
                )
                
                # è®°å½• StateGraph æ‰§è¡Œå…ƒæ•°æ®
                if result.metadata:
                    logger.info(f"ðŸ“Š [v7.17] StateGraph æ‰§è¡Œå®Œæˆ:")
                    logger.info(f"   - åˆ†æžæ¨¡å¼: {result.metadata.get('analysis_mode')}")
                    logger.info(f"   - èŠ‚ç‚¹è·¯å¾„: {result.metadata.get('node_path')}")
                    logger.info(f"   - æ€»è€—æ—¶: {result.metadata.get('total_elapsed_ms')}ms")
            else:
                # åŽŸæœ‰é€»è¾‘ï¼šä½¿ç”¨ AgentFactory åˆ›å»º
                agent = AgentFactory.create_agent(
                    AgentType.REQUIREMENTS_ANALYST,
                    llm_model=self.llm_model,
                    config=self.config
                )
                result = agent.execute(state, {}, self.store)

            # ðŸ†• æå–é¡¹ç›®ç±»åž‹ï¼ˆä»Ž structured_data ä¸­ï¼‰
            project_type = result.structured_data.get("project_type") if result.structured_data else None

            # ðŸ†• v7.0: ä»Ž primary_deliverables ä¸­æå–äº¤ä»˜ç‰©å…ƒæ•°æ®å’Œè´£ä»»è€…æ˜ å°„
            deliverable_owner_map = {}
            deliverable_metadata = {}

            if result.structured_data:
                primary_deliverables = result.structured_data.get("primary_deliverables", [])

                for deliverable in primary_deliverables:
                    if not isinstance(deliverable, dict):
                        continue

                    deliverable_id = deliverable.get("deliverable_id", "")
                    if not deliverable_id:
                        continue

                    # æå– owner ä¿¡æ¯
                    owner_suggestion = deliverable.get("deliverable_owner_suggestion", {})
                    primary_owner = owner_suggestion.get("primary_owner", "")
                    supporters = owner_suggestion.get("supporters", [])

                    # ðŸ”§ ä¿®å¤é—®é¢˜1: æ ‡è®°ä¸ºé¢„æµ‹å€¼ï¼ŒåŽç»­ç”± project_director æ ¡æ­£
                    # å¡«å…… deliverable_owner_mapï¼ˆæ­¤æ—¶æ˜¯éœ€æ±‚åˆ†æžå¸ˆçš„é¢„æµ‹ï¼‰
                    if primary_owner:
                        deliverable_owner_map[deliverable_id] = primary_owner

                    # å¡«å…… deliverable_metadataï¼ˆåŒ…å«å®Œæ•´ä¿¡æ¯ï¼‰
                    deliverable_metadata[deliverable_id] = {
                        "name": deliverable.get("description", deliverable_id),
                        "type": deliverable.get("type", "unknown"),
                        "priority": deliverable.get("priority", "MUST_HAVE"),
                        "owner": primary_owner,  # é¢„æµ‹å€¼ï¼Œå¾…æ ¡æ­£
                        "owner_predicted": True,  # ðŸ†• æ ‡è®°ä¸ºé¢„æµ‹å€¼
                        "supporters": supporters,
                        "acceptance_criteria": deliverable.get("acceptance_criteria", []),
                        "format_requirements": deliverable.get("format_requirements", {}),
                        "source_requirement": deliverable.get("source_requirement", "")
                    }

                if deliverable_owner_map:
                    logger.info(f"ðŸŽ¯ [v7.0] æå–åˆ° {len(deliverable_owner_map)} ä¸ªäº¤ä»˜ç‰©è´£ä»»è€…æ˜ å°„ï¼ˆé¢„æµ‹å€¼ï¼‰: {deliverable_owner_map}")

            # åªè¿”å›žéœ€è¦æ›´æ–°çš„å­—æ®µ
            # âœ… ä¿®å¤: ä½¿ç”¨ agent_type.value ä¿æŒä¸€è‡´æ€§
            # ðŸ”§ ä¿®å¤: ä¿ç•™é‡è¦çš„æµç¨‹æŽ§åˆ¶æ ‡å¿—ï¼Œé¿å…å¾ªçŽ¯
            update_dict = {
                "current_stage": AnalysisStage.REQUIREMENT_COLLECTION.value,
                "structured_requirements": result.structured_data,
                "project_type": project_type,  # ðŸ†• æ·»åŠ é¡¹ç›®ç±»åž‹å­—æ®µ
                "deliverable_owner_map": deliverable_owner_map,  # ðŸ†• v7.0: äº¤ä»˜ç‰©è´£ä»»è€…æ˜ å°„
                "deliverable_metadata": deliverable_metadata,  # ðŸ†• v7.0: äº¤ä»˜ç‰©å®Œæ•´å…ƒæ•°æ®
                "agent_results": {
                    AgentType.REQUIREMENTS_ANALYST.value: result.to_dict()
                },
                "updated_at": datetime.now().isoformat()
            }
            
            # ðŸ”§ å…³é”®ä¿®å¤: ä»Žå®Œæ•´çŠ¶æ€ä¸­æå–å¹¶ä¿ç•™æµç¨‹æŽ§åˆ¶æ ‡å¿—
            # æ³¨æ„: Command.update çš„å­—æ®µåœ¨ç›®æ ‡èŠ‚ç‚¹æ‰§è¡Œæ—¶ä¸å¯è§,
            # æ‰€ä»¥è¿™é‡Œéœ€è¦ä»Ž agent_results ä¸­èŽ·å–åŽŸå§‹ state çš„æ ‡å¿—å€¼
            # ä½†å®žé™…ä¸Š,æˆ‘ä»¬åº”è¯¥è®©è¿™äº›æ ‡å¿—"ç©¿é€"æ•´ä¸ªåˆ†æžè¿‡ç¨‹
            full_state = dict(state)  # èŽ·å–å®Œæ•´çŠ¶æ€å‰¯æœ¬
            
            # ðŸ›¡ï¸ å¢žå¼ºä¿®å¤ï¼šæ£€æŸ¥æ ‡å¿— OR æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦å­˜åœ¨
            has_processed = (
                ("calibration_processed" in full_state and full_state["calibration_processed"]) or 
                ("calibration_answers" in full_state and full_state["calibration_answers"])
            )
            
            if has_processed:
                update_dict["calibration_processed"] = True
                logger.info("ðŸ” [DEBUG] ä¿ç•™/æ¢å¤ calibration_processed=True æ ‡å¿—")
                
            if "calibration_skipped" in full_state and full_state["calibration_skipped"]:
                update_dict["calibration_skipped"] = True
                logger.info("ðŸ” [DEBUG] ä¿ç•™ calibration_skipped=True æ ‡å¿—")
            # ðŸ”¥ å…³é”®ä¿®å¤ï¼šä¿ç•™ modification_confirmation_round è½®æ¬¡è®¡æ•°
            if "modification_confirmation_round" in full_state:
                update_dict["modification_confirmation_round"] = full_state["modification_confirmation_round"]
                logger.info(f"ðŸ” [DEBUG] ä¿ç•™ modification_confirmation_round={full_state['modification_confirmation_round']} æ ‡å¿—")
            # ðŸ”¥ å…³é”®ä¿®å¤ï¼šä¿ç•™ skip_unified_review æ ‡å¿—
            if "skip_unified_review" in full_state and full_state["skip_unified_review"]:
                update_dict["skip_unified_review"] = True
                logger.info("ðŸ” [DEBUG] ä¿ç•™ skip_unified_review=True æ ‡å¿—")
            
            logger.info(f"ðŸ” [DEBUG] requirements_analyst è¿”å›žçš„å­—æ®µ: {list(update_dict.keys())}")
            return update_dict

        except Exception as e:
            logger.error(f"Requirements analyst node failed: {e}")
            import traceback
            traceback.print_exc()
            return {
                "error": str(e),
                "updated_at": datetime.now().isoformat()
            }

    def _feasibility_analyst_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        V1.5 å¯è¡Œæ€§åˆ†æžå¸ˆèŠ‚ç‚¹ï¼ˆåŽå°å†³ç­–æ”¯æŒï¼‰

        èŒè´£:
        1. åŸºäºŽV1çš„structured_requirementsè¿›è¡Œå¯è¡Œæ€§åˆ†æž
        2. æ£€æµ‹é¢„ç®—/æ—¶é—´/ç©ºé—´å†²çª
        3. è®¡ç®—éœ€æ±‚ä¼˜å…ˆçº§
        4. ç”Ÿæˆå†³ç­–å»ºè®®
        5. å°†ç»“æžœå­˜å‚¨åˆ°state.feasibility_assessmentï¼ˆä¸å±•ç¤ºåœ¨å‰ç«¯ï¼‰
        6. åŽç»­åœ¨project_directorä¸­ç”¨äºŽæŒ‡å¯¼ä¸“å®¶ä»»åŠ¡åˆ†æ´¾
        """
        try:
            logger.info("Executing V1.5 feasibility analyst node")

            # åˆ›å»ºV1.5å¯è¡Œæ€§åˆ†æžå¸ˆæ™ºèƒ½ä½“
            feasibility_agent = FeasibilityAnalystAgent(
                llm_model=self.llm_model,
                config=self.config
            )

            # éªŒè¯è¾“å…¥ï¼šV1çš„structured_requirementså¿…é¡»å­˜åœ¨
            if not state.get("structured_requirements"):
                logger.warning("âš ï¸ V1.5è·³è¿‡ï¼šstructured_requirementsä¸å­˜åœ¨")
                return {"updated_at": datetime.now().isoformat()}

            # æ‰§è¡Œå¯è¡Œæ€§åˆ†æž
            result = feasibility_agent.execute(state, {}, self.store)

            # å­˜å‚¨åˆ†æžç»“æžœåˆ°stateï¼ˆä»…åŽå°å­˜å‚¨ï¼Œä¸å±•ç¤ºåˆ°å‰ç«¯ï¼‰
            update_dict = {
                "feasibility_assessment": result.structured_data,  # å®Œæ•´çš„å¯è¡Œæ€§åˆ†æžç»“æžœ
                "updated_at": datetime.now().isoformat()
            }

            # æ—¥å¿—è®°å½•å…³é”®å‘çŽ°ï¼ˆç”¨äºŽè°ƒè¯•å’Œç›‘æŽ§ï¼‰
            if result.structured_data:
                feasibility = result.structured_data.get("feasibility_assessment", {})
                overall = feasibility.get("overall_feasibility", "unknown")
                conflicts = result.structured_data.get("conflict_detection", {})

                logger.info(f"âœ… V1.5å¯è¡Œæ€§åˆ†æžå®Œæˆ: overall_feasibility={overall}")

                # è®°å½•å†²çªæ£€æµ‹ç»“æžœ
                budget_conflicts = conflicts.get("budget_conflicts", [])
                if budget_conflicts and budget_conflicts[0].get("detected"):
                    severity = budget_conflicts[0].get("severity", "unknown")
                    logger.info(f"âš ï¸ é¢„ç®—å†²çªæ£€æµ‹: severity={severity}")

                # è®°å½•ä¼˜å…ˆçº§çŸ©é˜µ
                priority_matrix = result.structured_data.get("priority_matrix", [])
                if priority_matrix:
                    top_req = priority_matrix[0]
                    logger.info(f"ðŸŽ¯ æœ€é«˜ä¼˜å…ˆçº§éœ€æ±‚: {top_req.get('requirement')} (score={top_req.get('priority_score')})")

            return update_dict

        except Exception as e:
            logger.error(f"V1.5 Feasibility analyst node failed: {e}")
            import traceback
            traceback.print_exc()
            return {
                "error": str(e),
                "updated_at": datetime.now().isoformat()
            }

    def _calibration_questionnaire_node(self, state: ProjectAnalysisState) -> Command:
        """
        æˆ˜ç•¥æ ¡å‡†é—®å·èŠ‚ç‚¹ï¼ˆæ—§ç‰ˆå•è½®é—®å·ï¼‰

        æ ¹æ®éœ€æ±‚åˆ†æžå¸ˆæ–‡æ¡£v1.0çš„è¦æ±‚ï¼š
        åœ¨å®Œæˆéœ€æ±‚åˆ†æžåŽï¼Œå¿…é¡»ç”Ÿæˆ"æˆ˜ç•¥æ ¡å‡†é—®å·"å¹¶ç­‰å¾…ç”¨æˆ·å›žç­”ã€‚

        æ³¨æ„: ä¸è¦æ•èŽ·Interruptå¼‚å¸¸!
        Interruptæ˜¯LangGraphçš„æ­£å¸¸æŽ§åˆ¶æµ,å¿…é¡»è®©å®ƒä¼ æ’­åˆ°æ¡†æž¶å±‚
        """
        logger.info("Executing calibration questionnaire node (legacy single-round)")
        return CalibrationQuestionnaireNode.execute(state, self.store)

    # ðŸ†• v7.87: ä¸‰æ­¥é€’è¿›å¼é—®å·èŠ‚ç‚¹å‡½æ•°
    def _progressive_step1_node(self, state: ProjectAnalysisState) -> Command:
        """Step 1: æ ¸å¿ƒä»»åŠ¡æ‹†è§£ä¸Žç¡®è®¤"""
        logger.info("ðŸŽ¯ [v7.87 Step 1] Executing progressive questionnaire - Core Task Decomposition")
        return progressive_step1_core_task_node(state, self.store)

    def _progressive_step2_node(self, state: ProjectAnalysisState) -> Command:
        """Step 2: é›·è¾¾å›¾ç»´åº¦é€‰æ‹©"""
        logger.info("ðŸŽ¯ [v7.87 Step 2] Executing progressive questionnaire - Radar Dimension Selection")
        return progressive_step2_radar_node(state, self.store)

    def _progressive_step3_node(self, state: ProjectAnalysisState) -> Command:
        """Step 3: å·®è·å¡«è¡¥è¿½é—®"""
        logger.info("ðŸŽ¯ [v7.87 Step 3] Executing progressive questionnaire - Gap Filling")
        return progressive_step3_gap_filling_node(state, self.store)

    def _requirements_confirmation_node(self, state: ProjectAnalysisState) -> Command:
        """
        éœ€æ±‚ç¡®è®¤èŠ‚ç‚¹

        æ³¨æ„: ä¸è¦æ•èŽ·Interruptå¼‚å¸¸!
        Interruptæ˜¯LangGraphçš„æ­£å¸¸æŽ§åˆ¶æµ,å¿…é¡»è®©å®ƒä¼ æ’­åˆ°æ¡†æž¶å±‚
        """
        logger.info("Executing requirements confirmation node")
        return RequirementsConfirmationNode.execute(state, self.store)
    
    def _find_matching_role(self, predicted_role: str, active_agents: List[str]) -> Optional[str]:
        """
        ðŸ”§ ä¿®å¤é—®é¢˜1: æŸ¥æ‰¾é¢„æµ‹è§’è‰²åœ¨å®žé™…é€‰å®šè§’è‰²ä¸­çš„åŒ¹é…é¡¹

        Args:
            predicted_role: éœ€æ±‚åˆ†æžå¸ˆé¢„æµ‹çš„è§’è‰²IDï¼ˆå¦‚ "V2_è®¾è®¡ç­–ç•¥ä¸“å®¶_3-2"ï¼‰
            active_agents: é¡¹ç›®æ€»ç›‘å®žé™…é€‰å®šçš„è§’è‰²åˆ—è¡¨ï¼ˆå¦‚ ["V2_è®¾è®¡æ€»ç›‘_2-2", ...]ï¼‰

        Returns:
            åŒ¹é…çš„å®žé™…è§’è‰²IDï¼Œå¦‚æžœæœªæ‰¾åˆ°åˆ™è¿”å›žNone

        åŒ¹é…è§„åˆ™ï¼š
        1. ç²¾ç¡®åŒ¹é…ï¼ˆpredicted_role in active_agentsï¼‰
        2. å‰ç¼€åŒ¹é…ï¼ˆæå– V2_è®¾è®¡ å‰ç¼€ï¼ŒæŸ¥æ‰¾ä»¥æ­¤å¼€å¤´çš„è§’è‰²ï¼‰
        3. å±‚çº§åŒ¹é…ï¼ˆæå– V2 å±‚çº§ï¼ŒæŸ¥æ‰¾åŒå±‚çº§è§’è‰²ï¼‰
        """
        # è§„åˆ™1: ç²¾ç¡®åŒ¹é…
        if predicted_role in active_agents:
            return predicted_role

        # è§„åˆ™2: å‰ç¼€åŒ¹é…ï¼ˆV2_è®¾è®¡ï¼‰
        parts = predicted_role.split("_")
        if len(parts) >= 2:
            prefix = f"{parts[0]}_{parts[1]}"  # V2_è®¾è®¡

            for agent_id in active_agents:
                if agent_id.startswith(prefix):
                    logger.debug(f"ðŸ” [åŒ¹é…] å‰ç¼€åŒ¹é…: {predicted_role} â†’ {agent_id}")
                    return agent_id

        # è§„åˆ™3: å±‚çº§åŒ¹é…ï¼ˆV2ï¼‰
        if len(parts) >= 1:
            level_prefix = parts[0]  # V2

            for agent_id in active_agents:
                if agent_id.startswith(level_prefix):
                    logger.debug(f"ðŸ” [åŒ¹é…] å±‚çº§åŒ¹é…: {predicted_role} â†’ {agent_id}")
                    return agent_id

        logger.warning(f"âš ï¸ [åŒ¹é…å¤±è´¥] æœªæ‰¾åˆ° {predicted_role} çš„åŒ¹é…è§’è‰²")
        return None

    def _project_director_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        é¡¹ç›®æ€»ç›‘èŠ‚ç‚¹ - è¿›è¡Œæˆ˜ç•¥åˆ†æžå¹¶å‡†å¤‡å¹¶è¡Œä»»åŠ¡ï¼ˆä»… Dynamic Modeï¼‰

        æ³¨æ„:
        - ProjectDirectorè¿”å›žCommandå¯¹è±¡,ä½†æˆ‘ä»¬éœ€è¦æå–çŠ¶æ€æ›´æ–°
        - ä»…æ”¯æŒåŠ¨æ€æ¨¡å¼: ä½¿ç”¨ active_agents å’Œ execution_mode
        """
        try:
            logger.info("Executing project director node (Dynamic Mode)")

            # åˆ›å»ºé¡¹ç›®æ€»ç›‘æ™ºèƒ½ä½“
            agent = AgentFactory.create_agent(
                AgentType.PROJECT_DIRECTOR,
                llm_model=self.llm_model,
                config=self.config
            )

            # æ‰§è¡Œåˆ†æž - è¿”å›žCommandå¯¹è±¡
            command = agent.execute(state, {}, self.store)

            # æå–çŠ¶æ€æ›´æ–°
            if isinstance(command, Command):
                state_update = command.update or {}
            else:
                logger.error(f"Unexpected return type from ProjectDirector: {type(command)}")
                state_update = {"error": "ProjectDirector returned unexpected type"}

            # ðŸ”¥ ä¿ç•™å…³é”®çŠ¶æ€æ ‡å¿—ï¼ˆé¿å…è¢«è¦†ç›–ï¼‰
            if state.get("skip_unified_review"):
                state_update["skip_unified_review"] = True
                logger.info("ðŸ”„ ä¿ç•™ skip_unified_review æ ‡å¿—")
            else:
                logger.warning("âš ï¸ [DEBUG] project_director è¿›å…¥æ—¶ skip_unified_review ä¸å­˜åœ¨æˆ–ä¸ºFalse")
                logger.info(f"ðŸ” [DEBUG] project_director è¾“å…¥ state keys: {list(state.keys())}")

            # åŠ¨æ€æ¨¡å¼: ä½¿ç”¨ active_agents
            active_agents = state_update.get("active_agents", [])
            logger.info(f"ðŸ“Œ Dynamic mode: Project director selected {len(active_agents)} dynamic roles")
            logger.debug(f"Active agents: {active_agents}")

            # ðŸ”§ ä¿®å¤é—®é¢˜1: æ ¡æ­£äº¤ä»˜ç‰©è´£ä»»è€…æ˜ å°„
            deliverable_metadata = state.get("deliverable_metadata") or {}
            deliverable_owner_map = state.get("deliverable_owner_map") or {}

            if deliverable_metadata and active_agents:
                corrected_owner_map = {}
                corrected_metadata = {}

                for deliverable_id, metadata in deliverable_metadata.items():
                    predicted_owner = metadata.get("owner", "")

                    if metadata.get("owner_predicted") and predicted_owner:
                        # æŸ¥æ‰¾å®žé™…é€‰å®šçš„åŒ¹é…è§’è‰²
                        actual_owner = self._find_matching_role(predicted_owner, active_agents)

                        if actual_owner and actual_owner != predicted_owner:
                            logger.info(f"ðŸ”§ [ä¿®å¤] äº¤ä»˜ç‰© {deliverable_id} è´£ä»»è€…æ ¡æ­£: {predicted_owner} â†’ {actual_owner}")
                            corrected_owner_map[deliverable_id] = actual_owner

                            # æ›´æ–°å…ƒæ•°æ®
                            corrected_metadata[deliverable_id] = {
                                **metadata,
                                "owner": actual_owner,
                                "owner_predicted": False,  # æ ‡è®°ä¸ºå·²æ ¡æ­£
                                "owner_original_prediction": predicted_owner  # ä¿ç•™åŽŸå§‹é¢„æµ‹
                            }
                        else:
                            # æœªæ‰¾åˆ°åŒ¹é…æˆ–å·²åŒ¹é…ï¼Œä¿æŒåŽŸå€¼
                            corrected_owner_map[deliverable_id] = predicted_owner
                            corrected_metadata[deliverable_id] = metadata
                    else:
                        # éžé¢„æµ‹å€¼ï¼Œä¿æŒåŽŸå€¼
                        corrected_owner_map[deliverable_id] = predicted_owner
                        corrected_metadata[deliverable_id] = metadata

                # æ›´æ–°çŠ¶æ€
                if corrected_owner_map:
                    state_update["deliverable_owner_map"] = corrected_owner_map
                    state_update["deliverable_metadata"] = corrected_metadata
                    logger.info(f"âœ… [ä¿®å¤] å·²æ ¡æ­£ {len(corrected_owner_map)} ä¸ªäº¤ä»˜ç‰©è´£ä»»è€…æ˜ å°„")

            # ç¡®ä¿ execution_mode è¢«è®¾ç½®ä¸º dynamic
            state_update["execution_mode"] = "dynamic"

            logger.info(f"Project director completed, prepared {len(active_agents)} agents for execution")

            # åªè¿”å›žçŠ¶æ€æ›´æ–°,ä¸è¿”å›žCommand
            # è·¯ç”±ç”±æ¡ä»¶è¾¹å¤„ç†
            state_update.setdefault("detail", f"è§„åˆ’ä¸“å®¶å›¢é˜Ÿå¹¶åˆ›å»º {len(active_agents)} åä¸“å®¶æ‰¹æ¬¡")
            return state_update

        except Exception as e:
            logger.error(f"Project director node failed: {e}")
            import traceback
            traceback.print_exc()
            
            # ðŸ”¥ è¿”å›žæ˜Žç¡®çš„é”™è¯¯çŠ¶æ€ï¼Œé˜²æ­¢ä¸‹æ¸¸èŠ‚ç‚¹å´©æºƒ
            return {
                "error": str(e),
                "strategic_analysis": None,  # æ˜Žç¡®æ ‡è®°ä¸º None
                "active_agents": [],
                "execution_mode": "dynamic",
                "errors": [{"node": "project_director", "error": str(e), "type": "ProjectDirectorFailure"}]
            }

    def _deliverable_id_generator_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        ðŸ†• v7.108 äº¤ä»˜ç‰©IDç”ŸæˆèŠ‚ç‚¹

        åœ¨project_directorä¹‹åŽã€role_task_unified_reviewä¹‹å‰æ‰§è¡Œï¼Œ
        ä¸ºæ‰€æœ‰é€‰å®šè§’è‰²é¢„ç”Ÿæˆäº¤ä»˜ç‰©IDï¼Œä»¥ä¾¿åŽç»­ä¸“å®¶æ‰§è¡Œé˜¶æ®µç”Ÿæˆæ¦‚å¿µå›¾æ—¶ä½¿ç”¨ã€‚

        Returns:
            åŒ…å«deliverable_metadataå’Œdeliverable_owner_mapçš„çŠ¶æ€æ›´æ–°
        """
        try:
            logger.info("ðŸ“‹ [deliverable_id_generator] å¼€å§‹ç”Ÿæˆäº¤ä»˜ç‰©ID...")

            # å¯¼å…¥èŠ‚ç‚¹å‡½æ•°
            from ..workflow.nodes.deliverable_id_generator_node import deliverable_id_generator_node

            # æ‰§è¡Œäº¤ä»˜ç‰©IDç”Ÿæˆ
            result = deliverable_id_generator_node(state)

            # è®°å½•ç”Ÿæˆç»“æžœ
            total_deliverables = len(result.get("deliverable_metadata", {}))
            logger.info(f"âœ… [deliverable_id_generator] æˆåŠŸç”Ÿæˆ {total_deliverables} ä¸ªäº¤ä»˜ç‰©ID")

            return result

        except Exception as e:
            logger.error(f"âŒ [deliverable_id_generator] ç”Ÿæˆäº¤ä»˜ç‰©IDå¤±è´¥: {e}")
            logger.exception(e)
            return {
                "deliverable_metadata": {},
                "deliverable_owner_map": {},
                "detail": f"äº¤ä»˜ç‰©IDç”Ÿæˆå¤±è´¥: {str(e)}"
            }

    def _role_task_unified_review_node(self, state: ProjectAnalysisState):
        """
        ðŸ†• è§’è‰²ä»»åŠ¡ç»Ÿä¸€å®¡æ ¸èŠ‚ç‚¹

        åˆå¹¶è§’è‰²é€‰æ‹©å®¡æ ¸å’Œä»»åŠ¡åˆ†æ´¾å®¡æ ¸ï¼Œå‡å°‘äººæœºäº¤äº’æ¬¡æ•°

        æ³¨æ„: ä¸è¦æ•èŽ·Interruptå¼‚å¸¸!
        Interruptæ˜¯LangGraphçš„æ­£å¸¸æŽ§åˆ¶æµ,å¿…é¡»è®©å®ƒä¼ æ’­åˆ°æ¡†æž¶å±‚
        
        è¿”å›ž: Commandå¯¹è±¡ï¼ˆç”¨äºŽåŠ¨æ€è·¯ç”±åˆ° batch_executor æˆ– project_directorï¼‰
        """
        try:
            logger.info("ðŸ” Executing unified role & task review node")
            return role_task_unified_review_node(state)  # è¿”å›žCommand
        except Exception as e:
            # åªæ•èŽ·éžInterruptå¼‚å¸¸
            if "Interrupt" not in str(type(e)):
                logger.error(f"âŒ Unified review node failed: {e}")
                import traceback
                traceback.print_exc()
                return {"error": str(e)}
            else:
                # é‡æ–°æŠ›å‡ºInterruptå¼‚å¸¸
                raise

    # ============================================================================
    # ðŸ—‘ï¸ å·²åºŸå¼ƒçš„ç‹¬ç«‹å®¡æ ¸èŠ‚ç‚¹ï¼ˆä¿ç•™æ³¨é‡Šä½œä¸ºåŽ†å²å‚è€ƒï¼‰
    # ============================================================================
    # def _role_selection_review_node(self, state: ProjectAnalysisState):
    #     """è§’è‰²é€‰æ‹©å®¡æ ¸èŠ‚ç‚¹ - å·²åˆå¹¶åˆ° role_task_unified_review"""
    #     pass
    
    # def _task_assignment_review_node(self, state: ProjectAnalysisState):
    #     """ä»»åŠ¡åˆ†æ´¾å®¡æ ¸èŠ‚ç‚¹ - å·²åˆå¹¶åˆ° role_task_unified_review"""
    #     pass
    

    async def _quality_preflight_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        è´¨é‡é¢„æ£€èŠ‚ç‚¹ - å‰ç½®é¢„é˜²ç¬¬1å±‚ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰

        ðŸš€ P1ä¼˜åŒ–ï¼šä½¿ç”¨å¼‚æ­¥è°ƒç”¨ï¼Œé…åˆ QualityPreflightNode çš„å¼‚æ­¥å®žçŽ°
        ðŸ”¥ v7.16: æ”¯æŒæ–°ç‰ˆ LangGraph QualityPreflightAgent
        ðŸ”§ v7.23: ä¿®å¤ interrupt è¢«é”™è¯¯æ•èŽ·çš„é—®é¢˜

        åœ¨ä¸“å®¶æ‰§è¡Œå‰è¿›è¡Œ:
        1. é£Žé™©é¢„åˆ¤
        2. è´¨é‡æ£€æŸ¥æ¸…å•ç”Ÿæˆ
        3. èƒ½åŠ›åŒ¹é…åº¦éªŒè¯
        """
        try:
            logger.info("ðŸ” Executing quality preflight node (async)")
            
            # ðŸ†• v7.16: ä½¿ç”¨æ–°ç‰ˆ LangGraph Agentï¼ˆå¦‚æžœå¯ç”¨ï¼‰
            if USE_V716_AGENTS:
                logger.info("ðŸš€ [v7.16] ä½¿ç”¨ QualityPreflightAgent")
                node = QualityPreflightNodeCompat(self.llm_model)
            else:
                node = QualityPreflightNode(self.llm_model)
            
            result = await node(state)  # ðŸš€ P1ä¼˜åŒ–ï¼šä½¿ç”¨ await è°ƒç”¨å¼‚æ­¥æ–¹æ³•
            result["detail"] = "è´¨é‡é¢„æ£€ä¸Žé£Žé™©è¯„ä¼°"
            return result
        except Exception as e:
            # ðŸ”§ v7.24: å¢žå¼º Interrupt æ£€æµ‹ï¼Œæ”¯æŒå¤šç§å¼‚å¸¸æ ¼å¼
            from langgraph.types import Interrupt
            
            # æ£€æµ‹ Interrupt çš„å¤šç§æƒ…å†µï¼š
            # 1. ç›´æŽ¥æ˜¯ Interrupt ç±»åž‹
            # 2. e.args[0] æ˜¯ Interruptï¼ˆéƒ¨åˆ† LangGraph ç‰ˆæœ¬ï¼‰
            # 3. e æ˜¯åŒ…å« Interrupt çš„ tupleï¼ˆæŸäº›å¼‚å¸¸åŒ…è£…æƒ…å†µï¼‰
            is_interrupt = False
            if isinstance(e, Interrupt):
                is_interrupt = True
            elif hasattr(e, 'args') and e.args:
                if isinstance(e.args[0], Interrupt):
                    is_interrupt = True
                elif isinstance(e.args[0], tuple) and e.args[0] and isinstance(e.args[0][0], Interrupt):
                    is_interrupt = True
            
            if is_interrupt:
                logger.info("ðŸ”„ Quality preflight interrupt triggered, pausing for user confirmation")
                raise  # é‡æ–°æŠ›å‡º Interruptï¼Œè®© LangGraph æ­£å¸¸å¤„ç†
            
            # ðŸ”§ v7.24: é¢å¤–æ£€æŸ¥ - æ£€æŸ¥é”™è¯¯æ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å« Interrupt å…³é”®å­—
            error_str = str(e)
            if 'Interrupt' in error_str and 'value=' in error_str:
                logger.warning(f"âš ï¸ Detected Interrupt in error message, re-raising: {error_str[:100]}...")
                raise
            
            logger.error(f"âŒ Quality preflight node failed: {e}")
            import traceback
            traceback.print_exc()
            return {"preflight_completed": False, "error": str(e), "detail": "è´¨é‡é¢„æ£€å¤±è´¥"}

    def _continue_to_first_batch_agents(self, state: ProjectAnalysisState) -> List[Send]:
        """
        è·¯ç”±å‡½æ•° - åˆ›å»ºç¬¬ä¸€æ‰¹å¹¶è¡Œä»»åŠ¡(V3-V4-V5)ï¼ˆä»… Dynamic Modeï¼‰

        ç¬¬ä¸€æ‰¹ä¸“å®¶:
        - V3: æŠ€æœ¯æž¶æž„å¸ˆ
        - V4: ç”¨æˆ·ä½“éªŒè®¾è®¡å¸ˆ
        - V5: å•†ä¸šåˆ†æžå¸ˆ

        è¿™äº›ä¸“å®¶çš„åˆ†æžç»“æžœå°†è¢«V2å’ŒV6ä½¿ç”¨

        æ”¯æŒé’ˆå¯¹æ€§é‡æ‰§è¡Œï¼š
        - å¦‚æžœ state ä¸­æœ‰ specific_agents_to_runï¼Œåªæ‰§è¡ŒæŒ‡å®šçš„ä¸“å®¶
        """
        try:
            logger.info("Creating first batch parallel tasks (V3-V4-V5) using Send API (Dynamic Mode)")

            # æ£€æŸ¥æ˜¯å¦æœ‰æŒ‡å®šéœ€è¦é‡æ–°æ‰§è¡Œçš„ä¸“å®¶
            specific_agents = state.get("specific_agents_to_run", [])

            # åŠ¨æ€æ¨¡å¼: ä»Ž active_agents ä¸­ç­›é€‰ç¬¬ä¸€æ‰¹ä¸“å®¶ (V3, V4, V5)
            active_agents = state.get("active_agents", [])
            logger.info(f"ðŸŽ¯ Dynamic mode: filtering first batch from {len(active_agents)} active agents")

            # ç¬¬ä¸€æ‰¹ä¸“å®¶: ä»¥ V3_, V4_, V5_ å¼€å¤´çš„è§’è‰²
            first_batch_roles = [
                role_id for role_id in active_agents
                if role_id.startswith("V3_") or role_id.startswith("V4_") or role_id.startswith("V5_")
            ]

            logger.info(f"Preparing to execute {len(first_batch_roles)} dynamic agents in first batch")
            logger.debug(f"First batch roles: {first_batch_roles}")

            # åˆ›å»ºSendå¯¹è±¡åˆ—è¡¨
            send_list = []
            for role_id in first_batch_roles:
                # åˆ›å»ºä¸€ä¸ªåŒ…å«role_idçš„æ–°çŠ¶æ€
                agent_state = dict(state)  # å¤åˆ¶å®Œæ•´çŠ¶æ€
                agent_state["role_id"] = role_id  # æ·»åŠ role_id (åŠ¨æ€æ¨¡å¼)
                agent_state["current_stage"] = AnalysisStage.PARALLEL_ANALYSIS.value
                agent_state["execution_batch"] = "first"  # æ ‡è®°ä¸ºç¬¬ä¸€æ‰¹
                agent_state["is_rerun"] = bool(specific_agents)  # æ ‡è®°æ˜¯å¦ä¸ºé‡æ–°æ‰§è¡Œ

                # ðŸ”§ ä¿®å¤ (2025-11-19): ä½¿ç”¨æ­£ç¡®çš„èŠ‚ç‚¹å agent_executor
                send_list.append(Send("agent_executor", agent_state))
                logger.debug(f"Created Send for first batch dynamic agent: {role_id}")

            return send_list

        except Exception as e:
            logger.error(f"Failed to create first batch parallel tasks: {e}")
            import traceback
            traceback.print_exc()
            return {
                "error": str(e),
                "strategic_analysis": None,  # æ˜Žç¡®æ ‡è®°ä¸º None
                "active_agents": [],
                "detail": "é¡¹ç›®æ€»ç›‘åˆ†æžå¤±è´¥"
            }

    def _continue_to_second_batch_agents(self, state: ProjectAnalysisState) -> List[Send]:
        """
        è·¯ç”±å‡½æ•° - åˆ›å»ºç¬¬äºŒæ‰¹å¹¶è¡Œä»»åŠ¡(V2-V6)ï¼ˆä»… Dynamic Modeï¼‰

        ç¬¬äºŒæ‰¹ä¸“å®¶:
        - V2: è®¾è®¡ç ”ç©¶åˆ†æžå¸ˆ (ä¾èµ–V3-V4-V5çš„è¾“å‡º)
        - V6: å®žæ–½è§„åˆ’å¸ˆ (ä¾èµ–V3-V4-V5çš„è¾“å‡º)

        è¿™äº›ä¸“å®¶å¯ä»¥è®¿é—® state["agent_results"] ä¸­ V3-V4-V5 çš„åˆ†æžç»“æžœ

        æ”¯æŒé’ˆå¯¹æ€§é‡æ‰§è¡Œ:
        - å¦‚æžœ state ä¸­æœ‰ specific_agents_to_runï¼Œåªæ‰§è¡ŒæŒ‡å®šçš„ä¸“å®¶
        """
        try:
            logger.info("Creating second batch parallel tasks (V2-V6) using Send API (Dynamic Mode)")

            # æ£€æŸ¥æ˜¯å¦æœ‰æŒ‡å®šéœ€è¦é‡æ–°æ‰§è¡Œçš„ä¸“å®¶
            specific_agents = state.get("specific_agents_to_run", [])

            # åŠ¨æ€æ¨¡å¼: ä»Ž active_agents ä¸­ç­›é€‰ç¬¬äºŒæ‰¹ä¸“å®¶ (V2, V6)
            active_agents = state.get("active_agents", [])
            logger.info(f"ðŸŽ¯ Dynamic mode: filtering second batch from {len(active_agents)} active agents")

            # ç¬¬äºŒæ‰¹ä¸“å®¶: ä»¥ V2_, V6_ å¼€å¤´çš„è§’è‰²
            second_batch_roles = [
                role_id for role_id in active_agents
                if role_id.startswith("V2_") or role_id.startswith("V6_")
            ]

            logger.info(f"Preparing to execute {len(second_batch_roles)} dynamic agents in second batch")
            logger.debug(f"Second batch roles: {second_batch_roles}")

            # éªŒè¯ç¬¬ä¸€æ‰¹ç»“æžœæ˜¯å¦å­˜åœ¨
            agent_results = state.get("agent_results", {})
            first_batch_roles = [
                role_id for role_id in active_agents
                if role_id.startswith("V3_") or role_id.startswith("V4_") or role_id.startswith("V5_")
            ]

            first_batch_completed = all(
                role_id in agent_results
                for role_id in first_batch_roles
            )

            if not first_batch_completed:
                logger.warning(f"âš ï¸ First batch agents not all completed, but proceeding with second batch")
                logger.debug(f"Expected: {first_batch_roles}, Got: {list(agent_results.keys())}")

            # åˆ›å»ºSendå¯¹è±¡åˆ—è¡¨
            send_list = []
            for role_id in second_batch_roles:
                # åˆ›å»ºä¸€ä¸ªåŒ…å«role_idçš„æ–°çŠ¶æ€
                agent_state = dict(state)  # å¤åˆ¶å®Œæ•´çŠ¶æ€
                agent_state["role_id"] = role_id  # æ·»åŠ role_id (åŠ¨æ€æ¨¡å¼)
                agent_state["current_stage"] = AnalysisStage.PARALLEL_ANALYSIS.value
                agent_state["execution_batch"] = "second"  # æ ‡è®°ä¸ºç¬¬äºŒæ‰¹
                agent_state["is_rerun"] = bool(specific_agents)  # æ ‡è®°æ˜¯å¦ä¸ºé‡æ–°æ‰§è¡Œ

                # ðŸ”§ ä¿®å¤ (2025-11-19): ä½¿ç”¨æ­£ç¡®çš„èŠ‚ç‚¹å agent_executor
                send_list.append(Send("agent_executor", agent_state))
                logger.debug(f"Created Send for second batch dynamic agent: {role_id}")

            return send_list

        except Exception as e:
            logger.error(f"Failed to create second batch parallel tasks: {e}")
            import traceback
            traceback.print_exc()
            return []

    async def _execute_agent_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•ä¸ªæ™ºèƒ½ä½“èŠ‚ç‚¹ï¼ˆä»… Dynamic Modeï¼‰

        æ ¹æ®LangGraph Send API,è¿™ä¸ªèŠ‚ç‚¹æŽ¥æ”¶çš„stateæ˜¯é€šè¿‡Sendä¼ é€’çš„å®Œæ•´çŠ¶æ€
        çŠ¶æ€ä¸­åº”è¯¥åŒ…å«role_idå­—æ®µ,æŒ‡ç¤ºè¦æ‰§è¡Œå“ªä¸ªæ™ºèƒ½ä½“

        âœ¨ é›†æˆå®žæ—¶è´¨é‡ç›‘æŽ§ï¼ˆç¬¬2å±‚é¢„é˜²ï¼‰:
        - æ‰§è¡Œå‰ï¼šæ³¨å…¥è´¨é‡çº¦æŸåˆ°prompt
        - æ‰§è¡ŒåŽï¼šå¿«é€ŸéªŒè¯è¾“å‡ºè´¨é‡
        - å¦‚æžœè´¨é‡ä¸è¾¾æ ‡ï¼šç»™äºˆä¸€æ¬¡é‡è¯•æœºä¼š

        é‡è¦:
        - åªè¿”å›žéœ€è¦æ›´æ–°çš„å­—æ®µ,ä¸è¦è¿”å›žå®Œæ•´çŠ¶æ€
        - agent_resultsä½¿ç”¨äº†Annotated[Dict, merge_agent_results]
        - å¤šä¸ªå¹¶å‘èŠ‚ç‚¹çš„agent_resultsä¼šè¢«è‡ªåŠ¨åˆå¹¶
        """
        try:
            # èŽ·å– role_id
            role_id = state.get("role_id")

            if not role_id:
                logger.error("âŒ No role_id specified in state")
                logger.debug(f"State keys: {list(state.keys())}")
                return {"error": "No role_id specified"}

            # èŽ·å–è´¨é‡æ£€æŸ¥æ¸…å•
            quality_checklists = state.get("quality_checklists", {})
            quality_checklist = quality_checklists.get(role_id, {})

            # èŽ·å–é‡è¯•ä¿¡æ¯
            review_round = state.get("review_round", 0)
            review_feedback = state.get("review_feedback")
            retry_count = state.get(f"retry_count_{role_id}", 0)  # ðŸ†• é‡è¯•è®¡æ•°

            # åŠ¨æ€æ¨¡å¼: ä½¿ç”¨ TaskOrientedExpertFactory (v2.0)
            logger.info(f"ðŸŽ¯ Executing task-oriented agent: {role_id} (è½®æ¬¡{review_round}, é‡è¯•{retry_count})")

            # å¯¼å…¥ä»»åŠ¡å¯¼å‘ä¸“å®¶å·¥åŽ‚
            from intelligent_project_analyzer.agents.task_oriented_expert_factory import TaskOrientedExpertFactory
            from intelligent_project_analyzer.core.role_manager import RoleManager

            # åˆå§‹åŒ–è§’è‰²ç®¡ç†å™¨
            role_manager = RoleManager()

            # è§£æžè§’è‰²ID
            logger.info(f"ðŸ” [DEBUG] Parsing role_id: {role_id}")
            base_type, rid = role_manager.parse_full_role_id(role_id)
            logger.info(f"ðŸ” [DEBUG] Parsed: base_type={base_type}, rid={rid}")

            # âœ… å°è¯•ç›´æŽ¥èŽ·å–é…ç½®ï¼Œå¦‚æžœå¤±è´¥åˆ™ç”¨å‰ç¼€åŒ¹é…
            role_config = role_manager.get_role_config(base_type, rid)
            
            if not role_config:
                # ä½¿ç”¨å‰ç¼€åŒ¹é…ï¼ˆå¦‚ "V5"ï¼‰æŸ¥æ‰¾é…ç½®
                v_prefix = role_id.split("_")[0]  # æå– "V5"
                logger.info(f"ðŸ” [DEBUG] Direct match failed, trying prefix: {v_prefix}")
                
                # éåŽ†æ‰€æœ‰è§’è‰²é…ç½®ï¼Œæ‰¾åˆ°åŒ¹é…å‰ç¼€çš„
                for config_key in role_manager.roles.keys():
                    if config_key.startswith(v_prefix):
                        role_config = role_manager.get_role_config(config_key, rid)
                        if role_config:
                            logger.info(f"âœ… Found config using prefix match: {config_key}")
                            break
            
            logger.info(f"ðŸ” [DEBUG] role_config found: {role_config is not None}")

            if not role_config:
                logger.error(f"âŒ Role config not found for {role_id}")
                return {"error": f"Role config not found for {role_id}"}

            # ðŸ†• P0-1ä¿®å¤ï¼šæ‰§è¡Œå‰æ€»æ˜¯æ³¨å…¥è´¨é‡çº¦æŸï¼ˆæ— è®ºæ˜¯å¦é‡è¯•ï¼‰
            if quality_checklist:
                risk_level = quality_checklist.get("risk_level", "low")
                # åªå¯¹mediumå’Œhighé£Žé™©ä»»åŠ¡æ³¨å…¥è´¨é‡çº¦æŸ
                if risk_level in ["medium", "high"]:
                    logger.info(f"ðŸ” æ³¨å…¥è´¨é‡çº¦æŸåˆ° {role_id} (é£Žé™©ç­‰çº§: {risk_level}, è½®æ¬¡: {review_round}, é‡è¯•: {retry_count})")
                    original_prompt = role_config.get("system_prompt", "")
                    enhanced_prompt = QualityMonitor.inject_quality_constraints(
                        original_prompt, quality_checklist
                    )
                    role_config["system_prompt"] = enhanced_prompt
                else:
                    logger.debug(f"â­ï¸ {role_id} é£Žé™©ç­‰çº§ä¸º {risk_level}ï¼Œè·³è¿‡è´¨é‡çº¦æŸæ³¨å…¥")

            # ðŸ†• åŠ¨æ€æœ¬ä½“è®ºæ³¨å…¥é€»è¾‘
            # 1. åˆ¤æ–­é¡¹ç›®ç±»åž‹ï¼ˆå¦‚ personal_residentialï¼‰
            project_type = state.get("project_type")
            if project_type:
                ontology_fragment = self.ontology_loader.get_ontology_by_type(project_type)
            else:
                ontology_fragment = self.ontology_loader.get_meta_framework()
            # 2. æ³¨å…¥åˆ° system_prompt å ä½ç¬¦
            if role_config and "system_prompt" in role_config:
                prompt = role_config["system_prompt"]
                if "{{DYNAMIC_ONTOLOGY_INJECTION}}" in prompt:
                    injected = prompt.replace("{{DYNAMIC_ONTOLOGY_INJECTION}}", yaml.dump(ontology_fragment, allow_unicode=True, default_flow_style=False))
                    role_config["system_prompt"] = injected
                    logger.info(f"âœ… å·²åŠ¨æ€æ³¨å…¥æœ¬ä½“è®ºç‰‡æ®µåˆ° {role_id} çš„ system_prompt")
                else:
                    logger.warning(f"âš ï¸ {role_id} çš„ system_prompt æœªåŒ…å«åŠ¨æ€æ³¨å…¥å ä½ç¬¦")

            # åˆ›å»ºä»»åŠ¡å¯¼å‘ä¸“å®¶å·¥åŽ‚å®žä¾‹
            expert_factory = TaskOrientedExpertFactory()

            # æž„å»ºä¸Šä¸‹æ–‡
            context = self._build_context_for_expert(state)
            
            # æž„å»ºè§’è‰²å¯¹è±¡ï¼ˆåŒ…å«TaskInstructionï¼‰
            # æ³¨æ„ï¼šProjectAnalysisStateæ˜¯TypedDictï¼Œä¸èƒ½ç›´æŽ¥å®žä¾‹åŒ–
            # æˆ‘ä»¬ç›´æŽ¥ä½¿ç”¨stateä½œä¸ºä¸Šä¸‹æ–‡
            
            # ðŸ”§ ä¿®å¤v4.0ï¼šä»Ž strategic_analysis.selected_roles ä¸­èŽ·å– role_object
            # æ”¯æŒä¸¤ç§åŒ¹é…æ–¹å¼ï¼šç²¾ç¡®åŒ¹é… å’Œ çŸ­æ ¼å¼åŒ¹é…
            strategic_analysis = state.get("strategic_analysis", {})
            selected_roles = strategic_analysis.get("selected_roles", [])
            role_object = None
            
            # æå–å½“å‰ role_id çš„çŸ­æ ¼å¼ (e.g., "V4_è®¾è®¡ç ”ç©¶å‘˜_4-1" -> "4-1")
            current_short_id = role_id.split('_')[-1] if '_' in role_id else role_id
            
            # ðŸ” è°ƒè¯•æ—¥å¿—
            logger.debug(f"ðŸ” [TaskInstructionæŸ¥æ‰¾] role_id={role_id}, current_short_id={current_short_id}")
            logger.debug(f"ðŸ” [TaskInstructionæŸ¥æ‰¾] selected_rolesæ•°é‡={len(selected_roles)}")
            if selected_roles:
                sample_ids = [r.get("role_id", "N/A") for r in selected_roles[:3]]
                logger.debug(f"ðŸ” [TaskInstructionæŸ¥æ‰¾] å‰3ä¸ªstored_role_id: {sample_ids}")
            
            for role in selected_roles:
                stored_role_id = role.get("role_id", "")
                # æå–å­˜å‚¨çš„ role_id çš„çŸ­æ ¼å¼
                stored_short_id = stored_role_id.split('_')[-1] if '_' in stored_role_id else stored_role_id
                
                # æ”¯æŒå¤šç§åŒ¹é…æ–¹å¼ï¼š
                # 1. ç²¾ç¡®åŒ¹é… (e.g., "V4_è®¾è®¡ç ”ç©¶å‘˜_4-1" == "V4_è®¾è®¡ç ”ç©¶å‘˜_4-1")
                # 2. çŸ­æ ¼å¼åŒ¹é… (e.g., "4-1" == "4-1")
                # 3. å­˜å‚¨çš„æ˜¯çŸ­æ ¼å¼ï¼ŒæŸ¥è¯¢çš„æ˜¯å®Œæ•´æ ¼å¼
                if (stored_role_id == role_id or 
                    stored_short_id == current_short_id):
                    role_object = role
                    logger.info(f"âœ… æ‰¾åˆ° {role_id} çš„TaskInstruction (stored_role_id={stored_role_id})")
                    break
            
            if not role_object:
                # æž„å»ºé»˜è®¤role_objectï¼ˆå‘åŽå…¼å®¹ï¼‰
                role_object = {
                    "role_id": role_id,
                    "role_name": role_config.get("name", role_id),
                    "dynamic_role_name": role_config.get("name", role_id),
                    "task_instruction": {
                        "objective": f"åŸºäºŽ{role_config.get('name', 'ä¸“ä¸šé¢†åŸŸ')}è¿›è¡Œæ·±åº¦åˆ†æž",
                        "deliverables": [
                            {
                                "name": "ä¸“ä¸šåˆ†æžæŠ¥å‘Š",
                                "description": "åŸºäºŽä¸“ä¸šé¢†åŸŸæä¾›æ·±åº¦åˆ†æžå’Œå»ºè®®",
                                "format": "analysis",
                                "priority": "high",
                                "success_criteria": ["åˆ†æžæ·±å…¥ä¸”ä¸“ä¸š", "å»ºè®®å…·æœ‰å¯æ“ä½œæ€§"]
                            }
                        ],
                        "success_criteria": ["è¾“å‡ºç¬¦åˆä¸“ä¸šæ ‡å‡†", "å»ºè®®å…·æœ‰å®žç”¨ä»·å€¼"],
                        "constraints": [],
                        "context_requirements": []
                    }
                }
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°{role_id}çš„TaskInstructionï¼Œä½¿ç”¨é»˜è®¤ç»“æž„")

            # æ‰§è¡Œä»»åŠ¡å¯¼å‘ä¸“å®¶åˆ†æž
            try:
                expert_result = await expert_factory.execute_expert(
                    role_object=role_object,
                    context=context,
                    state=state  # ç›´æŽ¥ä¼ é€’state
                )
                
                # æž„å»ºå…¼å®¹çš„ç»“æžœæ ¼å¼
                result_content = expert_result.get("analysis", "")
                structured_output = expert_result.get("structured_output")
                
                # è®°å½•æ‰§è¡Œä¿¡æ¯
                logger.info(f"âœ… ä»»åŠ¡å¯¼å‘ä¸“å®¶ {role_id} æ‰§è¡Œå®Œæˆ")
                if structured_output:
                    logger.info(f"ðŸ“Š ç»“æž„åŒ–è¾“å‡ºéªŒè¯æˆåŠŸ")
                else:
                    logger.warning(f"âš ï¸ ç»“æž„åŒ–è¾“å‡ºéªŒè¯å¤±è´¥ï¼Œä½¿ç”¨åŽŸå§‹è¾“å‡º")
                    
            except Exception as e:
                logger.error(f"âŒ æ‰§è¡Œä»»åŠ¡å¯¼å‘ä¸“å®¶å¤±è´¥: {str(e)}")
                result_content = f"æ‰§è¡Œå¤±è´¥: {str(e)}"
                expert_result = {
                    "expert_id": role_id,
                    "expert_name": role_config.get("name", role_id),
                    "analysis": result_content,
                    "structured_output": None,
                    "error": True
                }

            # ðŸ†• æ‰§è¡ŒåŽï¼šå¿«é€ŸéªŒè¯
            quality_checklist = state.get("quality_checklist", {})
            if quality_checklist and result_content:
                logger.info(f"ðŸ” å¿«é€ŸéªŒè¯ {role_id} çš„è¾“å‡ºè´¨é‡")
                validation_result = QualityMonitor.quick_validation(
                    result_content, quality_checklist, role_id
                )
                
                # åˆ¤æ–­æ˜¯å¦éœ€è¦é‡è¯•
                should_retry = QualityMonitor.should_retry(validation_result)
                
                if should_retry and retry_count == 0:
                    logger.warning(f"âš ï¸ {role_id} è´¨é‡ä¸è¾¾æ ‡ï¼Œè§¦å‘é‡è¯•")
                    
                    try:
                        # ä½¿ç”¨ç›¸åŒçš„role_objectè¿›è¡Œé‡è¯•
                        state[f"retry_count_{role_id}"] = 1
                        expert_result_retry = await expert_factory.execute_expert(
                            role_object=role_object,
                            context=context,
                            state=state  # ç›´æŽ¥ä½¿ç”¨state
                        )
                        
                        # æ›´æ–°ç»“æžœ
                        result_content = expert_result_retry.get("analysis", "")
                        expert_result = expert_result_retry
                        logger.info(f"âœ… {role_id} é‡è¯•å®Œæˆ")
                        
                    except Exception as retry_error:
                        logger.error(f"âŒ {role_id} é‡è¯•å¤±è´¥: {str(retry_error)}")
                
                # å°†éªŒè¯ç»“æžœé™„åŠ åˆ°è¾“å‡º
                if 'quality_validation' not in expert_result:
                    expert_result["quality_validation"] = validation_result

            # ðŸ”§ ä¿®å¤ï¼šå°†æˆåŠŸè¿”å›žé€»è¾‘ç§»å‡ºquality_checklistæ¡ä»¶å—
            logger.info(f"âœ… Dynamic agent {role_id} completed successfully")
            logger.debug(f"Result length: {len(result_content)} characters")

            # ðŸ”§ å­—æ®µåä¿®å¤ï¼šä½¿ç”¨ structured_outputï¼ˆè€Œéž parsed_resultï¼‰
            structured_output = expert_result.get("structured_output", {})
            if structured_output:
                structured_data = structured_output
                # ç¡®ä¿contentå­—æ®µå­˜åœ¨
                if "content" not in structured_data:
                    structured_data["content"] = result_content
                
                # ðŸ”¥ Debug: æ£€æŸ¥challenge_flagsæ˜¯å¦å­˜åœ¨
                if "challenge_flags" in structured_data:
                    challenge_count = len(structured_data["challenge_flags"])
                    logger.info(f"ðŸ”¥ [DEBUG] {role_id} çš„ structured_data åŒ…å« {challenge_count} ä¸ª challenge_flags")
                else:
                    logger.debug(f"â„¹ï¸ [DEBUG] {role_id} çš„ structured_data ä¸åŒ…å« challenge_flags")
            else:
                structured_data = {"content": result_content}
                logger.debug(f"â„¹ï¸ [DEBUG] {role_id} æ—  structured_outputï¼Œä½¿ç”¨é»˜è®¤ structured_data")

            # è¿”å›žç»“æžœ - ä½¿ç”¨ role_id ä½œä¸º key
            role_name = role_config.get("name", "æœªçŸ¥è§’è‰²")
            dynamic_role_name = role_object.get("dynamic_role_name", role_name)

            # ðŸ”§ ä¿®å¤ï¼ˆ2025-11-30ï¼‰ï¼šdetail æ˜¾ç¤ºå®Œæ•´çš„ role_idï¼Œä¾¿äºŽå‰ç«¯åŒæ­¥æ˜¾ç¤º
            # æ ¼å¼ï¼šä¸“å®¶ã€è§’è‰²IDã€‘æ‰§è¡Œä¸­
            detail_message = f"ä¸“å®¶ã€{role_id}ã€‘å®Œæˆåˆ†æž"

            # åªæœ‰åœ¨result_contentçœŸæ­£ä¸ºç©ºæ—¶æ‰è¿”å›žé”™è¯¯
            if not result_content:
                logger.warning(f"âš ï¸ Dynamic agent {role_id} returned no results")
                return {"error": f"No results from {role_id}"}

            # ðŸš€ P4ä¼˜åŒ–ï¼šå•ä¸ªä¸“å®¶å®Œæˆå³æŽ¨é€ç»“æžœï¼ˆæ¸è¿›å¼å±•ç¤ºï¼‰
            # èŽ·å–session_idç”¨äºŽWebSocketæŽ¨é€
            session_id = state.get("session_id")
            if session_id:
                try:
                    # å¯¼å…¥broadcastå‡½æ•°
                    from intelligent_project_analyzer.api.server import broadcast_to_websockets

                    # æŽ¨é€ä¸“å®¶ç»“æžœ
                    import asyncio
                    asyncio.create_task(broadcast_to_websockets(session_id, {
                        "type": "agent_result",
                        "role_id": role_id,
                        "role_name": role_name,
                        "dynamic_role_name": dynamic_role_name,
                        "analysis": result_content,
                        "structured_data": structured_data,
                        "timestamp": datetime.now().isoformat()
                    }))
                    logger.info(f"ðŸ“¤ [Progressive] å·²æŽ¨é€ä¸“å®¶ç»“æžœ: {role_id} ({dynamic_role_name})")
                except Exception as broadcast_error:
                    logger.warning(f"âš ï¸ WebSocketæŽ¨é€å¤±è´¥: {broadcast_error}")

            # ðŸ†• v7.108: ä¸ºè¯¥ä¸“å®¶çš„äº¤ä»˜ç‰©ç”Ÿæˆæ¦‚å¿µå›¾
            concept_images = []
            try:
                deliverable_owner_map = state.get("deliverable_owner_map", {})
                deliverable_metadata = state.get("deliverable_metadata", {})
                deliverable_ids = deliverable_owner_map.get(role_id, [])

                if deliverable_ids and deliverable_metadata:
                    logger.info(f"ðŸŽ¨ [v7.108] ä¸ºè§’è‰² {role_id} çš„ {len(deliverable_ids)} ä¸ªäº¤ä»˜ç‰©ç”Ÿæˆæ¦‚å¿µå›¾...")

                    # å¯¼å…¥å›¾ç‰‡ç”ŸæˆæœåŠ¡
                    from intelligent_project_analyzer.services.image_generator import ImageGeneratorService

                    # åˆå§‹åŒ–å›¾ç‰‡ç”Ÿæˆå™¨
                    image_generator = ImageGeneratorService()

                    # èŽ·å–ä¸“å®¶åˆ†æžæ‘˜è¦ï¼ˆç”¨äºŽå›¾ç‰‡ç”Ÿæˆï¼‰
                    expert_summary = result_content[:500]  # å–å‰500å­—ç¬¦
                    session_id_for_image = state.get("session_id", "unknown")
                    project_type = state.get("project_type", "interior")

                    # ä¸ºæ¯ä¸ªäº¤ä»˜ç‰©ç”Ÿæˆæ¦‚å¿µå›¾
                    for deliverable_id in deliverable_ids:
                        metadata = deliverable_metadata.get(deliverable_id)
                        if not metadata:
                            logger.warning(f"  âš ï¸ äº¤ä»˜ç‰© {deliverable_id} å…ƒæ•°æ®ç¼ºå¤±ï¼Œè·³è¿‡å›¾ç‰‡ç”Ÿæˆ")
                            continue

                        try:
                            image_metadata = await image_generator.generate_deliverable_image(
                                deliverable_metadata=metadata,
                                expert_analysis=expert_summary,
                                session_id=session_id_for_image,
                                project_type=project_type,
                                aspect_ratio="16:9"
                            )

                            # è½¬æ¢ä¸ºå­—å…¸å­˜å‚¨
                            concept_images.append(image_metadata.model_dump())
                            logger.info(f"  âœ… ç”Ÿæˆæ¦‚å¿µå›¾: {image_metadata.filename}")

                        except Exception as img_error:
                            logger.error(f"  âŒ ç”Ÿæˆæ¦‚å¿µå›¾å¤±è´¥ (äº¤ä»˜ç‰© {deliverable_id}): {img_error}")
                            # ä¸é˜»å¡žworkflowï¼Œç»§ç»­æ‰§è¡Œ

                    if concept_images:
                        logger.info(f"âœ… [v7.108] æˆåŠŸä¸ºè§’è‰² {role_id} ç”Ÿæˆ {len(concept_images)} å¼ æ¦‚å¿µå›¾")
                    else:
                        logger.warning(f"âš ï¸ [v7.108] è§’è‰² {role_id} æœªç”Ÿæˆä»»ä½•æ¦‚å¿µå›¾")
                else:
                    logger.debug(f"[v7.108] è§’è‰² {role_id} æ— äº¤ä»˜ç‰©ï¼Œè·³è¿‡å›¾ç‰‡ç”Ÿæˆ")

            except Exception as e:
                logger.error(f"âŒ [v7.108] æ¦‚å¿µå›¾ç”Ÿæˆæµç¨‹å¤±è´¥: {e}")
                logger.exception(e)
                # ä¸é˜»å¡žworkflowï¼Œä¸“å®¶åˆ†æžä»ç„¶æœ‰æ•ˆ

            return {
                "agent_results": {
                    role_id: {
                        "role_id": role_id,
                        "role_name": role_name,
                        "analysis": result_content,
                        "confidence": 0.8,  # é»˜è®¤ç½®ä¿¡åº¦
                        "structured_data": structured_data,  # ðŸ†• ä½¿ç”¨å®Œæ•´çš„parsed_result
                        "concept_images": concept_images  # ðŸ†• v7.108: å…³è”çš„æ¦‚å¿µå›¾
                    }
                },
                "detail": detail_message
            }

        except Exception as e:
            logger.error(f"Agent execution failed: {e}")
            import traceback
            traceback.print_exc()
            return {"error": str(e)}

    def _batch_executor_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        æ‰¹æ¬¡æ‰§è¡Œå™¨èŠ‚ç‚¹ - éªŒè¯å¹¶å‡†å¤‡æ‰§è¡Œå½“å‰æ‰¹æ¬¡

        âœ… ä¿®å¤ï¼šèŠ‚ç‚¹å‡½æ•°åªè¿”å›žçŠ¶æ€æ›´æ–°ï¼ˆDictï¼‰ï¼ŒSend å¯¹è±¡çš„åˆ›å»ºäº¤ç»™æ¡ä»¶è¾¹å‡½æ•°
        âœ… æ–°å¢žï¼šæ”¯æŒ specific_agents_to_runï¼ˆå®¡æ ¸è§¦å‘çš„é‡æ–°æ‰§è¡Œï¼‰

        è¿™æ˜¯åŠ¨æ€æ‰¹æ¬¡æ‰§è¡Œçš„æ ¸å¿ƒèŠ‚ç‚¹ï¼Œæ›¿ä»£äº†ç¡¬ç¼–ç çš„ first_batch_agent å’Œ second_batch_agentã€‚

        å·¥ä½œåŽŸç†:
        1. æ£€æŸ¥æ˜¯å¦æœ‰ specific_agents_to_runï¼ˆå®¡æ ¸è§¦å‘çš„é‡æ–°æ‰§è¡Œï¼‰
        2. å¦‚æžœæœ‰ï¼Œåˆ›å»ºä¸€ä¸ªä¸´æ—¶æ‰¹æ¬¡åªåŒ…å«æŒ‡å®šçš„ä¸“å®¶
        3. å¦åˆ™ï¼Œä»Ž state["execution_batches"] è¯»å–æ­£å¸¸æ‰¹æ¬¡
        4. è¿”å›žç©ºå­—å…¸ï¼ˆçŠ¶æ€æ— éœ€æ›´æ–°ï¼‰
        5. åŽç»­ç”±æ¡ä»¶è¾¹å‡½æ•° _create_batch_sends åˆ›å»º Send ä»»åŠ¡

        çŠ¶æ€è¾“å…¥:
        - specific_agents_to_run: List[str] - å®¡æ ¸æŒ‡å®šéœ€è¦é‡æ–°æ‰§è¡Œçš„ä¸“å®¶ï¼ˆä¼˜å…ˆï¼‰
        - execution_batches: List[List[str]] - æ‰€æœ‰æ‰¹æ¬¡çš„è§’è‰²åˆ—è¡¨
        - current_batch: int - å½“å‰æ‰¹æ¬¡ç¼–å·ï¼ˆ1-basedï¼‰
        - agent_results: Dict - å·²å®Œæˆä¸“å®¶çš„ç»“æžœ

        è¿”å›ž:
        - Dict[str, Any] - çŠ¶æ€æ›´æ–°ï¼ˆé€šå¸¸ä¸ºç©ºå­—å…¸ï¼‰
        """
        try:
            # âœ… ä¼˜å…ˆæ£€æŸ¥æ˜¯å¦æœ‰å®¡æ ¸è§¦å‘çš„é‡æ–°æ‰§è¡Œ
            specific_agents = state.get("specific_agents_to_run", [])
            
            if specific_agents:
                logger.info(f"ðŸ”„ å®¡æ ¸è§¦å‘çš„é‡æ–°æ‰§è¡Œ: {len(specific_agents)} ä¸ªä¸“å®¶")
                logger.info(f"   ä¸“å®¶åˆ—è¡¨: {specific_agents}")
                
                # åˆ›å»ºä¸´æ—¶æ‰¹æ¬¡
                return {
                    "execution_batches": [specific_agents],
                    "current_batch": 1,
                    "total_batches": 1,
                    "is_rerun": True,
                    "skip_role_review": True,
                    "skip_task_review": True
                }
            
            # æ­£å¸¸æ‰¹æ¬¡æ‰§è¡Œ
            batches = state.get("execution_batches", [])
            current_batch = state.get("current_batch", 1)
            total_batches = len(batches)

            logger.info(f"ðŸŽ¯ Batch executor: preparing batch {current_batch}/{total_batches}")

            # éªŒè¯æ‰¹æ¬¡ç¼–å·
            if current_batch < 1 or current_batch > total_batches:
                logger.error(f"âŒ Invalid batch number: {current_batch} (total: {total_batches})")
                return {"errors": [f"Invalid batch number: {current_batch}"]}

            if not batches:
                logger.error("âŒ No execution_batches found in state")
                return {"errors": ["No execution batches defined"]}

            # èŽ·å–å½“å‰æ‰¹æ¬¡çš„è§’è‰²åˆ—è¡¨ï¼ˆ0-indexedï¼‰
            batch_roles = batches[current_batch - 1]
            display_roles = [format_role_display_name(r) for r in batch_roles]
            logger.info(f"ðŸ“‹ Batch {current_batch} contains {len(batch_roles)} agents: {display_roles}")

            # âœ… åªè¿”å›žçŠ¶æ€æ›´æ–°ï¼Œä¸åˆ›å»º Send å¯¹è±¡
            return {"detail": f"æ‰¹æ¬¡ {current_batch}/{total_batches}: {len(batch_roles)} ä½ä¸“å®¶"}

        except Exception as e:
            logger.error(f"âŒ Batch executor failed: {e}")
            import traceback
            traceback.print_exc()
            return {"errors": [str(e)]}

    def _create_batch_sends(self, state: ProjectAnalysisState) -> List[Send]:
        """
        æ‰¹æ¬¡ Send åˆ›å»ºå™¨ - æ¡ä»¶è¾¹å‡½æ•°ï¼Œè¿”å›žå¹¶è¡Œä»»åŠ¡åˆ—è¡¨

        âœ… æ–°å¢žï¼šä¸“é—¨è´Ÿè´£åˆ›å»º Send å¯¹è±¡çš„æ¡ä»¶è¾¹å‡½æ•°ï¼Œé…åˆ _batch_executor_node ä½¿ç”¨
        âœ… ä¿®å¤ï¼šæ”¯æŒå®¡æ ¸è§¦å‘çš„é‡æ–°æ‰§è¡Œï¼ˆspecific_agents_to_runï¼‰

        è¿™ä¸ªå‡½æ•°ä½œä¸ºæ¡ä»¶è¾¹å‡½æ•°ï¼Œæ ¹æ®å½“å‰æ‰¹æ¬¡åˆ›å»ºå¯¹åº”çš„ Send ä»»åŠ¡åˆ—è¡¨ã€‚
        LangGraph ä¼šè‡ªåŠ¨å¹¶è¡Œæ‰§è¡Œè¿™äº›ä»»åŠ¡ã€‚

        å·¥ä½œåŽŸç†:
        1. ä»Ž state["execution_batches"] è¯»å–æ‰€æœ‰æ‰¹æ¬¡
        2. æ ¹æ® state["current_batch"] ç¡®å®šå½“å‰è¦æ‰§è¡Œçš„æ‰¹æ¬¡
        3. ä¸ºæ‰¹æ¬¡ä¸­çš„æ¯ä¸ªè§’è‰²åˆ›å»º Send å¯¹è±¡
        4. è¿”å›ž Send åˆ—è¡¨ï¼ŒLangGraph è‡ªåŠ¨å¹¶è¡Œè°ƒåº¦

        çŠ¶æ€è¾“å…¥:
        - execution_batches: List[List[str]] - æ‰€æœ‰æ‰¹æ¬¡çš„è§’è‰²åˆ—è¡¨
        - current_batch: int - å½“å‰æ‰¹æ¬¡ç¼–å·ï¼ˆ1-basedï¼‰
        - is_rerun: bool - æ˜¯å¦ä¸ºå®¡æ ¸è§¦å‘çš„é‡æ–°æ‰§è¡Œ
        - review_feedback: Dict - å®¡æ ¸åé¦ˆï¼ˆå¦‚æžœæ˜¯é‡æ–°æ‰§è¡Œï¼‰

        è¿”å›ž:
        - List[Send] - å¹¶è¡Œä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ª Send æ‰§è¡Œä¸€ä¸ªæ™ºèƒ½ä½“

        æ³¨æ„:
        - è¿™ä¸ªå‡½æ•°å¿…é¡»æ˜¯æ¡ä»¶è¾¹å‡½æ•°ï¼Œä¸èƒ½æ˜¯èŠ‚ç‚¹å‡½æ•°
        - é€šè¿‡ add_conditional_edges é…ç½®åˆ°å›¾ä¸­
        """
        batches = state.get("execution_batches") or []
        current_batch = state.get("current_batch", 1)
        is_rerun = state.get("is_rerun", False)
        review_feedback = state.get("review_feedback", {})

        # è¾¹ç•Œæ£€æŸ¥
        if not batches:
            logger.warning(f"âš ï¸ No batches found, batches is None or empty")
            return []
        
        if current_batch < 1 or current_batch > len(batches):
            logger.warning(
                f"âš ï¸ No valid batch to execute: current={current_batch}, total={len(batches)}"
            )
            return []

        # èŽ·å–å½“å‰æ‰¹æ¬¡çš„è§’è‰²åˆ—è¡¨ï¼ˆ0-indexedï¼‰
        batch_roles = batches[current_batch - 1]
        
        if is_rerun:
            display_roles = [format_role_display_name(r) for r in batch_roles]
            logger.info(f"ðŸ”„ åˆ›å»ºé‡æ–°æ‰§è¡Œä»»åŠ¡: {len(batch_roles)} ä¸ªä¸“å®¶ {display_roles}")
        else:
            display_roles = [format_role_display_name(r) for r in batch_roles]
            logger.info(f"ðŸ“‹ åˆ›å»ºæ‰¹æ¬¡ {current_batch} ä»»åŠ¡: {len(batch_roles)} ä¸ªä¸“å®¶ {display_roles}")

        # åˆ›å»º Send å¯¹è±¡åˆ—è¡¨
        send_list = []
        for role_id in batch_roles:
            # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“åˆ›å»ºç‹¬ç«‹çš„çŠ¶æ€å‰¯æœ¬
            agent_state = dict(state)
            agent_state["role_id"] = role_id
            agent_state["execution_batch"] = f"batch_{current_batch}"
            agent_state["current_stage"] = AnalysisStage.PARALLEL_ANALYSIS.value
            agent_state["is_rerun"] = is_rerun
            
            # âœ… å¦‚æžœæ˜¯é‡æ–°æ‰§è¡Œï¼Œæ·»åŠ é’ˆå¯¹è¯¥ä¸“å®¶çš„å®¡æ ¸åé¦ˆ
            if is_rerun and review_feedback:
                feedback_by_agent = review_feedback.get("feedback_by_agent", {})
                agent_feedback = feedback_by_agent.get(role_id, {})
                if agent_feedback:
                    agent_state["review_feedback_for_agent"] = agent_feedback
                    logger.info(f"   {role_id}: é™„åŠ å®¡æ ¸åé¦ˆ ({len(agent_feedback.get('issues', []))} ä¸ªé—®é¢˜)")

            send_list.append(Send("agent_executor", agent_state))
            logger.debug(f"  âž¤ Created Send for: {role_id}")

        logger.info(f"âœ… Created {len(send_list)} parallel tasks for batch {current_batch}")
        return send_list

    def _intermediate_aggregator_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        æ‰¹æ¬¡èšåˆèŠ‚ç‚¹ - éªŒè¯å½“å‰æ‰¹æ¬¡çš„æ‰§è¡Œç»“æžœï¼ˆé€šç”¨ç‰ˆï¼Œ2025-11-18é‡æž„ï¼‰

        åŠŸèƒ½:
        1. éªŒè¯å½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰ä¸“å®¶æ˜¯å¦æˆåŠŸå®Œæˆ
        2. è®°å½•è¯¦ç»†æ—¥å¿—
        3. æ ‡è®°å½“å‰æ‰¹æ¬¡ä¸ºå·²å®Œæˆ
        4. ä¸ºä¸‹ä¸€æ‰¹æ¬¡å‡†å¤‡ä¾èµ–æ•°æ®
        5. è¿”å›žçŠ¶æ€æ›´æ–°ï¼Œè§¦å‘ä¸‹ä¸€æ‰¹æ¬¡æˆ–è¿›å…¥å®¡æ ¸

        é‡æž„è¯´æ˜Žï¼š
        - ä»Žå›ºå®šçš„V3/V4/V5æ£€æŸ¥æ”¹ä¸ºåŸºäºŽexecution_batchesçš„åŠ¨æ€æ£€æŸ¥
        - æ”¯æŒä»»æ„æ•°é‡çš„æ‰¹æ¬¡ï¼ˆ1-Næ‰¹ï¼‰
        - é€šè¿‡current_batchè·Ÿè¸ªå½“å‰æ‰¹æ¬¡
        
        ðŸ”§ P1ä¿®å¤ï¼ˆ2025-11-25ï¼‰ï¼š
        - ç­‰å¾…å½“å‰æ‰¹æ¬¡æ‰€æœ‰agentå®ŒæˆåŽå†æ‰§è¡Œèšåˆ
        - é€šè¿‡æ£€æŸ¥agent_resultsç¡®ä¿ä¸ä¼šè¿‡æ—©è§¦å‘
        
        ðŸ”§ N2ä¼˜åŒ–ï¼ˆ2025-11-25ï¼‰ï¼š
        - LangGraph Send APIä¼šåœ¨æ¯ä¸ªå¹¶è¡Œä»»åŠ¡å®Œæˆæ—¶è§¦å‘æ­¤èŠ‚ç‚¹ï¼ˆé¢„æœŸè¡Œä¸ºï¼‰
        - æœ¬èŠ‚ç‚¹æ£€æŸ¥æ‰€æœ‰ä»»åŠ¡æ˜¯å¦å®Œæˆï¼Œæœªå®Œæˆæ—¶è¿”å›žç©ºå­—å…¸ç­‰å¾…
        - è¿™æ˜¯LangGraphå¹¶è¡Œæ‰§è¡Œçš„æ ‡å‡†"è½®è¯¢æ¨¡å¼"
        """
        try:
            agent_results = state.get("agent_results", {})
            batches = state.get("execution_batches", [])
            current_batch = state.get("current_batch", 1)
            total_batches = state.get("total_batches", len(batches))

            logger.info(f"ðŸ“Š å½“å‰æ‰¹æ¬¡: {current_batch}/{total_batches}")

            if not batches:
                logger.warning("æœªæ‰¾åˆ° execution_batchesï¼Œæ— æ³•éªŒè¯æ‰¹æ¬¡å®Œæˆæƒ…å†µ")
                # é™çº§åˆ°æ—§é€»è¾‘ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰
                return self._intermediate_aggregator_node_legacy(state)

            # èŽ·å–å½“å‰æ‰¹æ¬¡çš„è§’è‰²åˆ—è¡¨
            if current_batch < 1 or current_batch > len(batches):
                logger.error(f"å½“å‰æ‰¹æ¬¡ç¼–å· {current_batch} è¶…å‡ºèŒƒå›´")
                return {"error": f"Invalid batch number: {current_batch}"}

            current_batch_roles = batches[current_batch - 1]

            # ðŸ”§ N2ä¼˜åŒ–ï¼šå¿«é€Ÿæ£€æŸ¥æ˜¯å¦æ‰€æœ‰agentå·²å®Œæˆï¼ˆå‡å°‘æ— æ•ˆæ—¥å¿—ï¼‰
            pending_agents = [role_id for role_id in current_batch_roles if role_id not in agent_results]
            if pending_agents:
                # âš¡ LangGraphå¹¶è¡Œæ¨¡å¼ï¼šéƒ¨åˆ†ä»»åŠ¡å®Œæˆæ—¶ä¼šè§¦å‘æ­¤èŠ‚ç‚¹ï¼Œè¿™æ˜¯é¢„æœŸè¡Œä¸º
                # è¿”å›žç©ºå­—å…¸ï¼Œç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®ŒæˆåŽå†æ¬¡è°ƒç”¨

                # ðŸš€ P2ä¼˜åŒ–ï¼šæ·»åŠ è½®è¯¢è®¡æ•°å™¨ï¼Œä¼˜åŒ–æ—¥å¿—çº§åˆ«
                poll_count_key = f"_aggregator_poll_count_batch_{current_batch}"
                poll_count = state.get(poll_count_key, 0) + 1

                # åªåœ¨ç¬¬ä¸€æ¬¡è½®è¯¢æ—¶è®°å½• info çº§åˆ«æ—¥å¿—ï¼ŒåŽç»­ä½¿ç”¨ debug çº§åˆ«
                if poll_count == 1:
                    logger.info(f"â³ [Polling] æ‰¹æ¬¡ {current_batch} å¼€å§‹ç­‰å¾…: {len(pending_agents)}/{len(current_batch_roles)} æœªå®Œæˆ")
                else:
                    logger.debug(f"â³ [Polling #{poll_count}] æ‰¹æ¬¡ {current_batch} ç­‰å¾…ä¸­: {len(pending_agents)}/{len(current_batch_roles)} æœªå®Œæˆ")

                # è¿”å›žæ›´æ–°çš„è½®è¯¢è®¡æ•°
                return {poll_count_key: poll_count}
            
            # âœ… æ‰€æœ‰agentå·²å®Œæˆï¼Œå¼€å§‹è¯¦ç»†èšåˆ
            # ðŸš€ P2ä¼˜åŒ–ï¼šè®°å½•æ€»è½®è¯¢æ¬¡æ•°
            poll_count_key = f"_aggregator_poll_count_batch_{current_batch}"
            total_polls = state.get(poll_count_key, 0)
            if total_polls > 0:
                logger.info(f"âœ… [Aggregator] æ‰¹æ¬¡ {current_batch}/{total_batches} æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ˆç»è¿‡ {total_polls} æ¬¡è½®è¯¢ï¼‰ï¼Œå¼€å§‹èšåˆ")
            else:
                logger.info(f"âœ… [Aggregator] æ‰¹æ¬¡ {current_batch}/{total_batches} æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œå¼€å§‹èšåˆ")

            # æ£€æŸ¥å®Œæˆæƒ…å†µ
            completed_agents = []
            failed_agents = []

            for role_id in current_batch_roles:
                if role_id in agent_results:
                    result = agent_results[role_id]
                    confidence = result.get("confidence", 0)
                    has_error = "error" in result

                    if has_error:
                        failed_agents.append(role_id)
                        logger.warning(f"{role_id} completed with error: {result.get('error')}")
                    else:
                        completed_agents.append(role_id)
                        logger.info(f"âœ… {role_id} completed successfully (confidence: {confidence:.2%})")
                else:
                    failed_agents.append(role_id)
                    logger.warning(f"âš ï¸ {role_id} not found in results")

            # è®°å½•æ±‡æ€»ä¿¡æ¯
            logger.info(f"æ‰¹æ¬¡ {current_batch} æ±‡æ€»: {len(completed_agents)}/{len(current_batch_roles)} æˆåŠŸå®Œæˆ")
            if completed_agents:
                logger.info(f"âœ… å·²å®Œæˆ: {', '.join(completed_agents)}")
            if failed_agents:
                logger.warning(f"âŒ å¤±è´¥/ç¼ºå¤±: {', '.join(failed_agents)}")

            # å‡†å¤‡ä¾èµ–æ•°æ®æ‘˜è¦
            dependency_summary = {
                "batch_number": current_batch,
                "batch_completed": len(completed_agents) == len(current_batch_roles),
                "completed_count": len(completed_agents),
                "total_count": len(current_batch_roles),
                "completed_agents": completed_agents,
                "failed_agents": failed_agents,
                "timestamp": datetime.now().isoformat()
            }

            # æ›´æ–°å·²å®Œæˆæ‰¹æ¬¡åˆ—è¡¨
            completed_batches = state.get("completed_batches", [])
            if current_batch not in completed_batches:
                completed_batches_updated = completed_batches + [current_batch]
            else:
                completed_batches_updated = completed_batches

            logger.info(f"âœ… æ‰¹æ¬¡ {current_batch} èšåˆå®Œæˆ")
            logger.info(f"ðŸ”„ å·²å®Œæˆæ‰¹æ¬¡: {completed_batches_updated}")

            # è¿”å›žçŠ¶æ€æ›´æ–°ï¼ˆä¸‹ä¸€æ­¥ç”±second_batch_strategy_reviewå†³å®šè·¯ç”±ï¼‰

            # ðŸ”§ ä¿®å¤ï¼ˆ2025-11-30ï¼‰ï¼šæž„é€ æ›´å…·ä½“çš„detailæ¶ˆæ¯ï¼Œæ˜¾ç¤ºåˆšå®Œæˆçš„ä¸“å®¶
            if completed_agents:
                # å¦‚æžœåªæœ‰1ä¸ªä¸“å®¶ï¼Œæ˜¾ç¤ºå…·ä½“åç§°ï¼›å¦‚æžœå¤šä¸ªï¼Œæ˜¾ç¤ºæ•°é‡å’Œåˆ—è¡¨
                if len(completed_agents) == 1:
                    detail_message = f"ä¸“å®¶ã€{completed_agents[0]}ã€‘åˆ†æžå®Œæˆ"
                else:
                    agent_list = "ã€".join([agent.split("_")[-1] if "_" in agent else agent for agent in completed_agents])
                    detail_message = f"æ‰¹æ¬¡ {current_batch} å®Œæˆï¼š{agent_list} ç­‰ {len(completed_agents)} ä½ä¸“å®¶"
            else:
                detail_message = f"æ‰¹æ¬¡ {current_batch} æ‰§è¡Œä¸­..."

            return {
                "dependency_summary": dependency_summary,
                "completed_batches": completed_batches_updated,
                "updated_at": datetime.now().isoformat(),
                "detail": detail_message
            }

        except Exception as e:
            logger.error(f"Intermediate aggregator node failed: {e}")
            import traceback
            traceback.print_exc()
            return {
                "error": str(e),
                "updated_at": datetime.now().isoformat()
            }

    def _intermediate_aggregator_node_legacy(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        æ—§ç‰ˆæ‰¹æ¬¡èšåˆé€»è¾‘ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰

        å½“stateä¸­æ²¡æœ‰execution_batchesæ—¶ä½¿ç”¨æ­¤æ–¹æ³•
        """
        logger.warning("ä½¿ç”¨æ—§ç‰ˆintermediate_aggregatoré€»è¾‘")

        agent_results = state.get("agent_results", {})
        active_agents = state.get("active_agents", [])

        # ç¡¬ç¼–ç æ£€æŸ¥V3/V4/V5
        first_batch_roles = [
            role_id for role_id in active_agents
            if role_id.startswith("V3_") or role_id.startswith("V4_") or role_id.startswith("V5_")
        ]

        completed_agents = []
        failed_agents = []

        for role_id in first_batch_roles:
            if role_id in agent_results:
                result = agent_results[role_id]
                has_error = "error" in result
                if has_error:
                    failed_agents.append(role_id)
                else:
                    completed_agents.append(role_id)
            else:
                failed_agents.append(role_id)

        dependency_summary = {
            "first_batch_completed": len(completed_agents) == len(first_batch_roles),
            "completed_count": len(completed_agents),
            "total_count": len(first_batch_roles),
            "completed_agents": completed_agents,
            "failed_agents": failed_agents,
            "timestamp": datetime.now().isoformat()
        }

        return {
            "dependency_summary": dependency_summary,
            "updated_at": datetime.now().isoformat()
        }

    # ============================================================================
    # ðŸ†• v3.5 Expert Collaboration: Challenge Detection Node
    # ============================================================================
    
    def _detect_challenges_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        ðŸ†• v3.5 æŒ‘æˆ˜æ£€æµ‹èŠ‚ç‚¹ - æ£€æµ‹ä¸“å®¶è¾“å‡ºä¸­çš„challenge_flagså¹¶å¤„ç†
        
        ðŸ”¥ v7.16: æ”¯æŒæ–°ç‰ˆ LangGraph ChallengeDetectionAgent
        
        å·¥ä½œæµç¨‹:
        1. ä»Žstateä¸­æå–æ‰€æœ‰ä¸“å®¶çš„è¾“å‡º
        2. è°ƒç”¨detect_and_handle_challenges_node()æ£€æµ‹æŒ‘æˆ˜
        3. æ›´æ–°stateåŒ…å«æŒ‘æˆ˜æ£€æµ‹å’Œå¤„ç†ç»“æžœ
        4. è®¾ç½®requires_feedback_loopæ ‡å¿—
        
        çŠ¶æ€è¾“å…¥:
        - batch_results: Dict - å„æ‰¹æ¬¡ä¸“å®¶çš„è¾“å‡º
        - (å¯é€‰) v2_output, v3_outputç­‰ç›´æŽ¥å­—æ®µ
        
        çŠ¶æ€è¾“å‡º:
        - challenge_detection: Dict - æŒ‘æˆ˜æ£€æµ‹ç»“æžœ
        - challenge_handling: Dict - æŒ‘æˆ˜å¤„ç†ç»“æžœ
        - has_active_challenges: bool - æ˜¯å¦æœ‰æ´»è·ƒæŒ‘æˆ˜
        - requires_feedback_loop: bool - æ˜¯å¦éœ€è¦åé¦ˆå¾ªçŽ¯
        - feedback_loop_reason: str - åé¦ˆå¾ªçŽ¯åŽŸå› ï¼ˆå¦‚æœ‰ï¼‰
        
        Returns:
            æ›´æ–°çš„çŠ¶æ€å­—å…¸ï¼ˆåªåŒ…å«æ–°å¢ž/ä¿®æ”¹çš„å­—æ®µï¼‰
        """
        logger.info("ðŸ” [v3.5] å¼€å§‹æ£€æµ‹ä¸“å®¶æŒ‘æˆ˜...")
        
        try:
            # ðŸ†• v7.16: ä½¿ç”¨æ–°ç‰ˆ LangGraph Agentï¼ˆå¦‚æžœå¯ç”¨ï¼‰
            if USE_V716_AGENTS:
                logger.info("ðŸš€ [v7.16] ä½¿ç”¨ ChallengeDetectionAgent")
                updated_state = detect_and_handle_challenges_v2(state)
            else:
                # è°ƒç”¨æ ¸å¿ƒæŒ‘æˆ˜æ£€æµ‹å‡½æ•°ï¼ˆçŽ°åœ¨åªè¿”å›žæ–°å¢žå­—æ®µï¼‰
                updated_state = detect_and_handle_challenges_node(state)
            
            # è®°å½•æ£€æµ‹ç»“æžœ
            if updated_state.get("has_active_challenges"):
                challenge_count = len(updated_state.get("challenge_detection", {}).get("challenges", []))
                logger.info(f"ðŸ”¥ [v3.5] æ£€æµ‹åˆ° {challenge_count} ä¸ªä¸“å®¶æŒ‘æˆ˜")
                
                if updated_state.get("requires_feedback_loop"):
                    logger.warning("âš ï¸ [v3.5] éœ€è¦å¯åŠ¨åé¦ˆå¾ªçŽ¯å›žè®¿éœ€æ±‚åˆ†æžå¸ˆ")
            else:
                logger.info("âœ… [v3.5] æœªæ£€æµ‹åˆ°æŒ‘æˆ˜ï¼Œä¸“å®¶æŽ¥å—éœ€æ±‚åˆ†æžå¸ˆçš„æ´žå¯Ÿ")
            
            return updated_state
            
        except Exception as e:
            logger.error(f"âŒ [v3.5] æŒ‘æˆ˜æ£€æµ‹å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            # è¿”å›žå®‰å…¨çš„é»˜è®¤å€¼
            return {
                "has_active_challenges": False,
                "requires_feedback_loop": False,
                "challenge_detection": {"has_challenges": False, "challenges": []},
                "challenge_handling": {"requires_revisit": False}
            }
            # å¤±è´¥æ—¶è¿”å›žåŽŸçŠ¶æ€ï¼Œä¸å½±å“å·¥ä½œæµç»§ç»­
            return {
                **state,
                "has_active_challenges": False,
                "requires_feedback_loop": False,
                "challenge_detection_error": str(e)
            }
    
    def _route_after_challenge_detection(self, state: ProjectAnalysisState) -> str:
        """
        ðŸ†• v3.5 æŒ‘æˆ˜æ£€æµ‹åŽçš„è·¯ç”±å†³ç­–
        
        æ ¹æ®æŒ‘æˆ˜æ£€æµ‹ç»“æžœå†³å®šä¸‹ä¸€æ­¥:
        - å¦‚æžœrequires_manual_review=True â†’ "manual_review" (>3ä¸ªmust_fixé—®é¢˜)
        - å¦‚æžœrequires_client_review=True â†’ "analysis_review" (äº¤ç”²æ–¹è£å†³)
        - å¦‚æžœrequires_feedback_loop=True â†’ "revisit_requirements" (å›žè®¿éœ€æ±‚åˆ†æžå¸ˆ)
        - å¦åˆ™ â†’ "continue_workflow" (ç»§ç»­æ­£å¸¸æµç¨‹)
        
        ä¼˜å…ˆçº§: manual_review > escalate > revisit_ra > continue
        
        Args:
            state: å½“å‰å·¥ä½œæµçŠ¶æ€
            
        Returns:
            è·¯ç”±ç›®æ ‡: "manual_review" | "analysis_review" | "revisit_requirements" | "continue_workflow"
        """
        # ðŸ†• æœ€é«˜ä¼˜å…ˆçº§ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦äººå·¥å®¡æ ¸ï¼ˆ>3ä¸ªmust_fixï¼‰
        requires_manual_review = state.get("requires_manual_review", False)
        if requires_manual_review:
            issues_count = state.get("critical_issues_count", 0)
            logger.error(f"ðŸš¨ [Manual Review] å‘çŽ°{issues_count}ä¸ªä¸¥é‡è´¨é‡é—®é¢˜ï¼ˆè¶…è¿‡é˜ˆå€¼ï¼‰ï¼Œè§¦å‘äººå·¥å®¡æ ¸")
            return "manual_review"
        
        # ðŸ†• ä¼˜å…ˆæ£€æŸ¥æ˜¯å¦éœ€è¦ç”²æ–¹è£å†³ï¼ˆescalateé—­çŽ¯ï¼‰
        requires_client_review = state.get("requires_client_review", False)
        if requires_client_review:
            escalated_count = len(state.get("escalated_challenges", []))
            logger.warning(f"ðŸš¨ [v3.5 Escalate] {escalated_count}ä¸ªæŒ‘æˆ˜éœ€è¦ç”²æ–¹è£å†³ï¼Œè·¯ç”±åˆ°å®¡æ ¸èŠ‚ç‚¹")
            return "analysis_review"
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å›žè®¿éœ€æ±‚åˆ†æžå¸ˆï¼ˆrevisit_raé—­çŽ¯ï¼‰
        requires_feedback = state.get("requires_feedback_loop", False)
        if requires_feedback:
            reason = state.get("feedback_loop_reason", "ä¸“å®¶æŒ‘æˆ˜éœ€è¦æ¾„æ¸…")
            logger.info(f"ðŸ”„ [v3.5] å¯åŠ¨åé¦ˆå¾ªçŽ¯: {reason}")
            return "revisit_requirements"
        
        # é»˜è®¤ç»§ç»­æ­£å¸¸æµç¨‹
        logger.info("âž¡ï¸ [v3.5] ç»§ç»­æ­£å¸¸å·¥ä½œæµ")
        return "continue_workflow"

    def _batch_router_node(self, state: ProjectAnalysisState) -> Command:
        """
        æ‰¹æ¬¡è·¯ç”±å™¨èŠ‚ç‚¹ - å†³å®šä¸‹ä¸€æ­¥æ‰§è¡Œä»€ä¹ˆ

        è¿™æ˜¯åŠ¨æ€æ‰¹æ¬¡æ‰§è¡Œçš„å…³é”®è·¯ç”±èŠ‚ç‚¹ï¼Œæ›¿ä»£äº†ç¡¬ç¼–ç çš„è¾¹è¿žæŽ¥ã€‚

        å·¥ä½œåŽŸç†:
        1. æ£€æŸ¥æ˜¯å¦ä¸ºå®¡æ ¸è§¦å‘çš„é‡æ–°æ‰§è¡Œ
        2. å¦‚æžœæ˜¯é‡æ–°æ‰§è¡Œä¸”skip_second_review=True â†’ è·³è¿‡å®¡æ ¸ï¼Œç›´æŽ¥è¿›å…¥åŽç»­æµç¨‹
        3. å¦‚æžœæ˜¯é‡æ–°æ‰§è¡Œä¸”éœ€è¦å®¡æ ¸ â†’ è¿”å›žåˆ†æžå®¡æ ¸
        4. æ£€æŸ¥å½“å‰æ‰¹æ¬¡å’Œæ€»æ‰¹æ¬¡æ•°
        5. å¦‚æžœè¿˜æœ‰ä¸‹ä¸€æ‰¹æ¬¡ â†’ å¢žåŠ æ‰¹æ¬¡å·å¹¶è·¯ç”±åˆ°æ‰¹æ¬¡ç­–ç•¥å®¡æ ¸
        6. å¦‚æžœæ‰€æœ‰æ‰¹æ¬¡å®Œæˆ â†’ è·¯ç”±åˆ°åˆ†æžå®¡æ ¸

        çŠ¶æ€è¾“å…¥:
        - is_rerun: bool - æ˜¯å¦ä¸ºå®¡æ ¸è§¦å‘çš„é‡æ–°æ‰§è¡Œ
        - skip_second_review: bool - æ•´æ”¹åŽæ˜¯å¦è·³è¿‡äºŒæ¬¡å®¡æ ¸
        - current_batch: int - å½“å‰å·²å®Œæˆçš„æ‰¹æ¬¡ç¼–å·
        - total_batches: int - æ€»æ‰¹æ¬¡æ•°
        - dependency_summary: Dict - å½“å‰æ‰¹æ¬¡çš„å®Œæˆæ‘˜è¦

        è¿”å›ž:
        - Command(goto="detect_challenges") - å¦‚æžœæ˜¯æ•´æ”¹åŽè·³è¿‡å®¡æ ¸
        - Command(goto="analysis_review") - å¦‚æžœéœ€è¦å®¡æ ¸
        - Command(goto="batch_strategy_review") - å¦‚æžœè¿˜æœ‰ä¸‹ä¸€æ‰¹æ¬¡
        """
        try:
            is_rerun = state.get("is_rerun", False)
            skip_second_review = state.get("skip_second_review", False)
            current_batch = state.get("current_batch", 1)
            batches = state.get("execution_batches", [])
            total_batches = state.get("total_batches", len(batches))
            dependency_summary = state.get("dependency_summary", {})

            logger.info(f"ðŸ”€ Batch router: current={current_batch}, total={total_batches}, is_rerun={is_rerun}, skip_second_review={skip_second_review}")

            # âœ… ä¼˜å…ˆæ£€æŸ¥ï¼šå¦‚æžœæ˜¯å®¡æ ¸è§¦å‘çš„é‡æ–°æ‰§è¡Œï¼ˆæ•´æ”¹ï¼‰
            if is_rerun:
                # ðŸŽ¯ å…³é”®æ”¹è¿›ï¼šæ£€æŸ¥æ˜¯å¦è·³è¿‡äºŒæ¬¡å®¡æ ¸
                if skip_second_review:
                    logger.info("âœ… ä¸“å®¶æ•´æ”¹å®Œæˆï¼Œè·³è¿‡äºŒæ¬¡å®¡æ ¸ï¼Œç›´æŽ¥è¿›å…¥æŒ‘æˆ˜æ£€æµ‹å’ŒæŠ¥å‘Šç”Ÿæˆ")
                    return Command(
                        update={
                            "is_rerun": False,  # æ¸…é™¤æ ‡è®°
                            "skip_second_review": False,  # æ¸…é™¤æ ‡è®°
                            "specific_agents_to_run": [],  # æ¸…ç©º
                            "analysis_approved": True,  # æ ‡è®°ä¸ºå®¡æ ¸é€šè¿‡ï¼ˆæ•´æ”¹è§†ä¸ºé€šè¿‡ï¼‰
                        },
                        goto="detect_challenges"  # ðŸ”‘ è·³è¿‡å®¡æ ¸ï¼Œç›´æŽ¥è¿›å…¥æŒ‘æˆ˜æ£€æµ‹
                    )
                else:
                    # éœ€è¦äºŒæ¬¡å®¡æ ¸ï¼ˆä¸€èˆ¬ä¸ä¼šèµ°åˆ°è¿™é‡Œï¼Œå› ä¸ºæˆ‘ä»¬è®¾ç½®äº†skip_second_review=Trueï¼‰
                    logger.info("ðŸ”„ é‡æ–°æ‰§è¡Œå®Œæˆï¼Œè¿”å›žåˆ†æžå®¡æ ¸è¿›è¡Œè¯„ä¼°")
                    return Command(
                        update={
                            "is_rerun": False,  # æ¸…é™¤æ ‡è®°
                            "specific_agents_to_run": []  # æ¸…ç©º
                        },
                        goto="analysis_review"
                    )

            # éªŒè¯æ‰¹æ¬¡å®Œæˆæƒ…å†µ
            batch_completed = dependency_summary.get("batch_completed", False)
            if not batch_completed:
                logger.warning(f"âš ï¸ Batch {current_batch} not fully completed, but routing to next step")
                completed_count = dependency_summary.get("completed_count", 0)
                total_count = dependency_summary.get("total_count", 0)
                logger.warning(f"   Completed: {completed_count}/{total_count}")

            # å†³ç­–è·¯ç”±
            if current_batch < total_batches:
                # è¿˜æœ‰ä¸‹ä¸€æ‰¹æ¬¡ï¼Œå¢žåŠ æ‰¹æ¬¡å·
                next_batch = current_batch + 1
                logger.info(f"âž¡ï¸  Routing to next batch: {next_batch}/{total_batches}")
                logger.info(f"   Next batch will contain: {len(batches[next_batch - 1])} agents")

                return Command(
                    update={"current_batch": next_batch},
                    goto="batch_strategy_review"
                )
            else:
                # æ‰€æœ‰æ‰¹æ¬¡å®Œæˆï¼Œæ£€æŸ¥æ˜¯å¦å·²å®¡æ ¸é€šè¿‡
                logger.info(f"âœ… All {total_batches} batches completed")
                
                # âœ… ä¿®å¤ï¼šå¦‚æžœå·²å®¡æ ¸é€šè¿‡ï¼ˆç¬¬1è½®è§¦å‘detect_challengesï¼‰ï¼Œä¸å†é‡å¤è§¦å‘
                if state.get("analysis_approved", False):
                    logger.info("âœ… åˆ†æžå·²å®¡æ ¸é€šè¿‡ï¼Œè·³è¿‡é‡å¤å®¡æ ¸")
                    return Command(goto=END)
                
                logger.info("âž¡ï¸  Routing to analysis review")
                return Command(goto="analysis_review")

        except Exception as e:
            logger.error(f"âŒ Batch router failed: {e}")
            import traceback
            traceback.print_exc()
            # å‡ºé”™æ—¶é»˜è®¤è¿›å…¥å®¡æ ¸
            return Command(goto="analysis_review")

    def _batch_strategy_review_node(self, state: ProjectAnalysisState) -> Command:
        """
        æ‰¹æ¬¡ç­–ç•¥å®¡æ ¸èŠ‚ç‚¹ - æ”¯æŒå¤šç§æ‰§è¡Œæ¨¡å¼ (v3.6ä¼˜åŒ–)

        ðŸ”„ v3.6ä¼˜åŒ– (2025-11-29): æ”¯æŒç”¨æˆ·ç¡®è®¤æ¨¡å¼
        - é»˜è®¤ï¼šæ‰‹åŠ¨ç¡®è®¤æ¨¡å¼ï¼Œç”¨æˆ·å¯æŸ¥çœ‹å¹¶æ‰¹å‡†æ¯ä¸ªæ‰¹æ¬¡
        - å¯é€‰ï¼šè‡ªåŠ¨æ‰§è¡Œæ¨¡å¼ï¼ˆé€šè¿‡execution_modeé…ç½®ï¼‰

        æ”¯æŒçš„æ‰§è¡Œæ¨¡å¼:
        - "manual": æ¯æ‰¹æ¬¡éƒ½éœ€è¦ç”¨æˆ·ç¡®è®¤ï¼ˆé»˜è®¤ï¼‰
        - "automatic": å…¨è‡ªåŠ¨æ‰§è¡Œï¼ˆæ—§æ–¹æ¡ˆCè¡Œä¸ºï¼‰
        - "preview": æ˜¾ç¤ºè®¡åˆ’åŽè‡ªåŠ¨æ‰§è¡Œ

        è®¾è®¡ç†å¿µ:
        - æ‰¹æ¬¡ç­–ç•¥æ˜¯æŠ€æœ¯å®žçŽ°ç»†èŠ‚ï¼Œä½†ç”¨æˆ·åº”è¯¥æœ‰é€‰æ‹©æƒ
        - æä¾›é€æ˜Žåº¦å’ŒæŽ§åˆ¶æ„Ÿï¼ŒåŒæ—¶ä¿æŒçµæ´»æ€§
        - é»˜è®¤å®‰å…¨ï¼ˆéœ€ç¡®è®¤ï¼‰ï¼Œä½†æ”¯æŒé«˜æ•ˆæ¨¡å¼ï¼ˆè‡ªåŠ¨ï¼‰

        å·¥ä½œåŽŸç†:
        1. è¯»å–æ‰§è¡Œæ¨¡å¼é…ç½®
        2. æ ¹æ®æ¨¡å¼é€‰æ‹©æ˜¯å¦è§¦å‘ç”¨æˆ·ç¡®è®¤
        3. æ”¶é›†æ‰¹æ¬¡ä¿¡æ¯å¹¶æ˜¾ç¤ºç»™ç”¨æˆ·

        çŠ¶æ€è¾“å…¥:
        - current_batch: int - å³å°†æ‰§è¡Œçš„æ‰¹æ¬¡ç¼–å·
        - execution_batches: List[List[str]] - æ‰€æœ‰æ‰¹æ¬¡
        - execution_mode: str - æ‰§è¡Œæ¨¡å¼é…ç½®ï¼ˆå¯é€‰ï¼‰

        è¿”å›ž:
        - Command - æ ¹æ®æ¨¡å¼è¿”å›žä¸åŒçš„å‘½ä»¤
        """
        try:
            from langgraph.types import interrupt

            current_batch = state.get("current_batch", 1)
            batches = state.get("execution_batches", [])
            execution_mode = state.get("execution_mode", "manual")  # é»˜è®¤æ‰‹åŠ¨ç¡®è®¤

            # èŽ·å–å½“å‰æ‰¹æ¬¡çš„ä¸“å®¶åˆ—è¡¨
            batch_agents = batches[current_batch - 1] if current_batch <= len(batches) else []

            # ðŸ”¥ v3.6: æ ¹æ®æ‰§è¡Œæ¨¡å¼å†³å®šæ˜¯å¦éœ€è¦ç”¨æˆ·ç¡®è®¤
            if execution_mode == "automatic":
                # æ–¹æ¡ˆCï¼šå…¨è‡ªåŠ¨æ‰§è¡Œï¼ˆæ—§è¡Œä¸ºï¼‰
                logger.info(f"âš¡ æ‰¹æ¬¡ {current_batch}/{len(batches)} è‡ªåŠ¨æ‰§è¡Œï¼ˆæ–¹æ¡ˆCï¼šå…¨è‡ªåŠ¨æ‰¹æ¬¡è°ƒåº¦ï¼‰")
                return Command(
                    update={
                        "batch_strategy_approved": True,
                        "auto_approved": True,
                        "auto_approval_reason": "execution_mode=automatic"
                    },
                    goto="batch_executor"
                )
            elif execution_mode == "preview":
                # æ–¹æ¡ˆDï¼šæ˜¾ç¤ºè®¡åˆ’åŽè‡ªåŠ¨æ‰§è¡Œ
                logger.info(f"ðŸ“‹ æ‰¹æ¬¡ {current_batch}/{len(batches)} æ˜¾ç¤ºè®¡åˆ’åŽè‡ªåŠ¨æ‰§è¡Œ")
                # TODO: å¯ä»¥åœ¨è¿™é‡Œå‘é€é€šçŸ¥ç»™å‰ç«¯
                return Command(
                    update={
                        "batch_strategy_approved": True,
                        "auto_approved": True,
                        "auto_approval_reason": "execution_mode=preview"
                    },
                    goto="batch_executor"
                )
            else:
                # æ–¹æ¡ˆAï¼šæ‰‹åŠ¨ç¡®è®¤æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
                logger.info(f"ðŸ‘¤ æ‰¹æ¬¡ {current_batch}/{len(batches)} ç­‰å¾…ç”¨æˆ·ç¡®è®¤")

                # æž„å»ºæ‰¹æ¬¡ä¿¡æ¯
                batch_info = {
                    "interaction_type": "batch_confirmation",  # ðŸ”¥ ä¿®å¤ï¼šä¸Žå…¶ä»– interrupt ä¿æŒä¸€è‡´ä½¿ç”¨ interaction_type
                    "current_batch": current_batch,
                    "total_batches": len(batches),
                    "agents_in_batch": batch_agents,
                    "execution_strategy": "parallel" if len(batch_agents) > 1 else "sequential",
                    "message": f"å‡†å¤‡æ‰§è¡Œæ‰¹æ¬¡ {current_batch}/{len(batches)}",
                    "details": {
                        "ä¸“å®¶åˆ—è¡¨": batch_agents,
                        "æ‰§è¡Œæ–¹å¼": "å¹¶è¡Œæ‰§è¡Œ" if len(batch_agents) > 1 else "é¡ºåºæ‰§è¡Œ",
                        "é¢„è®¡è€—æ—¶": f"{len(batch_agents) * 3}åˆ†é’Ÿ" if len(batch_agents) == 1 else f"{4}åˆ†é’Ÿ"
                    },
                    "options": {
                        "approve": "æ‰¹å‡†æ‰§è¡Œ",
                        "skip": "è·³è¿‡æ­¤æ‰¹æ¬¡",
                        "modify": "ä¿®æ”¹æ‰¹æ¬¡é…ç½®"
                    }
                }

                # è§¦å‘ä¸­æ–­ï¼Œç­‰å¾…ç”¨æˆ·ç¡®è®¤
                logger.info(f"ðŸ” [DEBUG] è§¦å‘æ‰¹æ¬¡ç¡®è®¤ä¸­æ–­: {batch_info}")
                user_response = interrupt(batch_info)
                logger.info(f"ðŸ“¥ æ”¶åˆ°ç”¨æˆ·å“åº”: {user_response}")

                # å¤„ç†ç”¨æˆ·å“åº”
                if user_response == "skip" or (isinstance(user_response, dict) and user_response.get("action") == "skip"):
                    logger.info("â­ï¸ ç”¨æˆ·é€‰æ‹©è·³è¿‡æ­¤æ‰¹æ¬¡")
                    # è·³è¿‡å½“å‰æ‰¹æ¬¡ï¼Œè¿›å…¥ä¸‹ä¸€æ‰¹æ¬¡
                    return Command(
                        update={
                            "current_batch": current_batch + 1,
                            "batch_strategy_approved": False,
                            "batch_skipped": True
                        },
                        goto="batch_router"  # è¿”å›žè·¯ç”±å™¨æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šæ‰¹æ¬¡
                    )
                else:
                    # æ‰¹å‡†æ‰§è¡Œï¼ˆé»˜è®¤è¡Œä¸ºï¼‰
                    logger.info("âœ… ç”¨æˆ·æ‰¹å‡†æ‰§è¡Œæ­¤æ‰¹æ¬¡")
                    return Command(
                        update={
                            "batch_strategy_approved": True,
                            "auto_approved": False,
                            "user_confirmed": True
                        },
                        goto="batch_executor"
                    )

        except Exception as e:
            # åªæ•èŽ·éžInterruptå¼‚å¸¸
            if "Interrupt" not in str(type(e)):
                logger.error(f"âŒ Batch strategy review failed: {e}")
                import traceback
                traceback.print_exc()
                # å‡ºé”™æ—¶é»˜è®¤æ‰¹å‡†å¹¶ç»§ç»­
                return Command(goto="batch_executor")
            else:
                # é‡æ–°æŠ›å‡ºInterruptå¼‚å¸¸ï¼ˆLangGraphéœ€è¦ï¼‰
                raise

    def _analysis_review_node(self, state: ProjectAnalysisState) -> Command:
        """
        åˆ†æžå®¡æ ¸èŠ‚ç‚¹ - é€’è¿›å¼å•è½®å®¡æ ¸ (v2.0)

        æ ¸å¿ƒæ”¹è¿›:
        1. ç§»é™¤å¤šè½®è¿­ä»£é€»è¾‘
        2. é€’è¿›å¼ä¸‰é˜¶æ®µï¼šçº¢â†’è“â†’è¯„å§”â†’ç”²æ–¹
        3. è¾“å‡ºæ”¹è¿›å»ºè®®ï¼ˆè€Œéžé‡æ–°æ‰§è¡Œï¼‰
        4. è®°å½•final_rulingåˆ°state
        
        ðŸ”¥ v7.16: æ”¯æŒæ–°ç‰ˆ LangGraph AnalysisReviewAgent
        """
        logger.info("Executing progressive single-round analysis review node")
        
        # ðŸ†• v7.16: ä½¿ç”¨æ–°ç‰ˆ LangGraph Agentï¼ˆå¦‚æžœå¯ç”¨ï¼‰
        if USE_V716_AGENTS:
            logger.info("ðŸš€ [v7.16] ä½¿ç”¨ AnalysisReviewAgent")
            return AnalysisReviewNodeCompat.execute(
                state=state,
                store=self.store,
                llm_model=self.llm_model,
                config=self.config
            )
        
        return AnalysisReviewNode.execute(
            state=state,
            store=self.store,
            llm_model=self.llm_model,
            config=self.config
        )
    
    def _manual_review_node(self, state: ProjectAnalysisState) -> Command:
        """
        äººå·¥å®¡æ ¸èŠ‚ç‚¹ - å¤„ç†ä¸¥é‡è´¨é‡é—®é¢˜ (ðŸ†•)
        
        å½“å®¡æ ¸ç³»ç»Ÿå‘çŽ°>3ä¸ªmust_fixé—®é¢˜æ—¶è§¦å‘ï¼Œæš‚åœæµç¨‹ç­‰å¾…ç”¨æˆ·è£å†³ï¼š
        1. ç»§ç»­ï¼šæŽ¥å—é£Žé™©ç”ŸæˆæŠ¥å‘Š
        2. ç»ˆæ­¢ï¼šå…¨é¢æ•´æ”¹åŽå†ç”ŸæˆæŠ¥å‘Š
        3. é€‰æ‹©æ€§æ•´æ”¹ï¼šç”¨æˆ·é€‰æ‹©å…³é”®é—®é¢˜è¿›è¡Œæ•´æ”¹
        
        æ³¨æ„: ä¸è¦æ•èŽ·Interruptå¼‚å¸¸!
        Interruptæ˜¯LangGraphçš„æ­£å¸¸æŽ§åˆ¶æµ,å¿…é¡»è®©å®ƒä¼ æ’­åˆ°æ¡†æž¶å±‚
        """
        logger.info("ðŸš¨ Executing manual review node for critical quality issues")
        return ManualReviewNode.execute(
            state=state,
            store=self.store
        )
    
    def _result_aggregator_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        ç»“æžœèšåˆèŠ‚ç‚¹

        æ³¨æ„: åªè¿”å›žéœ€è¦æ›´æ–°çš„å­—æ®µ,ä¸è¿”å›žå®Œæ•´çŠ¶æ€
        è¿™æ ·å¯ä»¥é¿å…å¹¶å‘æ›´æ–°å†²çª
        
        ðŸ”§ ä¿®å¤: ä¸æ›´æ–°current_stage,é¿å…ä¸Žpdf_generatorå¹¶å‘å†²çª
        ðŸ”¥ v7.16: æ”¯æŒæ–°ç‰ˆ LangGraph ResultAggregatorAgentV2
        """
        try:
            logger.info("Executing result aggregator node")

            # ðŸ†• v7.16: ä½¿ç”¨æ–°ç‰ˆ LangGraph Agentï¼ˆå¦‚æžœå¯ç”¨ï¼‰
            if USE_V716_AGENTS:
                logger.info("ðŸš€ [v7.16] ä½¿ç”¨ ResultAggregatorAgentV2")
                agent = ResultAggregatorAgentCompat(
                    llm_model=self.llm_model,
                    config=self.config
                )
            else:
                # åˆ›å»ºç»“æžœèšåˆå™¨æ™ºèƒ½ä½“
                agent = ResultAggregatorAgent(
                    llm_model=self.llm_model,
                    config=self.config
                )

            # æ‰§è¡Œèšåˆ
            result = agent.execute(state, {}, self.store)

            # åªè¿”å›žéœ€è¦æ›´æ–°çš„å­—æ®µ (ä¸æ›´æ–°current_stage)
            return {
                "final_report": result.structured_data,
                "agent_results": {
                    "RESULT_AGGREGATOR": result.to_dict()
                },
                "updated_at": datetime.now().isoformat(),
                "detail": "æ•´åˆä¸“å®¶æˆæžœï¼Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šè‰ç¨¿"
            }

        except Exception as e:
            logger.error(f"Result aggregator node failed: {e}")
            return {
                "error": str(e),
                "updated_at": datetime.now().isoformat(),
                "detail": "ç»“æžœèšåˆå¤±è´¥"
            }
    
    # def _final_review_node(self, state: ProjectAnalysisState) -> Command:
    #     """
    #     æœ€ç»ˆå®¡æ ¸èŠ‚ç‚¹ - å·²ç§»é™¤
    #
    #     æ ¹æ®å®¢æˆ·éœ€æ±‚,å·¥ä½œæµç¨‹åº”è¯¥æ˜¯:
    #     ç»“æžœèšåˆ â†’ PDFç”Ÿæˆ â†’ ç»“æŸ
    #     ä¸éœ€è¦æœ€ç»ˆå®¡æ ¸é˜¶æ®µ
    #
    #     æ³¨æ„: ä¸è¦æ•èŽ·Interruptå¼‚å¸¸!
    #     Interruptæ˜¯LangGraphçš„æ­£å¸¸æŽ§åˆ¶æµ,å¿…é¡»è®©å®ƒä¼ æ’­åˆ°æ¡†æž¶å±‚
    #     """
    #     logger.info("Executing final review node")
    #     return FinalReviewNode.execute(state, self.store)
    
    def _pdf_generator_node(self, state: ProjectAnalysisState) -> Dict[str, Any]:
        """
        æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹ (ä½¿ç”¨çº¯æ–‡æœ¬ç”Ÿæˆå™¨è¿›è¡Œæµ‹è¯•)

        æ³¨æ„: åªè¿”å›žéœ€è¦æ›´æ–°çš„å­—æ®µ,ä¸è¿”å›žå®Œæ•´çŠ¶æ€
        è¿™æ ·å¯ä»¥é¿å…å¹¶å‘æ›´æ–°å†²çª
        """
        try:
            logger.info("Executing report generator node (text mode for testing)")

            # ä½¿ç”¨çº¯æ–‡æœ¬ç”Ÿæˆå™¨è¿›è¡Œæµ‹è¯•
            from ..report.text_generator import TextGeneratorAgent
            agent = TextGeneratorAgent(config=self.config)

            # æ‰§è¡ŒæŠ¥å‘Šç”Ÿæˆ
            result = agent.execute(state, {}, self.store)

            # åªè¿”å›žéœ€è¦æ›´æ–°çš„å­—æ®µ
            return {
                "current_stage": AnalysisStage.COMPLETED.value,
                "pdf_path": result.structured_data.get("file_path"),
                "agent_results": {
                    "REPORT_GENERATOR": result.to_dict()
                },
                "updated_at": datetime.now().isoformat(),
                "detail": "ç”Ÿæˆæœ€ç»ˆäº¤ä»˜æ–‡æ¡£"
            }

        except Exception as e:
            logger.error(f"Report generator node failed: {e}")
            return {
                "error": str(e),
                "updated_at": datetime.now().isoformat(),
                "detail": "æŠ¥å‘Šç”Ÿæˆå¤±è´¥"
            }
    
    def _user_question_node(self, state: ProjectAnalysisState) -> Command:
        """
        ç”¨æˆ·è¿½é—®èŠ‚ç‚¹
        
        ðŸ”§ ä¿®å¤ (2025-11-27): ä¸è¦æ•èŽ· Interrupt å¼‚å¸¸
        Interrupt æ˜¯ LangGraph çš„æ­£å¸¸æŽ§åˆ¶æµï¼Œå¿…é¡»è®©å®ƒä¼ æ’­åˆ°æ¡†æž¶å±‚
        """
        logger.info("Executing user question node")
        # âŒ ä¸è¦ç”¨ try-except æ•èŽ·ï¼Interrupt å¿…é¡»ä¼ æ’­
        return UserQuestionNode.execute(state, self.store)
    
    # è·¯ç”±å‡½æ•°
    def _route_from_batch_aggregator(self, state: ProjectAnalysisState) -> str:
        """
        æ‰¹æ¬¡èšåˆå™¨åŽçš„è·¯ç”±å‡½æ•°

        ðŸ”§ ä¿®å¤ï¼ˆ2025-11-30ï¼‰ï¼šç§»é™¤ analysis_approved çš„ END è·¯å¾„
        - BugåŽŸå› ï¼šå½“ analysis_approved=True æ—¶ç›´æŽ¥ ENDï¼Œè·³è¿‡äº† result_aggregator å’Œ pdf_generator
        - ä¿®å¤ï¼šå§‹ç»ˆè·¯ç”±åˆ° batch_routerï¼Œè®© batch_router å†³å®šæ˜¯å¦è¿›å…¥ analysis_review
        - analysis_review â†’ detect_challenges â†’ result_aggregator æ˜¯å”¯ä¸€ç”ŸæˆæŠ¥å‘Šçš„è·¯å¾„

        é€»è¾‘:
        - å§‹ç»ˆ â†’ batch_router
          - batch_router æ£€æŸ¥æ‰¹æ¬¡å®Œæˆæƒ…å†µï¼š
            - å¦‚æžœè¿˜æœ‰æ‰¹æ¬¡ï¼šç»§ç»­ä¸‹ä¸€æ‰¹
            - å¦‚æžœæ‰€æœ‰æ‰¹æ¬¡å®Œæˆï¼šè¿›å…¥ analysis_review â†’ detect_challenges â†’ result_aggregator â†’ pdf_generator
        """
        logger.info("ðŸ”€ [batch_aggregator] è·¯ç”±åˆ° batch_router æ£€æŸ¥æ‰¹æ¬¡çŠ¶æ€")
        return "batch_router"
    
    def _route_after_requirements_confirmation(self, state: ProjectAnalysisState) -> Literal["project_director", "requirements_analyst"]:
        """éœ€æ±‚ç¡®è®¤åŽçš„è·¯ç”±"""
        if state.get("requirements_confirmed"):
            return "project_director"
        else:
            return "requirements_analyst"
    
    def _route_after_pdf_generator(self, state: ProjectAnalysisState) -> Union[str, Any]:
        """æŠ¥å‘Šç”ŸæˆåŽçš„è·¯ç”±: ç›´æŽ¥ç»“æŸï¼Œç”±å‰ç«¯è´Ÿè´£ç»“æžœå‘ˆçŽ°å’Œè¿½é—®äº¤äº’"""
        # æ ‡è®°è¿½é—®åŠŸèƒ½å¯ç”¨ï¼Œå‰ç«¯ä¼šæ ¹æ®æ­¤æ ‡å¿—æ˜¾ç¤ºè¿½é—®å…¥å£
        state["post_completion_followup_available"] = self.config.get("post_completion_followup_enabled", True)
        
        logger.info("âœ… [pdf_generator] æŠ¥å‘Šå·²ç”Ÿæˆï¼Œæµç¨‹ç»“æŸï¼Œå‰ç«¯æŽ¥ç®¡ç»“æžœå‘ˆçŽ°")
        return END

    def _route_after_analysis_review(self, state: ProjectAnalysisState) -> Literal["result_aggregator", "project_director", "user_question"]:
        """åˆ†æžå®¡æ ¸åŽçš„è·¯ç”±"""
        if state.get("analysis_approved"):
            return "result_aggregator"
        elif state.get("additional_questions"):
            return "user_question"
        else:
            return "project_director"
    
    # def _route_after_final_review(self, state: ProjectAnalysisState) -> Literal["pdf_generator", "result_aggregator"]:
    #     """æœ€ç»ˆå®¡æ ¸åŽçš„è·¯ç”± - å·²ç§»é™¤ï¼Œå› ä¸ºä¸å†æœ‰æœ€ç»ˆå®¡æ ¸é˜¶æ®µ"""
    #     if state.get("final_approved"):
    #         return "pdf_generator"
    #     else:
    #         return "result_aggregator"
    
    def _route_after_user_question(self, state: ProjectAnalysisState) -> Literal["project_director", END]:
        """
        ç”¨æˆ·è¿½é—®åŽçš„è·¯ç”±

        ðŸ”§ ä¿®å¤ (2025-11-27): è¿½é—®å®ŒæˆåŽåº”è¯¥ç»“æŸæµç¨‹ï¼Œè€Œä¸æ˜¯å›žåˆ° result_aggregator
        é¿å…æ— é™å¾ªçŽ¯ï¼špdf_generator â†’ user_question â†’ result_aggregator â†’ pdf_generator

        ðŸ”¥ ä¿®å¤ (2025-11-29): additional_questionsæ˜¯listç±»åž‹ï¼Œä¸èƒ½è°ƒç”¨.strip()
        """
        additional_questions = state.get("additional_questions")

        # ðŸ”¥ ä¿®å¤ï¼šadditional_questions æ˜¯listï¼Œä¸æ˜¯string
        if additional_questions and len(additional_questions) > 0:
            logger.info(f"ðŸ“ ç”¨æˆ·æå‡ºè¿½é—®({len(additional_questions)}ä¸ª)ï¼Œè¿”å›ž project_director é‡æ–°åˆ†æž")
            return "project_director"
        else:
            logger.info("âœ… ç”¨æˆ·æœªè¿½é—®æˆ–è¿½é—®å®Œæˆï¼Œæµç¨‹ç»“æŸ")
            return END
    
    def run(self, user_input: str, session_id: Optional[str] = None) -> Dict[str, Any]:
        """
        è¿è¡Œå·¥ä½œæµ
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            session_id: ä¼šè¯ID
            
        Returns:
            å·¥ä½œæµæ‰§è¡Œç»“æžœ
        """
        try:
            # ç”Ÿæˆä¼šè¯ID
            if not session_id:
                session_id = str(uuid.uuid4())
            
            # åˆå§‹åŒ–çŠ¶æ€
            initial_state = StateManager.create_initial_state(
                user_input=user_input,
                session_id=session_id
            )
            
            logger.info(f"Starting workflow execution for session {session_id}")
            
            # æ‰§è¡Œå·¥ä½œæµ
            config = {"configurable": {"thread_id": session_id}}
            final_state = self.graph.invoke(initial_state, config)
            
            logger.info(f"Workflow execution completed for session {session_id}")
            
            return {
                "session_id": session_id,
                "status": "completed",
                "final_state": final_state,
                "pdf_path": final_state.get("pdf_path"),
                "execution_time": final_state.get("execution_time")
            }
            
        except Exception as e:
            logger.error(f"Workflow execution failed: {e}")
            return {
                "session_id": session_id,
                "status": "failed",
                "error": str(e)
            }

    def _build_context_for_expert(self, state: ProjectAnalysisState) -> str:
        """
        ä¸ºä»»åŠ¡å¯¼å‘ä¸“å®¶æž„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯

        ðŸ”¥ v7.18 å‡çº§4: ä¸“å®¶åä½œé€šé“ - ä¼ é€’å‰åºä¸“å®¶çš„å®Œæ•´è¾“å‡º

        Args:
            state: å½“å‰é¡¹ç›®çŠ¶æ€

        Returns:
            str: æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        context_parts = []

        # æ·»åŠ ç”¨æˆ·éœ€æ±‚
        task_description = state.get("task_description", "")
        if task_description:
            context_parts.append(f"## ç”¨æˆ·éœ€æ±‚\n{task_description}")

        # æ·»åŠ ç»“æž„åŒ–éœ€æ±‚
        structured_requirements = state.get("structured_requirements", {})
        if structured_requirements:
            context_parts.append("## ç»“æž„åŒ–éœ€æ±‚")
            for key, value in structured_requirements.items():
                if value:
                    context_parts.append(f"**{key}**: {value}")

        # ðŸ”¥ v7.18 å‡çº§4: ä¼ é€’å®Œæ•´çš„å‰åºä¸“å®¶è¾“å‡ºï¼ˆè€Œéžæˆªæ–­åˆ°500å­—ç¬¦ï¼‰
        agent_results = state.get("agent_results", {})
        if agent_results:
            context_parts.append("## å‰åºä¸“å®¶çš„åˆ†æžæˆæžœ")
            context_parts.append("**è¯´æ˜Ž**: ä»¥ä¸‹æ˜¯å‰åºä¸“å®¶çš„å®Œæ•´åˆ†æžç»“æžœï¼Œä½ å¯ä»¥å‚è€ƒå’Œå¼•ç”¨ã€‚\n")

            for expert_id, result in agent_results.items():
                if not isinstance(result, dict):
                    continue

                # èŽ·å–ä¸“å®¶åç§°
                expert_name = result.get("expert_name", expert_id)
                context_parts.append(f"### {expert_name} ({expert_id})")

                # ðŸ”¥ v7.18 å‡çº§4: æå–ç»“æž„åŒ–è¾“å‡ºä¸­çš„äº¤ä»˜ç‰©
                structured_output = result.get("structured_output")
                if structured_output and isinstance(structured_output, dict):
                    task_report = structured_output.get("task_execution_report", {})
                    deliverable_outputs = task_report.get("deliverable_outputs", [])

                    if deliverable_outputs:
                        context_parts.append(f"**äº¤ä»˜ç‰©æ•°é‡**: {len(deliverable_outputs)}\n")

                        for i, deliverable in enumerate(deliverable_outputs, 1):
                            deliverable_name = deliverable.get("deliverable_name", f"äº¤ä»˜ç‰©{i}")
                            content = deliverable.get("content", "")
                            completion_status = deliverable.get("completion_status", "unknown")

                            context_parts.append(f"#### äº¤ä»˜ç‰© {i}: {deliverable_name}")
                            context_parts.append(f"**çŠ¶æ€**: {completion_status}")

                            # ðŸ”¥ ä¼ é€’å®Œæ•´å†…å®¹ï¼ˆä¸æˆªæ–­ï¼‰
                            if content:
                                context_parts.append(f"**å†…å®¹**:\n{content}\n")
                    else:
                        # é™çº§ï¼šä½¿ç”¨analysiså­—æ®µ
                        analysis_content = result.get("analysis", "")
                        if analysis_content:
                            context_parts.append(f"**åˆ†æžå†…å®¹**:\n{analysis_content}\n")
                else:
                    # é™çº§ï¼šä½¿ç”¨analysiså­—æ®µ
                    analysis_content = result.get("analysis", "")
                    if analysis_content:
                        context_parts.append(f"**åˆ†æžå†…å®¹**:\n{analysis_content}\n")

        # æ·»åŠ é¡¹ç›®çŠ¶æ€ä¿¡æ¯
        context_parts.append(f"\n## é¡¹ç›®çŠ¶æ€ä¿¡æ¯")
        context_parts.append(f"- å½“å‰é˜¶æ®µ: {state.get('current_phase', 'unknown')}")
        context_parts.append(f"- å·²å®Œæˆä¸“å®¶æ•°: {len(agent_results)}")

        # æ·»åŠ è´¨é‡æ£€æŸ¥æ¸…å•ï¼ˆå¦‚æžœæœ‰ï¼‰
        quality_checklist = state.get("quality_checklist", {})
        if quality_checklist:
            context_parts.append("## è´¨é‡è¦æ±‚")
            for category, criteria in quality_checklist.items():
                if isinstance(criteria, list):
                    context_parts.append(f"**{category}**: {', '.join(criteria)}")
                else:
                    context_parts.append(f"**{category}**: {criteria}")
        
        return "\n\n".join(context_parts)
