"""
v7.118 æœç´¢å·¥å…·å…¨æµç¨‹æµ‹è¯•
æµ‹è¯•åœºæ™¯ï¼šåŒ—äº¬å››åˆé™¢æ”¹é€ ä¸ºçº½çº¦Lofté£æ ¼

ç›®æ ‡ï¼š
1. éªŒè¯SearchStrategyGeneratorç”ŸæˆæŸ¥è¯¢æ­£å¸¸
2. éªŒè¯æœç´¢å·¥å…·è°ƒç”¨ï¼ˆTavily, Arxiv, RAGFlowï¼‰
3. éªŒè¯æœç´¢ç»“æœå¤„ç†å’Œé›†æˆ
4. å‘ç°æœç´¢æµç¨‹ä¸­çš„bug
"""

import asyncio
import json
import time
from datetime import datetime

import aiohttp
from loguru import logger

# æµ‹è¯•é…ç½®
BASE_URL = "http://localhost:8000"
TEST_USER_INPUT = """
An American who grew up in Beijing bought a small courtyard house (Siheyuan).
He wants to preserve the traditional architectural 'Qi', but achieve New York Loft's openness, minimalism and party functions inside.
"""


class SearchWorkflowTester:
    """æœç´¢å·¥ä½œæµæµ‹è¯•å™¨"""

    def __init__(self):
        self.session_id = None
        self.errors = []
        self.warnings = []
        self.search_events = []
        self.deliverables_tested = set()

    async def run_full_test(self):
        """è¿è¡Œå®Œæ•´æœç´¢å·¥ä½œæµæµ‹è¯•"""
        logger.info("=" * 80)
        logger.info("ğŸ” å¼€å§‹ v7.118 æœç´¢å·¥å…·å…¨æµç¨‹æµ‹è¯•")
        logger.info("=" * 80)

        try:
            # æ­¥éª¤1: æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
            if not await self.check_server():
                logger.error("âŒ æœåŠ¡å™¨æœªè¿è¡Œï¼Œæµ‹è¯•ç»ˆæ­¢")
                return

            # æ­¥éª¤2: å¯åŠ¨åˆ†æ
            if not await self.start_analysis():
                logger.error("âŒ å¯åŠ¨åˆ†æå¤±è´¥ï¼Œæµ‹è¯•ç»ˆæ­¢")
                return

            # æ­¥éª¤3: ç›‘å¬WebSocketè·å–å®æ—¶è¿›åº¦
            await self.monitor_workflow_with_websocket()

            # æ­¥éª¤4: ç”ŸæˆæŠ¥å‘Š
            self.generate_report()

        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            self.errors.append(f"æµ‹è¯•å¼‚å¸¸: {e}")
            import traceback

            traceback.print_exc()

    async def check_server(self):
        """æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ¥ æ­¥éª¤1: æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€")
        logger.info("=" * 60)

        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{BASE_URL}/health", timeout=aiohttp.ClientTimeout(total=3)) as response:
                    if response.status == 200:
                        logger.success("âœ… æœåŠ¡å™¨è¿è¡Œæ­£å¸¸")
                        return True
                    else:
                        logger.error(f"âŒ æœåŠ¡å™¨å“åº”å¼‚å¸¸: HTTP {response.status}")
                        return False
        except Exception as e:
            logger.error(f"âŒ æ— æ³•è¿æ¥æœåŠ¡å™¨: {e}")
            logger.info("ğŸ’¡ è¯·å…ˆå¯åŠ¨æœåŠ¡å™¨: python -m intelligent_project_analyzer.api.server")
            return False

    async def start_analysis(self):
        """å¯åŠ¨åˆ†æ"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸš€ æ­¥éª¤2: å¯åŠ¨åˆ†æä¼šè¯")
        logger.info("=" * 60)

        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(
                    f"{BASE_URL}/api/analysis/start",
                    json={"user_input": TEST_USER_INPUT, "user_id": "test_search_v7118"},
                    timeout=aiohttp.ClientTimeout(total=10),
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        self.session_id = data.get("session_id")
                        logger.success(f"âœ… åˆ†æå¯åŠ¨æˆåŠŸ")
                        logger.info(f"   Session ID: {self.session_id}")
                        return True
                    else:
                        error = f"å¯åŠ¨å¤±è´¥: HTTP {response.status}"
                        logger.error(f"âŒ {error}")
                        self.errors.append(error)
                        return False

            except Exception as e:
                error = f"å¯åŠ¨å¼‚å¸¸: {e}"
                logger.error(f"âŒ {error}")
                self.errors.append(error)
                return False

    async def monitor_workflow_with_websocket(self):
        """é€šè¿‡WebSocketç›‘å¬å·¥ä½œæµå¹¶æ•è·æœç´¢äº‹ä»¶"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“¡ æ­¥éª¤3: ç›‘å¬å·¥ä½œæµè¿›åº¦ï¼ˆå…³æ³¨æœç´¢äº‹ä»¶ï¼‰")
        logger.info("=" * 60)

        if not self.session_id:
            logger.warning("âš ï¸ æ— session_idï¼Œè·³è¿‡ç›‘å¬")
            return

        ws_url = f"ws://localhost:8000/ws/{self.session_id}"

        try:
            async with aiohttp.ClientSession() as session:
                async with session.ws_connect(ws_url, timeout=300) as ws:
                    logger.success("âœ… WebSocketè¿æ¥å»ºç«‹")
                    logger.info("   ç­‰å¾…å·¥ä½œæµäº‹ä»¶...")

                    message_count = 0
                    start_time = time.time()
                    timeout_seconds = 180  # 3åˆ†é’Ÿè¶…æ—¶

                    async for msg in ws:
                        if msg.type == aiohttp.WSMsgType.TEXT:
                            message_count += 1
                            data = json.loads(msg.data)

                            # å¤„ç†æ¶ˆæ¯
                            self.process_workflow_message(data, message_count)

                            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                            if data.get("type") == "complete":
                                logger.success(f"âœ… å·¥ä½œæµå®Œæˆï¼ˆå…±æ”¶åˆ°{message_count}æ¡æ¶ˆæ¯ï¼‰")
                                break

                            # æ£€æŸ¥é”™è¯¯
                            if data.get("type") == "error":
                                error = f"å·¥ä½œæµé”™è¯¯: {data.get('detail')}"
                                logger.error(f"âŒ {error}")
                                self.errors.append(error)
                                break

                        elif msg.type == aiohttp.WSMsgType.ERROR:
                            logger.error(f"âŒ WebSocketé”™è¯¯: {ws.exception()}")
                            break

                        # è¶…æ—¶æ£€æŸ¥
                        if time.time() - start_time > timeout_seconds:
                            warning = f"ç›‘å¬è¶…æ—¶({timeout_seconds}ç§’)ï¼Œå…±æ”¶åˆ°{message_count}æ¡æ¶ˆæ¯"
                            logger.warning(f"âš ï¸ {warning}")
                            self.warnings.append(warning)
                            break

        except Exception as e:
            error = f"WebSocketç›‘å¬å¤±è´¥: {e}"
            logger.error(f"âŒ {error}")
            self.errors.append(error)

    def process_workflow_message(self, data: dict, msg_num: int):
        """å¤„ç†å·¥ä½œæµæ¶ˆæ¯ï¼Œé‡ç‚¹å…³æ³¨æœç´¢ç›¸å…³äº‹ä»¶"""
        msg_type = data.get("type", "unknown")

        # èŠ‚ç‚¹è¿›åº¦æ¶ˆæ¯
        if msg_type == "node_progress":
            node = data.get("node", "")
            progress = data.get("progress", {})

            logger.debug(f"[æ¶ˆæ¯{msg_num}] èŠ‚ç‚¹: {node}")

            # ğŸ” é‡ç‚¹å…³æ³¨ï¼šæœç´¢æŸ¥è¯¢ç”ŸæˆèŠ‚ç‚¹
            if "search_query_generator" in node.lower():
                logger.info(f"ğŸ” [æœç´¢äº‹ä»¶] æœç´¢æŸ¥è¯¢ç”ŸæˆèŠ‚ç‚¹")
                self.search_events.append({"type": "query_generation", "node": node, "data": progress})

            # ğŸ” é‡ç‚¹å…³æ³¨ï¼šä¸“å®¶æœç´¢èŠ‚ç‚¹
            if "expert" in node.lower() and "search" in node.lower():
                logger.info(f"ğŸ” [æœç´¢äº‹ä»¶] ä¸“å®¶æœç´¢æ‰§è¡Œ")
                self.search_events.append({"type": "expert_search", "node": node, "data": progress})

            # ğŸ” æå–äº¤ä»˜ç‰©ä¿¡æ¯
            if progress.get("deliverable_name"):
                deliv_name = progress["deliverable_name"]
                self.deliverables_tested.add(deliv_name)
                logger.info(f"   äº¤ä»˜ç‰©: {deliv_name}")

        # å·¥å…·è°ƒç”¨æ¶ˆæ¯
        elif msg_type == "tool_call":
            tool_name = data.get("tool_name", "")
            logger.info(f"ğŸ”§ [å·¥å…·è°ƒç”¨] {tool_name}")

            # ğŸ” é‡ç‚¹å…³æ³¨ï¼šæœç´¢å·¥å…·
            if any(keyword in tool_name.lower() for keyword in ["tavily", "arxiv", "ragflow", "search"]):
                logger.success(f"âœ… [æœç´¢å·¥å…·è°ƒç”¨] {tool_name}")
                self.search_events.append({"type": "tool_call", "tool": tool_name, "data": data})

        # çŠ¶æ€æ›´æ–°æ¶ˆæ¯
        elif msg_type == "status_update":
            status = data.get("status", "")
            current_node = data.get("current_node", "")

            if status == "waiting_for_input":
                logger.info(f"â¸ï¸  ç­‰å¾…ç”¨æˆ·è¾“å…¥ - èŠ‚ç‚¹: {current_node}")
            elif status == "running":
                logger.debug(f"â–¶ï¸  è¿è¡Œä¸­ - èŠ‚ç‚¹: {current_node}")

        # é”™è¯¯æ¶ˆæ¯
        elif msg_type == "error":
            error = data.get("detail", "æœªçŸ¥é”™è¯¯")
            logger.error(f"âŒ [å·¥ä½œæµé”™è¯¯] {error}")
            self.errors.append(error)

    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š æœç´¢å·¥ä½œæµæµ‹è¯•æŠ¥å‘Š")
        logger.info("=" * 80)

        # æœç´¢äº‹ä»¶ç»Ÿè®¡
        logger.info(f"\nğŸ” æœç´¢äº‹ä»¶ç»Ÿè®¡: {len(self.search_events)}ä¸ª")

        query_gen_count = sum(1 for e in self.search_events if e["type"] == "query_generation")
        search_exec_count = sum(1 for e in self.search_events if e["type"] == "expert_search")
        tool_call_count = sum(1 for e in self.search_events if e["type"] == "tool_call")

        logger.info(f"  - æŸ¥è¯¢ç”Ÿæˆäº‹ä»¶: {query_gen_count}")
        logger.info(f"  - æœç´¢æ‰§è¡Œäº‹ä»¶: {search_exec_count}")
        logger.info(f"  - å·¥å…·è°ƒç”¨äº‹ä»¶: {tool_call_count}")

        # äº¤ä»˜ç‰©ç»Ÿè®¡
        logger.info(f"\nğŸ“¦ æµ‹è¯•çš„äº¤ä»˜ç‰©æ•°é‡: {len(self.deliverables_tested)}")
        for deliv in sorted(self.deliverables_tested):
            logger.info(f"  - {deliv}")

        # å·¥å…·è°ƒç”¨è¯¦æƒ…
        logger.info(f"\nğŸ”§ æœç´¢å·¥å…·è°ƒç”¨è¯¦æƒ…:")
        tool_calls = [e for e in self.search_events if e["type"] == "tool_call"]
        if tool_calls:
            for i, event in enumerate(tool_calls, 1):
                logger.info(f"  {i}. {event['tool']}")
        else:
            logger.warning("  âš ï¸ æœªæ£€æµ‹åˆ°ä»»ä½•æœç´¢å·¥å…·è°ƒç”¨")
            self.warnings.append("æœªæ£€æµ‹åˆ°æœç´¢å·¥å…·è°ƒç”¨")

        # é”™è¯¯ç»Ÿè®¡
        logger.info(f"\nâŒ é”™è¯¯æ•°é‡: {len(self.errors)}")
        if self.errors:
            for i, error in enumerate(self.errors, 1):
                logger.error(f"  {i}. {error}")
        else:
            logger.success("  âœ… æ— é”™è¯¯")

        # è­¦å‘Šç»Ÿè®¡
        logger.info(f"\nâš ï¸  è­¦å‘Šæ•°é‡: {len(self.warnings)}")
        if self.warnings:
            for i, warning in enumerate(self.warnings, 1):
                logger.warning(f"  {i}. {warning}")
        else:
            logger.success("  âœ… æ— è­¦å‘Š")

        # æ€»ä½“è¯„ä¼°
        logger.info("\n" + "=" * 80)
        if not self.errors and len(self.search_events) > 0:
            logger.success("âœ… æœç´¢å·¥ä½œæµæµ‹è¯•é€šè¿‡")
            logger.success(f"   æ£€æµ‹åˆ°{len(self.search_events)}ä¸ªæœç´¢ç›¸å…³äº‹ä»¶")
        elif not self.errors:
            logger.warning("âš ï¸ æµ‹è¯•å®Œæˆä½†æœªæ£€æµ‹åˆ°æœç´¢äº‹ä»¶")
        else:
            logger.error("âŒ æµ‹è¯•å¤±è´¥ - å‘ç°é”™è¯¯éœ€è¦ä¿®å¤")
        logger.info("=" * 80)


async def main():
    """ä¸»å‡½æ•°"""
    tester = SearchWorkflowTester()
    await tester.run_full_test()


if __name__ == "__main__":
    asyncio.run(main())
