"""
éªŒè¯ä¼˜å…ˆçº§é…ç½®ï¼šOpenAI å®˜æ–¹ â†’ OpenRouter â†’ DeepSeek
"""
import os
from dotenv import load_dotenv

load_dotenv()

def check_priority_config():
    """æ£€æŸ¥ä¼˜å…ˆçº§é…ç½®"""
    print("="*70)
    print("ğŸ”§ LLM æä¾›å•†ä¼˜å…ˆçº§é…ç½®æ£€æŸ¥")
    print("="*70)
    
    # è·å–é…ç½®
    provider = os.getenv("LLM_PROVIDER", "openai")
    auto_fallback = os.getenv("LLM_AUTO_FALLBACK", "false").lower() == "true"
    
    print(f"\nğŸ“‹ å½“å‰é…ç½®:")
    print(f"  ä¸»æä¾›å•†: {provider.upper()}")
    print(f"  è‡ªåŠ¨é™çº§: {'âœ… å¯ç”¨' if auto_fallback else 'âŒ ç¦ç”¨'}")
    
    # æ£€æŸ¥å„æä¾›å•† API Key çŠ¶æ€
    providers_status = {
        "OpenAI å®˜æ–¹": {
            "key_env": "OPENAI_API_KEY",
            "key": os.getenv("OPENAI_API_KEY"),
            "model": os.getenv("OPENAI_MODEL", "gpt-4.1")
        },
        "OpenRouter": {
            "key_env": "OPENROUTER_API_KEY",
            "key": os.getenv("OPENROUTER_API_KEY"),
            "model": os.getenv("OPENROUTER_MODEL", "openai/gpt-4o")
        },
        "DeepSeek": {
            "key_env": "DEEPSEEK_API_KEY",
            "key": os.getenv("DEEPSEEK_API_KEY"),
            "model": os.getenv("DEEPSEEK_MODEL", "deepseek-chat")
        },
        "Qwen": {
            "key_env": "QWEN_API_KEY",
            "key": os.getenv("QWEN_API_KEY"),
            "model": os.getenv("QWEN_MODEL", "qwen-max")
        }
    }
    
    print(f"\nğŸ“Š æä¾›å•†çŠ¶æ€:")
    for name, info in providers_status.items():
        key = info["key"]
        is_valid = key and key not in ["your_openai_api_key_here", "your_openrouter_api_key_here", 
                                       "your_deepseek_api_key_here", "your_qwen_api_key_here"]
        status = "âœ… å·²é…ç½®" if is_valid else "âŒ æœªé…ç½®"
        
        if is_valid:
            key_display = f"{key[:20]}...{key[-10:]}" if len(key) > 30 else key[:30]
            print(f"  {name:15} {status:10} | Key: {key_display} | Model: {info['model']}")
        else:
            print(f"  {name:15} {status:10}")
    
    # æ˜¾ç¤ºé™çº§ç­–ç•¥
    print(f"\nğŸ”„ é™çº§ç­–ç•¥:")
    if not auto_fallback:
        print(f"  âš ï¸ è‡ªåŠ¨é™çº§å·²ç¦ç”¨ï¼Œä»…ä½¿ç”¨ä¸»æä¾›å•†ï¼š{provider.upper()}")
    else:
        if provider == "openai":
            chain = ["OpenAI å®˜æ–¹"]
            if providers_status["OpenRouter"]["key"] and providers_status["OpenRouter"]["key"] != "your_openrouter_api_key_here":
                chain.append("OpenRouter (GPT)")
            if providers_status["DeepSeek"]["key"] and providers_status["DeepSeek"]["key"] != "your_deepseek_api_key_here":
                chain.append("DeepSeek")
            
            print(f"  âœ… {' â†’ '.join(chain)}")
            
            if len(chain) == 1:
                print(f"  âš ï¸ æ²¡æœ‰å¯ç”¨çš„å¤‡é€‰æä¾›å•†ï¼Œå»ºè®®é…ç½® OpenRouter æˆ– DeepSeek")
            elif "OpenRouter (GPT)" in chain and "DeepSeek" in chain:
                print(f"  ğŸ† å®Œç¾é…ç½®ï¼ä¸‰å±‚é™çº§ä¿éšœæœ€é«˜å¯ç”¨æ€§")
            else:
                print(f"  âœ… å¯ç”¨ï¼Œä½†å»ºè®®é…ç½®æ‰€æœ‰ä¸‰ä¸ªæä¾›å•†ä»¥è·å¾—æœ€ä½³å¯ç”¨æ€§")
        else:
            print(f"  â„¹ï¸ ä¸»æä¾›å•†æ˜¯ {provider.upper()}ï¼Œé™çº§é“¾å·²è‡ªåŠ¨ç”Ÿæˆ")
    # æ¨èé…ç½®
    print(f"\nğŸ’¡ æ¨èé…ç½®:")
    print(f"  1. LLM_PROVIDER=openai")
    print(f"  2. LLM_AUTO_FALLBACK=true")
    print(f"  3. é…ç½®ä¸‰ä¸ª API Key:")
    print(f"     - OPENAI_API_KEY (å®˜æ–¹ï¼Œé«˜è´¨é‡ä½†å›½å†…å¯èƒ½å—é™)")
    print(f"     - OPENROUTER_API_KEY (å›½å†…å¯ç”¨ï¼Œä»·æ ¼åŒå®˜æ–¹)")
    print(f"     - DEEPSEEK_API_KEY (å›½å†…æœ€å¿«ï¼Œæˆæœ¬æœ€ä½)")
    
    return provider, auto_fallback, providers_status

def test_fallback_chain():
    """æµ‹è¯•é™çº§é“¾"""
    print(f"\n" + "="*70)
    print("ğŸ§ª æµ‹è¯•é™çº§é“¾")
    print("="*70)
    
    try:
        from intelligent_project_analyzer.services.llm_factory import LLMFactory
        
        print(f"\nğŸ”§ åˆ›å»º LLM å®ä¾‹ï¼ˆä¼šè‡ªåŠ¨åº”ç”¨é™çº§é“¾ï¼‰...")
        llm = LLMFactory.create_llm(temperature=0.7, max_tokens=500)
        
        print(f"âœ… LLM å®ä¾‹åˆ›å»ºæˆåŠŸï¼")
        
        # å°è¯•è°ƒç”¨
        print(f"\nğŸ“¡ æµ‹è¯•è°ƒç”¨...")
        response = llm.invoke("ç”¨ä¸€å¥è¯è¯´æ˜å½“å‰ä½¿ç”¨çš„æ˜¯å“ªä¸ª LLM æä¾›å•†ã€‚")
        
        print(f"\nâœ… è°ƒç”¨æˆåŠŸï¼")
        print(f"ğŸ’¬ å“åº”å†…å®¹:")
        print(f"  {response.content}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        print(f"\nå¯èƒ½çš„åŸå› :")
        print(f"  1. ä¸»æä¾›å•† API Key æ— æ•ˆ")
        print(f"  2. æ‰€æœ‰å¤‡é€‰æä¾›å•†éƒ½ä¸å¯ç”¨")
        print(f"  3. ç½‘ç»œè¿æ¥é—®é¢˜")
        return False

def show_quick_fix():
    """æ˜¾ç¤ºå¿«é€Ÿä¿®å¤æŒ‡å—"""
    print(f"\n" + "="*70)
    print("ğŸ”§ å¿«é€Ÿä¿®å¤æŒ‡å—")
    print("="*70)
    
    provider = os.getenv("LLM_PROVIDER", "openai")
    
    if provider == "openai":
        openai_key = os.getenv("OPENAI_API_KEY")
        openrouter_key = os.getenv("OPENROUTER_API_KEY")
        
        if not openai_key or openai_key.startswith("your_"):
            print(f"\nâš ï¸ OpenAI å®˜æ–¹ API Key æœªé…ç½®")
            print(f"   å¦‚æœä½ åœ¨å›½å†…ä¸”æ— æ³•è®¿é—® OpenAIï¼Œå»ºè®®æ”¹ç”¨ OpenRouterï¼š")
            print(f"   1. ä¿®æ”¹ .env: LLM_PROVIDER=openrouter")
            print(f"   2. ç¡®ä¿å·²é…ç½®: OPENROUTER_API_KEY=sk-or-v1-...")
        
        if not openrouter_key or openrouter_key.startswith("your_"):
            print(f"\nğŸ’¡ å»ºè®®é…ç½® OpenRouter ä½œä¸ºå¤‡é€‰:")
            print(f"   1. è®¿é—® https://openrouter.ai/keys è·å– API Key")
            print(f"   2. ä¿®æ”¹ .env: OPENROUTER_API_KEY=sk-or-v1-...")
            print(f"   3. å·²ä¸ºä½ é…ç½®: OPENROUTER_MODEL=openai/gpt-4o")
        else:
            print(f"\nâœ… OpenRouter å·²é…ç½®ï¼Œé™çº§é“¾å®Œæ•´ï¼")
            print(f"   å½“ OpenAI å®˜æ–¹ API ä¸å¯ç”¨æ—¶ï¼Œä¼šè‡ªåŠ¨åˆ‡æ¢åˆ° OpenRouter")

if __name__ == "__main__":
    print("\nğŸš€ LLM ä¼˜å…ˆçº§é…ç½®éªŒè¯å·¥å…·")
    print("   ç›®æ ‡é…ç½®: OpenAI å®˜æ–¹ â†’ OpenRouter (GPT) â†’ DeepSeek")
    
    # 1. æ£€æŸ¥é…ç½®
    provider, auto_fallback, providers = check_priority_config()
    
    # 2. æµ‹è¯•é™çº§é“¾
    if auto_fallback:
        success = test_fallback_chain()
    else:
        print(f"\nâš ï¸ è‡ªåŠ¨é™çº§æœªå¯ç”¨ï¼Œè·³è¿‡é™çº§é“¾æµ‹è¯•")
        print(f"   å»ºè®®ä¿®æ”¹ .env: LLM_AUTO_FALLBACK=true")
        success = False
    
    # 3. æ˜¾ç¤ºä¿®å¤å»ºè®®
    show_quick_fix()
    
    print(f"\n" + "="*70)
    if success:
        print("âœ… é…ç½®éªŒè¯é€šè¿‡ï¼é™çº§é“¾æ­£å¸¸å·¥ä½œ")
    else:
        print("âš ï¸ é…ç½®éœ€è¦è°ƒæ•´ï¼Œè¯·å‚è€ƒä¸Šè¿°å»ºè®®")
    print("="*70)
