"""
é—®å·ç”Ÿæˆå™¨å•å…ƒæµ‹è¯•

æµ‹è¯•ä» calibration_questionnaire.py æå–çš„é—®å·ç”Ÿæˆå™¨æ¨¡å—ã€‚
"""

import pytest
from typing import Dict, Any

from intelligent_project_analyzer.interaction.questionnaire import (
    QuestionContext,
    FallbackQuestionGenerator,
    PhilosophyQuestionGenerator,
    BiddingStrategyGenerator,
    ConflictQuestionGenerator,
    QuestionAdjuster,
    AnswerParser
)


class TestFallbackQuestionGenerator:
    """æµ‹è¯•å…œåº•é—®é¢˜ç”Ÿæˆå™¨"""

    def test_generate_basic(self):
        """æµ‹è¯•åŸºæœ¬é—®é¢˜ç”Ÿæˆ"""
        structured_data = {
            "project_task": "è®¾è®¡ä¸€ä¸ªå’–å•¡é¦†",
            "design_challenge": "åŠŸèƒ½æ€§ä¸è‰ºæœ¯æ€§çš„å¹³è¡¡",
            "project_type": "commercial_enterprise",
            "resource_constraints": "é¢„ç®—æœ‰é™"
        }

        questions = FallbackQuestionGenerator.generate(structured_data)

        # éªŒè¯é—®é¢˜æ•°é‡ï¼ˆ7-10ä¸ªï¼‰
        assert 7 <= len(questions) <= 10, f"Expected 7-10 questions, got {len(questions)}"

        # éªŒè¯é—®é¢˜ç±»å‹åˆ†å¸ƒ
        single_choice = [q for q in questions if q["type"] == "single_choice"]
        multiple_choice = [q for q in questions if q["type"] == "multiple_choice"]
        open_ended = [q for q in questions if q["type"] == "open_ended"]

        assert len(single_choice) >= 2, "Should have at least 2 single choice questions"
        assert len(multiple_choice) >= 2, "Should have at least 2 multiple choice questions"
        assert len(open_ended) == 2, "Should have exactly 2 open-ended questions"

    def test_generate_with_tension_extraction(self):
        """æµ‹è¯•æ ¸å¿ƒçŸ›ç›¾æå–"""
        structured_data = {
            "design_challenge": '"å¿«é€Ÿè¿­ä»£éœ€æ±‚"ä¸"å“ç‰Œç¨³å®šæ€§"çš„å¯¹ç«‹',
            "project_type": "commercial_enterprise"
        }

        questions = FallbackQuestionGenerator.generate(structured_data)

        # éªŒè¯ç¬¬ä¸€ä¸ªé—®é¢˜åŒ…å«æå–çš„çŸ›ç›¾ï¼ˆæ£€æŸ¥é—®é¢˜IDè€Œä¸æ˜¯æ–‡æœ¬å†…å®¹ï¼Œé¿å…ç¼–ç é—®é¢˜ï¼‰
        first_question = questions[0]
        assert first_question["id"] in ["core_tension_priority", "orientation_preference"]
        assert first_question["type"] == "single_choice"

    def test_generate_residential_vs_commercial(self):
        """æµ‹è¯•ä½å®…ä¸å•†ä¸šé¡¹ç›®çš„é—®é¢˜å·®å¼‚"""
        residential_data = {"project_type": "personal_residential"}
        commercial_data = {"project_type": "commercial_enterprise"}

        residential_questions = FallbackQuestionGenerator.generate(residential_data)
        commercial_questions = FallbackQuestionGenerator.generate(commercial_data)

        # éªŒè¯ä½å®…é¡¹ç›®åŒ…å«æ„Ÿå®˜ä½“éªŒé—®é¢˜
        residential_ids = [q["id"] for q in residential_questions]
        assert "sensory_experience" in residential_ids

        # éªŒè¯å•†ä¸šé¡¹ç›®åŒ…å«å•†ä¸šä½“éªŒé—®é¢˜
        commercial_ids = [q["id"] for q in commercial_questions]
        assert "commercial_experience" in commercial_ids


class TestPhilosophyQuestionGenerator:
    """æµ‹è¯•ç†å¿µæ¢ç´¢é—®é¢˜ç”Ÿæˆå™¨"""

    def test_generate_with_design_challenge(self):
        """æµ‹è¯•åŸºäºè®¾è®¡æŒ‘æˆ˜ç”Ÿæˆç†å¿µé—®é¢˜"""
        structured_data = {
            "design_challenge": "ä½œä¸º[åˆ›ä¸šè€…]çš„[å¿«é€Ÿè¿­ä»£éœ€æ±‚]ä¸[å“ç‰Œç¨³å®šæ€§]",
            "project_task": "é›‡ä½£ç©ºé—´å®Œæˆ[åŠŸèƒ½æ€§]ä¸[æƒ…æ„ŸåŒ–]",
            "expert_handoff": {
                "design_challenge_spectrum": {
                    "æç«¯A": {"æ ‡ç­¾": "æç®€ä¸»ä¹‰"},
                    "æç«¯B": {"æ ‡ç­¾": "å¥¢åä¸»ä¹‰"},
                    "ä¸­é—´ç«‹åœº": [{"æ ‡ç­¾": "ç°ä»£ç®€çº¦"}]
                }
            }
        }

        questions = PhilosophyQuestionGenerator.generate(structured_data)

        assert len(questions) > 0, "Should generate at least one philosophy question"

        # éªŒè¯é—®é¢˜åŒ…å«ç†å¿µç›¸å…³å†…å®¹
        question_texts = [q["question"] for q in questions]
        assert any("ç†å¿µ" in text or "è®¤åŒ" in text for text in question_texts)

    def test_generate_empty_data(self):
        """æµ‹è¯•ç©ºæ•°æ®æƒ…å†µ"""
        structured_data = {}

        questions = PhilosophyQuestionGenerator.generate(structured_data)

        # ç©ºæ•°æ®åº”è¯¥è¿”å›ç©ºåˆ—è¡¨æˆ–ä¸å´©æºƒ
        assert isinstance(questions, list)


class TestBiddingStrategyGenerator:
    """æµ‹è¯•ç«æ ‡ç­–ç•¥é—®é¢˜ç”Ÿæˆå™¨"""

    def test_generate_with_competitors(self):
        """æµ‹è¯•åŒ…å«ç«äº‰å¯¹æ‰‹çš„åœºæ™¯"""
        user_input = "æˆ‘ä»¬è¦å‚åŠ æˆéƒ½æŸé…’åº—é¡¹ç›®çš„ç«æ ‡ï¼Œå¯¹æ‰‹æœ‰HBAã€CCDç­‰é‡é‡çº§è®¾è®¡å…¬å¸"
        structured_data = {"project_type": "commercial_enterprise"}

        questions = BiddingStrategyGenerator.generate(user_input, structured_data)

        assert len(questions) > 0, "Should generate bidding strategy questions"

        # éªŒè¯é—®é¢˜åŒ…å«ç«äº‰å¯¹æ‰‹ä¿¡æ¯
        question_texts = " ".join([q["question"] for q in questions])
        assert "HBA" in question_texts or "CCD" in question_texts or "é‡é‡çº§" in question_texts

    def test_generate_basic_bidding(self):
        """æµ‹è¯•åŸºæœ¬ç«æ ‡åœºæ™¯"""
        user_input = "ç«æ ‡é¡¹ç›®"
        structured_data = {}

        questions = BiddingStrategyGenerator.generate(user_input, structured_data)

        # åº”è¯¥ç”Ÿæˆè‡³å°‘3ä¸ªé—®é¢˜ï¼ˆå·®å¼‚åŒ–ã€å¯¹æ‰‹å¼±ç‚¹ã€è¯„å§”æ‰“åŠ¨ç‚¹ç­‰ï¼‰
        assert len(questions) >= 3


class TestConflictQuestionGenerator:
    """æµ‹è¯•èµ„æºå†²çªé—®é¢˜ç”Ÿæˆå™¨"""

    def test_generate_budget_conflict(self):
        """æµ‹è¯•é¢„ç®—å†²çªé—®é¢˜ç”Ÿæˆ"""
        feasibility = {
            "conflict_detection": {
                "budget_conflicts": [{
                    "detected": True,
                    "severity": "critical",
                    "description": "é¢„ç®—ä¸è¶³",
                    "details": {"gap": 500000, "gap_percentage": 30}
                }]
            }
        }

        # ğŸ”§ v7.4.1: å¿…é¡»ä¼ å…¥ user_mentioned_constraints å‚æ•°
        questions = ConflictQuestionGenerator.generate(
            feasibility,
            "design_execution",
            user_mentioned_constraints=["budget"]  # ç”¨æˆ·æåŠäº†é¢„ç®—çº¦æŸ
        )

        assert len(questions) > 0, "Should generate conflict questions"

        # éªŒè¯é—®é¢˜åŒ…å«é¢„ç®—ç›¸å…³å†…å®¹
        first_question = questions[0]
        assert "é¢„ç®—" in first_question["question"]
        assert first_question["type"] == "single_choice"

    def test_skip_conflicts_for_bidding_scenario(self):
        """æµ‹è¯•ç«æ ‡åœºæ™¯è·³è¿‡æ–½å·¥å†²çª"""
        feasibility = {
            "conflict_detection": {
                "budget_conflicts": [{
                    "detected": True,
                    "severity": "critical",
                    "description": "é¢„ç®—ä¸è¶³"
                }]
            }
        }

        questions = ConflictQuestionGenerator.generate(feasibility, "bidding_strategy")

        # ç«æ ‡åœºæ™¯åº”è¯¥è·³è¿‡æ–½å·¥ç›¸å…³å†²çª
        assert len(questions) == 0, "Bidding strategy should skip construction conflicts"

    def test_no_conflicts(self):
        """æµ‹è¯•æ— å†²çªæƒ…å†µ"""
        feasibility = {"conflict_detection": {}}

        questions = ConflictQuestionGenerator.generate(feasibility, "design_execution")

        assert len(questions) == 0, "Should return empty list when no conflicts"


class TestQuestionAdjuster:
    """æµ‹è¯•é—®é¢˜æ•°é‡è°ƒæ•´å™¨"""

    def test_adjust_no_trimming_needed(self):
        """æµ‹è¯•ä¸éœ€è¦è£å‰ªçš„æƒ…å†µ"""
        philosophy_questions = [{"id": "q1", "type": "single_choice"}]
        conflict_questions = [{"id": "q2", "type": "single_choice"}]

        adjusted_phil, adjusted_conf = QuestionAdjuster.adjust(
            philosophy_questions=philosophy_questions,
            conflict_questions=conflict_questions,
            original_question_count=5,
            feasibility_data={}
        )

        # æ€»é•¿åº¦ <= 7ï¼Œä¸åº”è¯¥è£å‰ª
        assert len(adjusted_phil) == 1
        assert len(adjusted_conf) == 1

    def test_adjust_with_trimming(self):
        """æµ‹è¯•éœ€è¦è£å‰ªçš„æƒ…å†µ"""
        philosophy_questions = [
            {"id": f"phil_{i}", "type": "single_choice", "dimension": "philosophy"}
            for i in range(5)
        ]
        conflict_questions = [
            {"id": f"conf_{i}", "type": "single_choice", "severity": "medium"}
            for i in range(5)
        ]

        adjusted_phil, adjusted_conf = QuestionAdjuster.adjust(
            philosophy_questions=philosophy_questions,
            conflict_questions=conflict_questions,
            original_question_count=10,  # Total: 20, should trigger heavy trimming
            feasibility_data={}
        )

        # æ€»é•¿åº¦ >= 14ï¼Œåº”è¯¥é‡åº¦è£å‰ªï¼ˆä¿ç•™40%ï¼‰
        total_adjusted = len(adjusted_phil) + len(adjusted_conf)
        assert total_adjusted < 10, f"Expected trimming, got {total_adjusted} questions"


class TestAnswerParser:
    """æµ‹è¯•ç­”æ¡ˆè§£æå™¨"""

    def test_extract_raw_answers_from_dict(self):
        """æµ‹è¯•ä»å­—å…¸æå–ç­”æ¡ˆ"""
        user_response = {
            "answers": {"q1": "ç­”æ¡ˆ1", "q2": "ç­”æ¡ˆ2"},
            "additional_info": "è¡¥å……è¯´æ˜"
        }

        raw_answers, notes = AnswerParser.extract_raw_answers(user_response)

        assert raw_answers == {"q1": "ç­”æ¡ˆ1", "q2": "ç­”æ¡ˆ2"}
        assert notes == "è¡¥å……è¯´æ˜"

    def test_extract_raw_answers_from_list(self):
        """æµ‹è¯•ä»åˆ—è¡¨æå–ç­”æ¡ˆ"""
        user_response = [
            {"question_id": "q1", "answer": "ç­”æ¡ˆ1"},
            {"question_id": "q2", "answer": "ç­”æ¡ˆ2"}
        ]

        raw_answers, notes = AnswerParser.extract_raw_answers(user_response)

        assert isinstance(raw_answers, list)
        assert len(raw_answers) == 2

    def test_build_answer_entries(self):
        """æµ‹è¯•æ„å»ºç­”æ¡ˆæ¡ç›®"""
        questionnaire = {
            "questions": [
                {"id": "q1", "question": "é—®é¢˜1", "type": "single_choice"},
                {"id": "q2", "question": "é—®é¢˜2", "type": "multiple_choice"}
            ]
        }
        raw_answers = {"q1": "é€‰é¡¹A", "q2": ["é€‰é¡¹B", "é€‰é¡¹C"]}

        entries, answers_map = AnswerParser.build_answer_entries(questionnaire, raw_answers)

        assert len(entries) == 2
        assert entries[0]["id"] == "q1"
        assert entries[0]["value"] == "é€‰é¡¹A"
        assert entries[1]["id"] == "q2"
        assert entries[1]["value"] == ["é€‰é¡¹B", "é€‰é¡¹C"]

        assert answers_map["q1"] == "é€‰é¡¹A"
        assert answers_map["q2"] == ["é€‰é¡¹B", "é€‰é¡¹C"]


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
